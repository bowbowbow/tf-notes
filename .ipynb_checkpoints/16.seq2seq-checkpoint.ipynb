{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/golbin/TensorFlow-Tutorials \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S: 디코딩 입력의 시작을 나타내는 심볼\n",
    "- E: 디코딩 출력의 끝을 나타내는 심볼\n",
    "- P: 현재 배치 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n",
    "\n",
    "```\n",
    "예) 현재 배치 데이터의 최대 크기가 4인 경우\n",
    "word -> ['w', 'o', 'r', 'd']\n",
    "to   -> ['t', 'o', 'P', 'P']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "# 영어를 한글로 번역하기 위한 학습 데이터\n",
    "seq_data = [['word', '단어'], ['wood', '나무'],\n",
    "            ['game', '놀이'], ['girl', '소녀'],\n",
    "            ['kiss', '키스'], ['love', '사랑']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        # 인코더 셀의 입력값\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        # 디코더 셀의 입력값. 시작을 타나내는 S 심볼을 맨 앞에 붙여준다.\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E를 붙여준다.\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        \n",
    "        # sparse_softmax_cross_entropy_with_logits 사용할거라 one-hot 인코딩이 아님\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "# 입력과 출력의 형태가 one-hot 인코딩으로 같기 때문에\n",
    "n_class = n_input = dic_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-ba882fbbcb4a>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성\n",
    "\n",
    "# [batch_size, time_steps, input_size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "# 인코더 셀 구성\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "    \n",
    "# 디코더 셀 구성\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    # seq2seq에서는 인코더 셀의 최종 상태값을 디코더 셀의 초기값으로 넣어줘야함\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0001 cost =  3.696160\n",
      "Epoch : 0002 cost =  2.532786\n",
      "Epoch : 0003 cost =  1.729092\n",
      "Epoch : 0004 cost =  1.018924\n",
      "Epoch : 0005 cost =  0.757470\n",
      "Epoch : 0006 cost =  0.524391\n",
      "Epoch : 0007 cost =  0.384913\n",
      "Epoch : 0008 cost =  0.318902\n",
      "Epoch : 0009 cost =  0.320001\n",
      "Epoch : 0010 cost =  0.116064\n",
      "Epoch : 0011 cost =  0.185939\n",
      "Epoch : 0012 cost =  0.263134\n",
      "Epoch : 0013 cost =  0.091561\n",
      "Epoch : 0014 cost =  0.335818\n",
      "Epoch : 0015 cost =  0.152458\n",
      "Epoch : 0016 cost =  0.246534\n",
      "Epoch : 0017 cost =  0.137936\n",
      "Epoch : 0018 cost =  0.247754\n",
      "Epoch : 0019 cost =  0.135011\n",
      "Epoch : 0020 cost =  0.106507\n",
      "Epoch : 0021 cost =  0.075979\n",
      "Epoch : 0022 cost =  0.232578\n",
      "Epoch : 0023 cost =  0.099219\n",
      "Epoch : 0024 cost =  0.054392\n",
      "Epoch : 0025 cost =  0.035221\n",
      "Epoch : 0026 cost =  0.030721\n",
      "Epoch : 0027 cost =  0.086036\n",
      "Epoch : 0028 cost =  0.017701\n",
      "Epoch : 0029 cost =  0.036509\n",
      "Epoch : 0030 cost =  0.007901\n",
      "Epoch : 0031 cost =  0.009371\n",
      "Epoch : 0032 cost =  0.096299\n",
      "Epoch : 0033 cost =  0.013614\n",
      "Epoch : 0034 cost =  0.047555\n",
      "Epoch : 0035 cost =  0.007766\n",
      "Epoch : 0036 cost =  0.003198\n",
      "Epoch : 0037 cost =  0.007219\n",
      "Epoch : 0038 cost =  0.009219\n",
      "Epoch : 0039 cost =  0.004094\n",
      "Epoch : 0040 cost =  0.011513\n",
      "Epoch : 0041 cost =  0.005093\n",
      "Epoch : 0042 cost =  0.003227\n",
      "Epoch : 0043 cost =  0.008372\n",
      "Epoch : 0044 cost =  0.004290\n",
      "Epoch : 0045 cost =  0.001791\n",
      "Epoch : 0046 cost =  0.002830\n",
      "Epoch : 0047 cost =  0.000995\n",
      "Epoch : 0048 cost =  0.001487\n",
      "Epoch : 0049 cost =  0.002425\n",
      "Epoch : 0050 cost =  0.009765\n",
      "Epoch : 0051 cost =  0.004957\n",
      "Epoch : 0052 cost =  0.000936\n",
      "Epoch : 0053 cost =  0.003212\n",
      "Epoch : 0054 cost =  0.001746\n",
      "Epoch : 0055 cost =  0.006669\n",
      "Epoch : 0056 cost =  0.001370\n",
      "Epoch : 0057 cost =  0.001968\n",
      "Epoch : 0058 cost =  0.000796\n",
      "Epoch : 0059 cost =  0.000548\n",
      "Epoch : 0060 cost =  0.003022\n",
      "Epoch : 0061 cost =  0.001609\n",
      "Epoch : 0062 cost =  0.004366\n",
      "Epoch : 0063 cost =  0.000710\n",
      "Epoch : 0064 cost =  0.000891\n",
      "Epoch : 0065 cost =  0.001406\n",
      "Epoch : 0066 cost =  0.001741\n",
      "Epoch : 0067 cost =  0.001126\n",
      "Epoch : 0068 cost =  0.003137\n",
      "Epoch : 0069 cost =  0.019690\n",
      "Epoch : 0070 cost =  0.000671\n",
      "Epoch : 0071 cost =  0.000460\n",
      "Epoch : 0072 cost =  0.001137\n",
      "Epoch : 0073 cost =  0.000575\n",
      "Epoch : 0074 cost =  0.000534\n",
      "Epoch : 0075 cost =  0.003217\n",
      "Epoch : 0076 cost =  0.000438\n",
      "Epoch : 0077 cost =  0.000348\n",
      "Epoch : 0078 cost =  0.000406\n",
      "Epoch : 0079 cost =  0.000256\n",
      "Epoch : 0080 cost =  0.000394\n",
      "Epoch : 0081 cost =  0.000463\n",
      "Epoch : 0082 cost =  0.000798\n",
      "Epoch : 0083 cost =  0.000530\n",
      "Epoch : 0084 cost =  0.000225\n",
      "Epoch : 0085 cost =  0.000906\n",
      "Epoch : 0086 cost =  0.000334\n",
      "Epoch : 0087 cost =  0.000862\n",
      "Epoch : 0088 cost =  0.000485\n",
      "Epoch : 0089 cost =  0.000400\n",
      "Epoch : 0090 cost =  0.000480\n",
      "Epoch : 0091 cost =  0.000258\n",
      "Epoch : 0092 cost =  0.000820\n",
      "Epoch : 0093 cost =  0.000675\n",
      "Epoch : 0094 cost =  0.003524\n",
      "Epoch : 0095 cost =  0.000188\n",
      "Epoch : 0096 cost =  0.000648\n",
      "Epoch : 0097 cost =  0.001003\n",
      "Epoch : 0098 cost =  0.001156\n",
      "Epoch : 0099 cost =  0.001069\n",
      "Epoch : 0100 cost =  0.000220\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 학습\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost], feed_dict={\n",
    "                                                     enc_input: input_batch, \n",
    "                                                     dec_input: output_batch,\n",
    "                                                     targets: target_batch,\n",
    "                                                    })\n",
    "    \n",
    "    print('Epoch :', '%04d' % (epoch + 1), 'cost = ', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 번역 테스트 ===\n",
      "['단', '어', 'E', 'E', '무', '이', 'z', '녀', '사']\n",
      "word -> 단어\n",
      "['나', '무', 'E', 'E', '무']\n",
      "wodr -> 나무\n",
      "['사', '랑', 'E', 'E', '놀']\n",
      "love -> 사랑\n",
      "['사', '랑', 'E', 'E', '놀']\n",
      "loev -> 사랑\n",
      "['키', '스', 'E', 'E', '놀']\n",
      "abcd -> 키스\n"
     ]
    }
   ],
   "source": [
    "# 번역 테스트\n",
    "def translate(word):\n",
    "    # 예측시 디코더의 입출력값을 의미 없는 값인 P로 채운다.\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    \n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    # 결과가 [batch_size, time_step, input] 으로 나오기 때문에\n",
    "    # 2번째 차원원인 input에서 가장 확률이 높은 글자를 예측값으로 만든다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "    \n",
    "    result = sess.run(prediction, feed_dict={\n",
    "        enc_input: input_batch,\n",
    "        dec_input: output_batch,\n",
    "        targets: target_batch,\n",
    "    })\n",
    "    \n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    \n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "    print(decoded)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "print('\\n=== 번역 테스트 ===')\n",
    "\n",
    "print('word ->', translate('wordword'))\n",
    "print('wodr ->', translate('wodr'))\n",
    "print('love ->', translate('love'))\n",
    "print('loev ->', translate('loev'))\n",
    "print('abcd ->', translate('abcd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
