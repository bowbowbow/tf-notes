{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Reference : https://github.com/roomylee/self-attentive-emb-tf\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data loading params\n",
    "    labels_count = 4\n",
    "    max_sentence_length = 50\n",
    "    dev_sample_percentage = 0.1\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    embedding_dim = 300 # Dimensionality of word embedding\n",
    "    hidden_size= 256 # Size of LSTM hidden layer\n",
    "    d_a_size = 350 # Size of W_s1 embedding\n",
    "    r_size = 30 # Size of W_s2 embedding\n",
    "    fc_size = 2000 # Size of fully connected layer\n",
    "    p_coef = 1.0 # Coefficient for penalty\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 100\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 4\n",
    "    learning_rate = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Load AG's news topic classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wall st bears claw back into the black \\( reut...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>carlyle looks toward commercial aerospace \\( r...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oil and economy cloud stocks' outlook \\( reute...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>iraq halts oil exports from main southern pipe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oil prices soar to all time record , posing ne...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  labels\n",
       "0  wall st bears claw back into the black \\( reut...       3\n",
       "1  carlyle looks toward commercial aerospace \\( r...       3\n",
       "2  oil and economy cloud stocks' outlook \\( reute...       3\n",
       "3  iraq halts oil exports from main southern pipe...       3\n",
       "4  oil prices soar to all time record , posing ne...       3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "\n",
    "class Dataset:\n",
    "    def clean_str(self, string):\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for all datasets except for SST.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        return string.strip().lower()\n",
    "\n",
    "    def load_data_and_labels(self, path):\n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(path, 'r') as f:\n",
    "            rdr = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "            for row in rdr:\n",
    "                txt = \"\"\n",
    "                for s in row[1:]:\n",
    "                    txt = txt + re.sub(\"^\\s*(.-)\\s*$\", \"%1\", s).replace(\"\\\\n\", \"\\n\") + \" \"\n",
    "                txt = self.clean_str(txt)\n",
    "                data.append(txt)\n",
    "                labels.append(int(row[0]))\n",
    "\n",
    "        data = np.asarray(data)\n",
    "        labels = np.asarray(labels)\n",
    "        \n",
    "        df = pd.DataFrame({'data': data, 'labels': labels})\n",
    "        return df\n",
    "    \n",
    "    def download_and_load_datasets(self):\n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"AG_news_data.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/AG_news_data.zip\", \n",
    "          extract=True)\n",
    "        \n",
    "        train_file = 'AG_news_data/train.csv'\n",
    "        test_file = 'AG_news_data/test.csv'\n",
    "        \n",
    "        train_df = self.load_data_and_labels(os.path.join(os.path.dirname(dataset), train_file))\n",
    "        test_df = self.load_data_and_labels(os.path.join(os.path.dirname(dataset), test_file))\n",
    "        return train_df, test_df\n",
    "\n",
    "dataset = Dataset()\n",
    "train_df, test_df = dataset.download_and_load_datasets()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    def __init__(self, \n",
    "               sequence_length, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               embedding_size, \n",
    "               hidden_size,\n",
    "               d_a_size,\n",
    "               r_size,\n",
    "               fc_size,\n",
    "               p_coef,\n",
    "            ):\n",
    "        \n",
    "        self.input_text = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_text')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, num_classes], name='input_y')\n",
    "        \n",
    "        text_length = self._length(self.input_text)\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.name_scope('Embeddings'):\n",
    "            self.W_text = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name='W_text')\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_text)\n",
    "        \n",
    "        # Bidirectional RNN\n",
    "        with tf.name_scope('bi-lstm'):\n",
    "            fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "            bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "            (self.output_fw, self.output_bw), states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw=fw_cell,\n",
    "                cell_bw=bw_cell,\n",
    "                inputs=self.embedded_chars,\n",
    "                sequence_length=text_length,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "            self.H = tf.concat([self.output_fw, self.output_bw], axis=2) # [batch_size, sequence_length, 2*hidden_size]\n",
    "            H_reshape = tf.reshape(self.H, [-1, 2*hidden_size]) # [batch_size * sequence_length, 2*hidden_size]\n",
    "        \n",
    "        with tf.name_scope('self-attention'):\n",
    "            self.W_s1 = tf.get_variable('W_s1', shape=[2*hidden_size, d_a_size], initializer=initializer)\n",
    "            _H_s1 = tf.nn.tanh(tf.matmul(H_reshape, self.W_s1)) # [batch_size * sequence_length, d_a_size]\n",
    "            self.W_s2 = tf.get_variable('W_s2', shape=[d_a_size, r_size], initializer=initializer)\n",
    "            _H_s2 = tf.matmul(_H_s1, self.W_s2) # [batch_size * sequence_length, r_size]\n",
    "            _H_s2_reshape = tf.transpose(tf.reshape(_H_s2, [-1, sequence_length, r_size]), [0, 2, 1]) # [batch_size, r_size, sequence_length] ]\n",
    "            self.A = tf.nn.softmax(_H_s2_reshape, name='attention') # [batch_size, r_size, sequence_length]\n",
    "            \n",
    "        with tf.name_scope('sentence-embedding'):\n",
    "            self.M = tf.matmul(self.A, self.H) # [batch_size, r_size, 2*hidden_size]\n",
    "        \n",
    "        with tf.name_scope('fully-connected'):\n",
    "            self.M_flat = tf.reshape(self.M, shape=[-1, 2*hidden_size * r_size]) # [batch_size, 2*hidden_size * r_size]\n",
    "            W_fc = tf.get_variable('W_fc', shape=[2*hidden_size * r_size, fc_size], initializer=initializer)\n",
    "            b_fc = tf.Variable(tf.constant(0.1, shape=[fc_size]), name='b_fc')\n",
    "            self.fc = tf.nn.relu(tf.nn.xw_plus_b(self.M_flat, W_fc, b_fc), name='fc') # [batch_size, fc_size]\n",
    "            \n",
    "        with tf.name_scope('output'):\n",
    "            W_output = tf.get_variable('W_output', shape=[fc_size, num_classes], initializer=initializer)\n",
    "            b_output = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='b_output')\n",
    "            \n",
    "            self.logits = tf.nn.xw_plus_b(self.fc, W_output, b_output, name='logits') # [batch_size, num_classes]\n",
    "            self.predictions = tf.argmax(self.logits, 1, name='predictions')\n",
    "        \n",
    "        with tf.name_scope(\"penalization\"):\n",
    "            A_T = tf.transpose(self.A, [0, 2, 1]) # [batch_size, sequence_length, r_size]\n",
    "            self.AA_T = tf.matmul(self.A, A_T) # [batch_size, r_size, r_size]\n",
    "            # copy identity matrix by batch_size. [r_size, r_size] -> [batch_size*r_size, r_size]\n",
    "            I_ = tf.tile(tf.eye(r_size), [tf.shape(self.A)[0], 1]) # [batch_size*r_size, r_size]\n",
    "            self.I = tf.reshape(I_, [-1, r_size, r_size]) # [batch_size, r_size, r_size]\n",
    "            self.P = tf.square(tf.norm(self.AA_T - self.I, axis=[-2, -1], ord=\"fro\"))\n",
    "        \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.name_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n",
    "            self.loss_P = tf.reduce_mean(self.P * p_coef)\n",
    "            self.loss = tf.reduce_mean(losses) + self.loss_P\n",
    "        \n",
    "        # Accuracy    \n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "            \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-186e563e07d9>:2: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Text Vocabulary Size 67789\n",
      "X = (120000, 50)\n",
      "Y = (120000, 4)\n",
      "Train/Dev split: 108000/12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(Config.max_sentence_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(train_df['data'])))\n",
    "y = np.array([np.eye(Config.labels_count)[label - 1] for label in train_df['labels']])\n",
    "\n",
    "print('Text Vocabulary Size {}'.format(len(vocab_processor.vocabulary_)))\n",
    "print('X = {}'.format(x.shape))\n",
    "print('Y = {}'.format(y.shape))\n",
    "\n",
    "# Randomly shuffle data to split into train and dev\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled, y_shuffled = x[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "# Split train/dev set\n",
    "dev_sample_index = -1 * int(Config.dev_sample_percentage*float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:] \n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:] \n",
    "print(\"Train/Dev split: {:d}/{:d}\\n\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained word2vec\n",
    "def load_word2vec(embedding_dim, vocab):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"GoogleNews-vectors-negative300.bin.gz\", \n",
    "      origin=\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'GoogleNews-vectors-negative300.bin')\n",
    "    if not os.path.exists(embedding_path):\n",
    "        print('unzip :', embedding_path)\n",
    "        import gzip\n",
    "        import shutil\n",
    "        with gzip.open('{}.gz'.format(embedding_path), 'rb') as f_in:\n",
    "            with open(embedding_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) / np.sqrt(len(vocab.vocabulary_))\n",
    "    # load any vectors from the word2vec\n",
    "    print(\"Load word2vec file {0}\".format(embedding_path))\n",
    "    with open(embedding_path, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('latin-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            idx = vocab.vocabulary_.get(word)\n",
    "            if idx != 0:\n",
    "                initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return initW\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-5579c14d313e>:27: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-4-5579c14d313e>:34: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "I_ : Tensor(\"penalization/Tile:0\", shape=(?, 30), dtype=float32)\n",
      "I : Tensor(\"penalization/Reshape:0\", shape=(?, 30, 30), dtype=float32)\n",
      "Writing to /home/seungwon/project/tf-notes/28.runs/1557248376\n",
      "\n",
      "embedding_path : /home/seungwon/.keras/datasets/GoogleNews-vectors-negative300.bin\n",
      "Load word2vec file /home/seungwon/.keras/datasets/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/ipykernel_launcher.py:38: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to load pre-trained word2vec model!\n",
      "\n",
      "2019-05-08T02:00:25.094419: step 500, loss 30.4609, acc 0.6875\n",
      "2019-05-08T02:00:54.153484: step 1000, loss 30.4122, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:00:55.903486: step 1000, loss 30.382, acc 0.8015\n",
      "\n",
      "2019-05-08T02:01:24.984404: step 1500, loss 30.2903, acc 0.734375\n",
      "2019-05-08T02:01:54.062674: step 2000, loss 30.1578, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:01:55.573597: step 2000, loss 30.2121, acc 0.8184\n",
      "\n",
      "2019-05-08T02:02:24.681914: step 2500, loss 30.0708, acc 0.84375\n",
      "2019-05-08T02:02:53.758449: step 3000, loss 30.1179, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:02:55.256589: step 3000, loss 30.0493, acc 0.8261\n",
      "\n",
      "2019-05-08T02:03:24.368628: step 3500, loss 30.0016, acc 0.8125\n",
      "2019-05-08T02:03:53.474670: step 4000, loss 29.8781, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:03:54.981363: step 4000, loss 29.9179, acc 0.8314\n",
      "\n",
      "2019-05-08T02:04:24.093103: step 4500, loss 29.8669, acc 0.84375\n",
      "2019-05-08T02:04:53.207673: step 5000, loss 29.7117, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:04:54.700741: step 5000, loss 29.8234, acc 0.8353\n",
      "\n",
      "2019-05-08T02:05:23.818881: step 5500, loss 29.8286, acc 0.875\n",
      "2019-05-08T02:05:52.935130: step 6000, loss 29.7001, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:05:54.434948: step 6000, loss 29.7578, acc 0.8397\n",
      "\n",
      "2019-05-08T02:06:23.553024: step 6500, loss 29.7145, acc 0.828125\n",
      "2019-05-08T02:06:52.672504: step 7000, loss 29.688, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:06:54.162943: step 7000, loss 29.712, acc 0.8435\n",
      "\n",
      "2019-05-08T02:07:23.255050: step 7500, loss 29.7286, acc 0.765625\n",
      "2019-05-08T02:07:52.376422: step 8000, loss 29.6506, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:07:53.878172: step 8000, loss 29.6796, acc 0.8461\n",
      "\n",
      "2019-05-08T02:08:22.994043: step 8500, loss 29.7016, acc 0.84375\n",
      "2019-05-08T02:08:52.104976: step 9000, loss 29.6647, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:08:53.604197: step 9000, loss 29.6559, acc 0.8487\n",
      "\n",
      "2019-05-08T02:09:22.715650: step 9500, loss 29.6962, acc 0.828125\n",
      "2019-05-08T02:09:51.833815: step 10000, loss 29.6636, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:09:53.335292: step 10000, loss 29.6381, acc 0.8508\n",
      "\n",
      "2019-05-08T02:10:22.457339: step 10500, loss 29.7503, acc 0.765625\n",
      "2019-05-08T02:10:51.564707: step 11000, loss 29.6067, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:10:53.072076: step 11000, loss 29.6241, acc 0.8525\n",
      "\n",
      "2019-05-08T02:11:22.192232: step 11500, loss 29.6974, acc 0.828125\n",
      "2019-05-08T02:11:51.309657: step 12000, loss 29.609, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:11:52.794422: step 12000, loss 29.6131, acc 0.8543\n",
      "\n",
      "2019-05-08T02:12:21.901601: step 12500, loss 29.5426, acc 0.875\n",
      "2019-05-08T02:12:51.009991: step 13000, loss 29.5228, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:12:52.499992: step 13000, loss 29.6039, acc 0.8564\n",
      "\n",
      "2019-05-08T02:13:21.612010: step 13500, loss 29.559, acc 0.859375\n",
      "2019-05-08T02:13:50.726963: step 14000, loss 29.6454, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:13:52.229019: step 14000, loss 29.5965, acc 0.8575\n",
      "\n",
      "2019-05-08T02:14:21.366258: step 14500, loss 29.6372, acc 0.84375\n",
      "2019-05-08T02:14:50.470945: step 15000, loss 29.6038, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:14:51.975918: step 15000, loss 29.5899, acc 0.8589\n",
      "\n",
      "2019-05-08T02:15:21.092964: step 15500, loss 29.3979, acc 0.9375\n",
      "2019-05-08T02:15:50.185059: step 16000, loss 29.513, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:15:51.677410: step 16000, loss 29.5845, acc 0.8600\n",
      "\n",
      "2019-05-08T02:16:20.800414: step 16500, loss 29.6071, acc 0.890625\n",
      "2019-05-08T02:16:49.905820: step 17000, loss 29.6079, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:16:51.412661: step 17000, loss 29.5795, acc 0.8609\n",
      "\n",
      "2019-05-08T02:17:20.522925: step 17500, loss 29.4319, acc 0.90625\n",
      "2019-05-08T02:17:49.637310: step 18000, loss 29.595, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:17:51.138436: step 18000, loss 29.5751, acc 0.8624\n",
      "\n",
      "2019-05-08T02:18:20.239268: step 18500, loss 29.401, acc 0.90625\n",
      "2019-05-08T02:18:49.364036: step 19000, loss 29.7425, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:18:50.860632: step 19000, loss 29.5711, acc 0.8633\n",
      "\n",
      "2019-05-08T02:19:19.971485: step 19500, loss 29.4852, acc 0.859375\n",
      "2019-05-08T02:19:49.102537: step 20000, loss 29.4309, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:19:50.632927: step 20000, loss 29.5675, acc 0.8647\n",
      "\n",
      "2019-05-08T02:20:19.763400: step 20500, loss 29.5922, acc 0.859375\n",
      "2019-05-08T02:20:48.887801: step 21000, loss 29.5295, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:20:50.380663: step 21000, loss 29.564, acc 0.8650\n",
      "\n",
      "2019-05-08T02:21:19.485908: step 21500, loss 29.4823, acc 0.890625\n",
      "2019-05-08T02:21:48.598330: step 22000, loss 29.5342, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:21:50.105125: step 22000, loss 29.561, acc 0.8669\n",
      "\n",
      "2019-05-08T02:22:19.232421: step 22500, loss 29.4998, acc 0.90625\n",
      "2019-05-08T02:22:48.332617: step 23000, loss 29.3905, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:22:49.835009: step 23000, loss 29.5582, acc 0.8679\n",
      "\n",
      "2019-05-08T02:23:18.965562: step 23500, loss 29.4875, acc 0.875\n",
      "2019-05-08T02:23:48.094892: step 24000, loss 29.6279, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:23:49.601846: step 24000, loss 29.5553, acc 0.8683\n",
      "\n",
      "2019-05-08T02:24:18.711369: step 24500, loss 29.5002, acc 0.921875\n",
      "2019-05-08T02:24:47.811977: step 25000, loss 29.6107, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:24:49.300105: step 25000, loss 29.5526, acc 0.8686\n",
      "\n",
      "2019-05-08T02:25:18.434259: step 25500, loss 29.5588, acc 0.859375\n",
      "2019-05-08T02:25:47.546199: step 26000, loss 29.6438, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:25:49.039826: step 26000, loss 29.5502, acc 0.8694\n",
      "\n",
      "2019-05-08T02:26:18.145969: step 26500, loss 29.6665, acc 0.859375\n",
      "2019-05-08T02:26:47.244662: step 27000, loss 29.5333, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:26:48.745380: step 27000, loss 29.5478, acc 0.8703\n",
      "\n",
      "2019-05-08T02:27:17.876121: step 27500, loss 29.4364, acc 0.921875\n",
      "2019-05-08T02:27:46.991672: step 28000, loss 29.6188, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:27:48.476791: step 28000, loss 29.5455, acc 0.8712\n",
      "\n",
      "2019-05-08T02:28:17.603751: step 28500, loss 29.3733, acc 0.953125\n",
      "2019-05-08T02:28:46.723247: step 29000, loss 29.4797, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:28:48.229414: step 29000, loss 29.5436, acc 0.8723\n",
      "\n",
      "2019-05-08T02:29:17.361506: step 29500, loss 29.4205, acc 0.90625\n",
      "2019-05-08T02:29:46.464461: step 30000, loss 29.4491, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:29:47.967077: step 30000, loss 29.5415, acc 0.8721\n",
      "\n",
      "2019-05-08T02:30:17.066032: step 30500, loss 29.4015, acc 0.90625\n",
      "2019-05-08T02:30:46.169854: step 31000, loss 29.574, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:30:47.662416: step 31000, loss 29.5395, acc 0.8727\n",
      "\n",
      "2019-05-08T02:31:16.794968: step 31500, loss 29.7195, acc 0.78125\n",
      "2019-05-08T02:31:45.903649: step 32000, loss 29.4803, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:31:47.398989: step 32000, loss 29.5377, acc 0.8730\n",
      "\n",
      "2019-05-08T02:32:16.522676: step 32500, loss 29.4825, acc 0.890625\n",
      "2019-05-08T02:32:45.639253: step 33000, loss 29.6212, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:32:47.125572: step 33000, loss 29.5358, acc 0.8733\n",
      "\n",
      "2019-05-08T02:33:16.239129: step 33500, loss 29.5727, acc 0.828125\n",
      "2019-05-08T02:33:45.356350: step 34000, loss 29.5265, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:33:46.855627: step 34000, loss 29.5341, acc 0.8742\n",
      "\n",
      "2019-05-08T02:34:15.976959: step 34500, loss 29.5876, acc 0.859375\n",
      "2019-05-08T02:34:45.084959: step 35000, loss 29.4468, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:34:46.595751: step 35000, loss 29.5323, acc 0.8748\n",
      "\n",
      "2019-05-08T02:35:15.680593: step 35500, loss 29.506, acc 0.90625\n",
      "2019-05-08T02:35:44.801645: step 36000, loss 29.4842, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:35:46.299373: step 36000, loss 29.5308, acc 0.8752\n",
      "\n",
      "2019-05-08T02:36:15.423440: step 36500, loss 29.3619, acc 0.90625\n",
      "2019-05-08T02:36:44.530797: step 37000, loss 29.5799, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:36:46.055085: step 37000, loss 29.5292, acc 0.8758\n",
      "\n",
      "2019-05-08T02:37:15.157148: step 37500, loss 29.3616, acc 0.921875\n",
      "2019-05-08T02:37:44.263669: step 38000, loss 29.5774, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:37:45.765835: step 38000, loss 29.5276, acc 0.8758\n",
      "\n",
      "2019-05-08T02:40:14.330175: step 40500, loss 29.4492, acc 0.875\n",
      "2019-05-08T02:40:43.444908: step 41000, loss 29.4224, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:40:44.942115: step 41000, loss 29.5232, acc 0.8775\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-08T02:41:14.045445: step 41500, loss 29.6666, acc 0.84375\n",
      "2019-05-08T02:41:43.145286: step 42000, loss 29.4119, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:41:44.645379: step 42000, loss 29.5219, acc 0.8793\n",
      "\n",
      "2019-05-08T02:42:13.772007: step 42500, loss 29.6114, acc 0.859375\n",
      "2019-05-08T02:42:42.868586: step 43000, loss 29.6038, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:42:44.373597: step 43000, loss 29.5206, acc 0.8793\n",
      "\n",
      "2019-05-08T02:43:13.495066: step 43500, loss 29.3235, acc 0.953125\n",
      "2019-05-08T02:43:42.634246: step 44000, loss 29.5575, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:43:44.122400: step 44000, loss 29.5192, acc 0.8804\n",
      "\n",
      "2019-05-08T02:44:13.228692: step 44500, loss 29.5247, acc 0.84375\n",
      "2019-05-08T02:44:42.350854: step 45000, loss 29.4009, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:44:43.846834: step 45000, loss 29.5179, acc 0.8807\n",
      "\n",
      "2019-05-08T02:45:12.977389: step 45500, loss 29.4751, acc 0.859375\n",
      "2019-05-08T02:45:42.091099: step 46000, loss 29.4298, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:45:43.599072: step 46000, loss 29.5166, acc 0.8810\n",
      "\n",
      "2019-05-08T02:46:12.730427: step 46500, loss 29.5851, acc 0.875\n",
      "2019-05-08T02:46:41.860794: step 47000, loss 29.6053, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:46:43.346910: step 47000, loss 29.5155, acc 0.8813\n",
      "\n",
      "2019-05-08T02:47:12.478389: step 47500, loss 29.3513, acc 0.953125\n",
      "2019-05-08T02:47:41.609211: step 48000, loss 29.5768, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:47:43.096024: step 48000, loss 29.5142, acc 0.8812\n",
      "\n",
      "2019-05-08T02:48:12.208769: step 48500, loss 29.4773, acc 0.890625\n",
      "2019-05-08T02:48:41.320776: step 49000, loss 29.5058, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:48:42.830652: step 49000, loss 29.5131, acc 0.8813\n",
      "\n",
      "2019-05-08T02:49:11.960419: step 49500, loss 29.4815, acc 0.828125\n",
      "2019-05-08T02:49:41.063923: step 50000, loss 29.424, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:49:42.559794: step 50000, loss 29.5119, acc 0.8816\n",
      "\n",
      "2019-05-08T02:50:11.665574: step 50500, loss 29.3399, acc 0.96875\n",
      "2019-05-08T02:50:40.775696: step 51000, loss 29.5173, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:50:42.267269: step 51000, loss 29.5109, acc 0.8820\n",
      "\n",
      "2019-05-08T02:51:11.398282: step 51500, loss 29.4722, acc 0.875\n",
      "2019-05-08T02:51:40.507826: step 52000, loss 29.6718, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:51:42.006715: step 52000, loss 29.5097, acc 0.8818\n",
      "\n",
      "2019-05-08T02:52:11.121125: step 52500, loss 29.5561, acc 0.84375\n",
      "2019-05-08T02:52:40.230065: step 53000, loss 29.4135, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:52:41.727782: step 53000, loss 29.5087, acc 0.8828\n",
      "\n",
      "2019-05-08T02:53:10.837759: step 53500, loss 29.4617, acc 0.890625\n",
      "2019-05-08T02:53:39.967751: step 54000, loss 29.4715, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:53:41.452357: step 54000, loss 29.5075, acc 0.8831\n",
      "\n",
      "2019-05-08T02:54:10.576515: step 54500, loss 29.4993, acc 0.890625\n",
      "2019-05-08T02:54:39.687462: step 55000, loss 29.5676, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:54:41.188979: step 55000, loss 29.5065, acc 0.8836\n",
      "\n",
      "2019-05-08T02:55:10.315991: step 55500, loss 29.4234, acc 0.9375\n",
      "2019-05-08T02:55:39.432101: step 56000, loss 29.4338, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:55:40.936602: step 56000, loss 29.5055, acc 0.8838\n",
      "\n",
      "2019-05-08T02:56:10.057618: step 56500, loss 29.4264, acc 0.921875\n",
      "2019-05-08T02:56:39.157766: step 57000, loss 29.4794, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:56:40.659677: step 57000, loss 29.5044, acc 0.8839\n",
      "\n",
      "2019-05-08T02:57:09.784105: step 57500, loss 29.5243, acc 0.90625\n",
      "2019-05-08T02:57:38.891666: step 58000, loss 29.7339, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:57:40.387059: step 58000, loss 29.5034, acc 0.8842\n",
      "\n",
      "2019-05-08T02:58:09.485850: step 58500, loss 29.4795, acc 0.875\n",
      "2019-05-08T02:58:38.599340: step 59000, loss 29.641, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:58:40.095183: step 59000, loss 29.5023, acc 0.8840\n",
      "\n",
      "2019-05-08T02:59:09.213087: step 59500, loss 29.6398, acc 0.828125\n",
      "2019-05-08T02:59:38.328729: step 60000, loss 29.5624, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T02:59:39.849432: step 60000, loss 29.5014, acc 0.8844\n",
      "\n",
      "2019-05-08T03:00:08.947105: step 60500, loss 29.5803, acc 0.875\n",
      "2019-05-08T03:00:38.062149: step 61000, loss 29.4129, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:00:39.554650: step 61000, loss 29.5004, acc 0.8846\n",
      "\n",
      "2019-05-08T03:01:08.662636: step 61500, loss 29.5304, acc 0.875\n",
      "2019-05-08T03:01:37.759597: step 62000, loss 29.4705, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:01:39.262909: step 62000, loss 29.4996, acc 0.8848\n",
      "\n",
      "2019-05-08T03:02:08.394573: step 62500, loss 29.4496, acc 0.890625\n",
      "2019-05-08T03:02:37.503144: step 63000, loss 29.3385, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:02:38.999920: step 63000, loss 29.4984, acc 0.8850\n",
      "\n",
      "2019-05-08T03:03:08.114524: step 63500, loss 29.4814, acc 0.890625\n",
      "2019-05-08T03:05:07.568748: step 65500, loss 29.5631, acc 0.875\n",
      "2019-05-08T03:05:36.679289: step 66000, loss 29.3939, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:05:38.165059: step 66000, loss 29.4959, acc 0.8860\n",
      "\n",
      "2019-05-08T03:06:07.270259: step 66500, loss 29.5139, acc 0.859375\n",
      "2019-05-08T03:06:36.389156: step 67000, loss 29.4292, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:06:37.878809: step 67000, loss 29.4946, acc 0.8860\n",
      "\n",
      "2019-05-08T03:07:06.987214: step 67500, loss 29.4926, acc 0.890625\n",
      "2019-05-08T03:07:36.106346: step 68000, loss 29.5946, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:07:37.604522: step 68000, loss 29.4937, acc 0.8860\n",
      "\n",
      "2019-05-08T03:08:06.709154: step 68500, loss 29.4232, acc 0.90625\n",
      "2019-05-08T03:08:35.827415: step 69000, loss 29.6007, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:08:37.325211: step 69000, loss 29.4927, acc 0.8867\n",
      "\n",
      "2019-05-08T03:09:06.450304: step 69500, loss 29.388, acc 0.9375\n",
      "2019-05-08T03:09:35.564494: step 70000, loss 29.3623, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:09:37.062893: step 70000, loss 29.4916, acc 0.8866\n",
      "\n",
      "2019-05-08T03:10:06.180590: step 70500, loss 29.456, acc 0.890625\n",
      "2019-05-08T03:10:35.281418: step 71000, loss 29.519, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:10:36.780638: step 71000, loss 29.4908, acc 0.8873\n",
      "\n",
      "2019-05-08T03:11:05.900832: step 71500, loss 29.4478, acc 0.890625\n",
      "2019-05-08T03:11:35.017690: step 72000, loss 29.4555, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:11:36.518220: step 72000, loss 29.4897, acc 0.8871\n",
      "\n",
      "2019-05-08T03:12:05.626544: step 72500, loss 29.5703, acc 0.859375\n",
      "2019-05-08T03:12:34.758820: step 73000, loss 29.5326, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:12:36.265034: step 73000, loss 29.4886, acc 0.8873\n",
      "\n",
      "2019-05-08T03:13:05.378784: step 73500, loss 29.5745, acc 0.890625\n",
      "2019-05-08T03:13:34.491798: step 74000, loss 29.3693, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:13:35.982690: step 74000, loss 29.4876, acc 0.8874\n",
      "\n",
      "2019-05-08T03:14:05.097109: step 74500, loss 29.4996, acc 0.875\n",
      "2019-05-08T03:14:34.189958: step 75000, loss 29.3974, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:14:35.686185: step 75000, loss 29.4863, acc 0.8877\n",
      "\n",
      "2019-05-08T03:15:04.822894: step 75500, loss 29.3855, acc 0.921875\n",
      "2019-05-08T03:15:33.930431: step 76000, loss 29.4654, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:15:35.419876: step 76000, loss 29.4854, acc 0.8873\n",
      "\n",
      "2019-05-08T03:16:04.545328: step 76500, loss 29.658, acc 0.8125\n",
      "2019-05-08T03:16:33.655259: step 77000, loss 29.4744, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:16:35.159364: step 77000, loss 29.4841, acc 0.8873\n",
      "\n",
      "2019-05-08T03:17:04.262501: step 77500, loss 29.3625, acc 0.9375\n",
      "2019-05-08T03:17:33.383326: step 78000, loss 29.508, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:17:34.871222: step 78000, loss 29.4827, acc 0.8876\n",
      "\n",
      "2019-05-08T03:18:03.954384: step 78500, loss 29.4333, acc 0.921875\n",
      "2019-05-08T03:18:33.081168: step 79000, loss 29.5506, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:18:34.580466: step 79000, loss 29.4815, acc 0.8887\n",
      "\n",
      "2019-05-08T03:19:03.709713: step 79500, loss 29.4271, acc 0.875\n",
      "2019-05-08T03:19:32.825975: step 80000, loss 29.3407, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:19:34.323198: step 80000, loss 29.4799, acc 0.8882\n",
      "\n",
      "2019-05-08T03:20:03.440128: step 80500, loss 29.4239, acc 0.96875\n",
      "2019-05-08T03:20:32.568020: step 81000, loss 29.3821, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:20:34.072064: step 81000, loss 29.4785, acc 0.8890\n",
      "\n",
      "2019-05-08T03:21:03.184827: step 81500, loss 29.4706, acc 0.90625\n",
      "2019-05-08T03:21:32.307446: step 82000, loss 29.494, acc 0.90625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-08T03:21:33.805999: step 82000, loss 29.4768, acc 0.8888\n",
      "\n",
      "2019-05-08T03:22:02.938439: step 82500, loss 29.4013, acc 0.90625\n",
      "2019-05-08T03:22:32.053402: step 83000, loss 29.3702, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:22:33.533296: step 83000, loss 29.4745, acc 0.8892\n",
      "\n",
      "2019-05-08T03:23:02.643502: step 83500, loss 29.3344, acc 0.9375\n",
      "2019-05-08T03:23:31.760291: step 84000, loss 29.435, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:23:33.262918: step 84000, loss 29.4722, acc 0.8888\n",
      "\n",
      "2019-05-08T03:24:02.385783: step 84500, loss 29.584, acc 0.859375\n",
      "2019-05-08T03:24:31.490528: step 85000, loss 29.4631, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:24:32.998551: step 85000, loss 29.4693, acc 0.8890\n",
      "\n",
      "2019-05-08T03:25:02.100001: step 85500, loss 29.3227, acc 0.953125\n",
      "2019-05-08T03:25:31.206775: step 86000, loss 29.4638, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:25:32.695911: step 86000, loss 29.4659, acc 0.8889\n",
      "\n",
      "2019-05-08T03:26:01.817551: step 86500, loss 29.6391, acc 0.765625\n",
      "2019-05-08T03:26:30.911506: step 87000, loss 29.3735, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:26:32.417966: step 87000, loss 29.4611, acc 0.8895\n",
      "\n",
      "2019-05-08T03:27:01.533039: step 87500, loss 29.4124, acc 0.90625\n",
      "2019-05-08T03:27:30.634252: step 88000, loss 29.4275, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:27:32.120624: step 88000, loss 29.4546, acc 0.8894\n",
      "\n",
      "2019-05-08T03:28:01.242881: step 88500, loss 29.3421, acc 0.953125\n",
      "2019-05-08T03:28:30.352283: step 89000, loss 29.3698, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:28:31.853965: step 89000, loss 29.4455, acc 0.8890\n",
      "\n",
      "2019-05-08T03:29:00.977964: step 89500, loss 29.3969, acc 0.890625\n",
      "2019-05-08T03:29:30.115392: step 90000, loss 29.398, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:29:31.614685: step 90000, loss 29.43, acc 0.8892\n",
      "\n",
      "2019-05-08T03:30:00.728499: step 90500, loss 29.3364, acc 0.875\n",
      "2019-05-08T03:30:29.853182: step 91000, loss 29.409, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:30:31.342404: step 91000, loss 29.4014, acc 0.8892\n",
      "\n",
      "2019-05-08T03:31:00.456974: step 91500, loss 29.3643, acc 0.890625\n",
      "2019-05-08T03:31:29.566734: step 92000, loss 29.5141, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:31:31.066948: step 92000, loss 29.3452, acc 0.8882\n",
      "\n",
      "2019-05-08T03:32:00.194468: step 92500, loss 29.3128, acc 0.921875\n",
      "2019-05-08T03:32:29.328416: step 93000, loss 29.2258, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:32:30.829025: step 93000, loss 29.2377, acc 0.8874\n",
      "\n",
      "2019-05-08T03:32:59.944451: step 93500, loss 29.2772, acc 0.921875\n",
      "2019-05-08T03:33:29.066176: step 94000, loss 29.091, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:33:30.579805: step 94000, loss 29.0804, acc 0.8839\n",
      "\n",
      "2019-05-08T03:33:59.714366: step 94500, loss 28.9263, acc 0.875\n",
      "2019-05-08T03:34:28.817967: step 95000, loss 28.8818, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:34:30.310635: step 95000, loss 28.9175, acc 0.8829\n",
      "\n",
      "2019-05-08T03:34:59.437820: step 95500, loss 28.9064, acc 0.875\n",
      "2019-05-08T03:35:28.546776: step 96000, loss 28.5712, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:35:30.059887: step 96000, loss 28.7841, acc 0.8848\n",
      "\n",
      "2019-05-08T03:35:59.185226: step 96500, loss 28.8772, acc 0.84375\n",
      "2019-05-08T03:36:28.302382: step 97000, loss 28.6828, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:36:29.807534: step 97000, loss 28.6934, acc 0.8846\n",
      "\n",
      "2019-05-08T03:36:58.942068: step 97500, loss 28.8386, acc 0.84375\n",
      "2019-05-08T03:37:28.044000: step 98000, loss 28.7217, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:37:29.545350: step 98000, loss 28.6317, acc 0.8840\n",
      "\n",
      "2019-05-08T03:37:58.680574: step 98500, loss 28.5719, acc 0.875\n",
      "2019-05-08T03:38:27.794112: step 99000, loss 28.4326, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:38:29.297392: step 99000, loss 28.5882, acc 0.8831\n",
      "\n",
      "2019-05-08T03:38:58.410576: step 99500, loss 28.7401, acc 0.765625\n",
      "2019-05-08T03:39:27.534894: step 100000, loss 28.5411, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:39:29.022543: step 100000, loss 28.5536, acc 0.8820\n",
      "\n",
      "2019-05-08T03:39:58.126533: step 100500, loss 28.3254, acc 0.9375\n",
      "2019-05-08T03:40:27.230284: step 101000, loss 28.5903, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:40:28.710019: step 101000, loss 28.5241, acc 0.8822\n",
      "\n",
      "2019-05-08T03:40:57.833684: step 101500, loss 28.4691, acc 0.84375\n",
      "2019-05-08T03:41:26.941048: step 102000, loss 28.46, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:41:28.430635: step 102000, loss 28.4992, acc 0.8823\n",
      "\n",
      "2019-05-08T03:41:57.541717: step 102500, loss 28.4774, acc 0.890625\n",
      "2019-05-08T03:42:26.671864: step 103000, loss 28.5223, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:42:28.171769: step 103000, loss 28.4769, acc 0.8827\n",
      "\n",
      "2019-05-08T03:42:57.260731: step 103500, loss 28.5043, acc 0.90625\n",
      "2019-05-08T03:43:26.364157: step 104000, loss 28.3793, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:43:27.855654: step 104000, loss 28.4557, acc 0.8830\n",
      "\n",
      "2019-05-08T03:43:56.974756: step 104500, loss 28.3061, acc 0.96875\n",
      "2019-05-08T03:44:26.091813: step 105000, loss 28.4171, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:44:27.591107: step 105000, loss 28.4343, acc 0.8829\n",
      "\n",
      "2019-05-08T03:44:56.706685: step 105500, loss 28.5891, acc 0.796875\n",
      "2019-05-08T03:45:25.849252: step 106000, loss 28.3103, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:45:27.351904: step 106000, loss 28.4102, acc 0.8835\n",
      "\n",
      "2019-05-08T03:45:56.453680: step 106500, loss 28.2878, acc 0.859375\n",
      "2019-05-08T03:46:25.575137: step 107000, loss 28.3129, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:46:27.066482: step 107000, loss 28.3837, acc 0.8834\n",
      "\n",
      "2019-05-08T03:46:56.190160: step 107500, loss 28.26, acc 0.890625\n",
      "2019-05-08T03:47:25.313407: step 108000, loss 28.3779, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:47:26.815542: step 108000, loss 28.3515, acc 0.8839\n",
      "\n",
      "2019-05-08T03:47:55.959458: step 108500, loss 28.3336, acc 0.859375\n",
      "2019-05-08T03:48:25.080050: step 109000, loss 28.2222, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:48:26.598506: step 109000, loss 28.3101, acc 0.8823\n",
      "\n",
      "2019-05-08T03:48:55.710127: step 109500, loss 28.1928, acc 0.90625\n",
      "2019-05-08T03:49:24.818272: step 110000, loss 28.2729, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:49:26.312071: step 110000, loss 28.2574, acc 0.8817\n",
      "\n",
      "2019-05-08T03:49:55.451730: step 110500, loss 28.2493, acc 0.875\n",
      "2019-05-08T03:50:24.588032: step 111000, loss 28.1439, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:50:26.094188: step 111000, loss 28.1932, acc 0.8823\n",
      "\n",
      "2019-05-08T03:50:55.197396: step 111500, loss 28.2607, acc 0.890625\n",
      "2019-05-08T03:51:24.323274: step 112000, loss 28.0047, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:51:25.818766: step 112000, loss 28.1235, acc 0.8806\n",
      "\n",
      "2019-05-08T03:51:54.955591: step 112500, loss 28.1063, acc 0.875\n",
      "2019-05-08T03:52:24.069910: step 113000, loss 27.8435, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:52:25.588265: step 113000, loss 28.0562, acc 0.8817\n",
      "\n",
      "2019-05-08T03:52:54.696532: step 113500, loss 28.0475, acc 0.875\n",
      "2019-05-08T03:53:23.791816: step 114000, loss 27.9124, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:53:25.276930: step 114000, loss 27.9956, acc 0.8818\n",
      "\n",
      "2019-05-08T03:53:54.386122: step 114500, loss 27.8973, acc 0.90625\n",
      "2019-05-08T03:54:23.505190: step 115000, loss 27.8318, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:54:25.005308: step 115000, loss 27.943, acc 0.8821\n",
      "\n",
      "2019-05-08T03:54:54.138920: step 115500, loss 27.8389, acc 0.890625\n",
      "2019-05-08T03:55:23.260404: step 116000, loss 27.804, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:55:24.753758: step 116000, loss 27.8987, acc 0.8818\n",
      "\n",
      "2019-05-08T03:55:53.880232: step 116500, loss 27.8045, acc 0.9375\n",
      "2019-05-08T03:56:22.998028: step 117000, loss 27.7894, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:56:24.498026: step 117000, loss 27.8603, acc 0.8824\n",
      "\n",
      "2019-05-08T03:56:53.625162: step 117500, loss 27.8749, acc 0.859375\n",
      "2019-05-08T03:57:22.736644: step 118000, loss 27.8587, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:57:24.249242: step 118000, loss 27.8262, acc 0.8816\n",
      "\n",
      "2019-05-08T03:59:22.275771: step 120000, loss 27.8451, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T03:59:23.768332: step 120000, loss 27.7651, acc 0.8827\n",
      "\n",
      "2019-05-08T03:59:52.900983: step 120500, loss 27.7644, acc 0.875\n",
      "2019-05-08T04:00:22.021390: step 121000, loss 27.6525, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:00:23.515761: step 121000, loss 27.7361, acc 0.8826\n",
      "\n",
      "2019-05-08T04:00:52.612203: step 121500, loss 27.637, acc 0.875\n",
      "2019-05-08T04:01:21.733640: step 122000, loss 27.5355, acc 0.953125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-08T04:01:23.238976: step 122000, loss 27.709, acc 0.8843\n",
      "\n",
      "2019-05-08T04:01:52.361120: step 122500, loss 27.7287, acc 0.90625\n",
      "2019-05-08T04:02:21.455945: step 123000, loss 27.8605, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:02:22.983602: step 123000, loss 27.6808, acc 0.8838\n",
      "\n",
      "2019-05-08T04:02:52.117607: step 123500, loss 27.4897, acc 0.90625\n",
      "2019-05-08T04:03:21.243906: step 124000, loss 27.6135, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:03:22.750336: step 124000, loss 27.6532, acc 0.8839\n",
      "\n",
      "2019-05-08T04:03:51.867588: step 124500, loss 27.4982, acc 0.921875\n",
      "2019-05-08T04:04:20.999228: step 125000, loss 27.619, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:04:22.498024: step 125000, loss 27.6276, acc 0.8848\n",
      "\n",
      "2019-05-08T04:04:51.618354: step 125500, loss 27.7777, acc 0.765625\n",
      "2019-05-08T04:05:20.727951: step 126000, loss 27.3948, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:05:22.225073: step 126000, loss 27.5994, acc 0.8849\n",
      "\n",
      "2019-05-08T04:05:51.338707: step 126500, loss 27.5812, acc 0.90625\n",
      "2019-05-08T04:06:20.444124: step 127000, loss 27.5246, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:06:21.941869: step 127000, loss 27.5719, acc 0.8847\n",
      "\n",
      "2019-05-08T04:06:51.075335: step 127500, loss 27.6065, acc 0.890625\n",
      "2019-05-08T04:07:20.193607: step 128000, loss 27.4103, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:07:21.700477: step 128000, loss 27.5429, acc 0.8863\n",
      "\n",
      "2019-05-08T04:07:50.811797: step 128500, loss 27.4032, acc 0.9375\n",
      "2019-05-08T04:08:19.922128: step 129000, loss 27.4762, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:08:21.427623: step 129000, loss 27.5126, acc 0.8862\n",
      "\n",
      "2019-05-08T04:08:50.531059: step 129500, loss 27.4064, acc 0.96875\n",
      "2019-05-08T04:09:19.656248: step 130000, loss 27.4606, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:09:21.157154: step 130000, loss 27.4807, acc 0.8871\n",
      "\n",
      "2019-05-08T04:09:50.265384: step 130500, loss 27.5301, acc 0.890625\n",
      "2019-05-08T04:10:19.361169: step 131000, loss 27.2907, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:10:20.870320: step 131000, loss 27.4469, acc 0.8885\n",
      "\n",
      "2019-05-08T04:10:49.985741: step 131500, loss 27.382, acc 0.9375\n",
      "2019-05-08T04:11:19.104002: step 132000, loss 27.3673, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:11:20.623594: step 132000, loss 27.4094, acc 0.8880\n",
      "\n",
      "2019-05-08T04:11:49.748966: step 132500, loss 27.3973, acc 0.796875\n",
      "2019-05-08T04:12:18.859074: step 133000, loss 27.6178, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:12:20.360098: step 133000, loss 27.3695, acc 0.8874\n",
      "\n",
      "2019-05-08T04:12:49.485332: step 133500, loss 27.2026, acc 0.90625\n",
      "2019-05-08T04:13:18.608277: step 134000, loss 27.5153, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:13:20.121590: step 134000, loss 27.3254, acc 0.8883\n",
      "\n",
      "2019-05-08T04:13:49.245407: step 134500, loss 27.194, acc 0.96875\n",
      "2019-05-08T04:14:18.345117: step 135000, loss 27.4726, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:14:19.834424: step 135000, loss 27.2766, acc 0.8887\n",
      "\n",
      "2019-05-08T04:14:48.958838: step 135500, loss 27.1086, acc 0.953125\n",
      "2019-05-08T04:15:18.057985: step 136000, loss 27.2105, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:15:19.544056: step 136000, loss 27.2231, acc 0.8898\n",
      "\n",
      "2019-05-08T04:15:48.674268: step 136500, loss 27.188, acc 0.890625\n",
      "2019-05-08T04:16:17.802194: step 137000, loss 27.2286, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:16:19.290944: step 137000, loss 27.165, acc 0.8902\n",
      "\n",
      "2019-05-08T04:16:48.411041: step 137500, loss 27.0426, acc 0.9375\n",
      "2019-05-08T04:17:17.546464: step 138000, loss 27.197, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:17:19.027409: step 138000, loss 27.1014, acc 0.8888\n",
      "\n",
      "2019-05-08T04:17:48.161401: step 138500, loss 27.1641, acc 0.890625\n",
      "2019-05-08T04:18:17.273518: step 139000, loss 27.0375, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:18:18.771758: step 139000, loss 27.0343, acc 0.8897\n",
      "\n",
      "2019-05-08T04:18:47.900312: step 139500, loss 26.9474, acc 0.90625\n",
      "2019-05-08T04:19:17.010127: step 140000, loss 26.9066, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:19:18.505477: step 140000, loss 26.9616, acc 0.8906\n",
      "\n",
      "2019-05-08T04:19:47.610848: step 140500, loss 26.9209, acc 0.921875\n",
      "2019-05-08T04:20:16.718612: step 141000, loss 26.7954, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:20:18.219434: step 141000, loss 26.8875, acc 0.8898\n",
      "\n",
      "2019-05-08T04:20:47.347439: step 141500, loss 26.7794, acc 0.890625\n",
      "2019-05-08T04:21:16.487155: step 142000, loss 27.0756, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:21:17.977216: step 142000, loss 26.813, acc 0.8902\n",
      "\n",
      "2019-05-08T04:21:47.108667: step 142500, loss 26.745, acc 0.90625\n",
      "2019-05-08T04:22:16.224281: step 143000, loss 26.6847, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:22:17.713467: step 143000, loss 26.7395, acc 0.8909\n",
      "\n",
      "2019-05-08T04:22:46.842271: step 143500, loss 26.5644, acc 0.921875\n",
      "2019-05-08T04:23:15.952566: step 144000, loss 26.5423, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:23:17.446873: step 144000, loss 26.6637, acc 0.8912\n",
      "\n",
      "2019-05-08T04:23:46.586423: step 144500, loss 26.5454, acc 0.9375\n",
      "2019-05-08T04:24:15.717934: step 145000, loss 26.5957, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:24:17.202782: step 145000, loss 26.5869, acc 0.8907\n",
      "\n",
      "2019-05-08T04:24:46.332031: step 145500, loss 26.908, acc 0.8125\n",
      "2019-05-08T04:25:15.439846: step 146000, loss 26.4535, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:25:16.929140: step 146000, loss 26.5061, acc 0.8909\n",
      "\n",
      "2019-05-08T04:25:46.051543: step 146500, loss 26.6535, acc 0.828125\n",
      "2019-05-08T04:26:15.182055: step 147000, loss 26.5229, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:26:16.678636: step 147000, loss 26.4206, acc 0.8899\n",
      "\n",
      "2019-05-08T04:26:45.807265: step 147500, loss 26.3734, acc 0.90625\n",
      "2019-05-08T04:27:14.929513: step 148000, loss 26.3258, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:27:16.429976: step 148000, loss 26.3293, acc 0.8913\n",
      "\n",
      "2019-05-08T04:27:45.551684: step 148500, loss 26.2664, acc 0.9375\n",
      "2019-05-08T04:28:14.695700: step 149000, loss 26.2459, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:28:16.191103: step 149000, loss 26.2306, acc 0.8903\n",
      "\n",
      "2019-05-08T04:28:45.321675: step 149500, loss 26.0472, acc 0.921875\n",
      "2019-05-08T04:29:14.455270: step 150000, loss 26.043, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:29:15.950269: step 150000, loss 26.1264, acc 0.8918\n",
      "\n",
      "2019-05-08T04:29:45.075055: step 150500, loss 26.215, acc 0.90625\n",
      "2019-05-08T04:30:14.211686: step 151000, loss 26.0277, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:30:15.703926: step 151000, loss 26.0117, acc 0.8902\n",
      "\n",
      "2019-05-08T04:30:44.826973: step 151500, loss 25.8766, acc 0.9375\n",
      "2019-05-08T04:31:13.929413: step 152000, loss 25.8616, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:31:15.428839: step 152000, loss 25.8982, acc 0.8891\n",
      "\n",
      "2019-05-08T04:31:44.547143: step 152500, loss 25.7428, acc 0.90625\n",
      "2019-05-08T04:32:13.664091: step 153000, loss 25.5437, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:32:15.162634: step 153000, loss 25.7812, acc 0.8900\n",
      "\n",
      "2019-05-08T04:32:44.276769: step 153500, loss 25.6349, acc 0.921875\n",
      "2019-05-08T04:33:13.388879: step 154000, loss 25.6183, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:33:14.892779: step 154000, loss 25.6591, acc 0.8900\n",
      "\n",
      "2019-05-08T04:33:44.013673: step 154500, loss 25.4375, acc 0.953125\n",
      "2019-05-08T04:34:13.108266: step 155000, loss 25.6028, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:34:14.598419: step 155000, loss 25.5282, acc 0.8902\n",
      "\n",
      "2019-05-08T04:34:43.727082: step 155500, loss 25.3848, acc 0.9375\n",
      "2019-05-08T04:35:12.825894: step 156000, loss 25.4232, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:35:14.322855: step 156000, loss 25.3887, acc 0.8889\n",
      "\n",
      "2019-05-08T04:35:43.457103: step 156500, loss 25.1419, acc 0.9375\n",
      "2019-05-08T04:36:12.573917: step 157000, loss 25.2967, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:36:14.073521: step 157000, loss 25.2358, acc 0.8883\n",
      "\n",
      "2019-05-08T04:36:43.191818: step 157500, loss 25.2185, acc 0.8125\n",
      "2019-05-08T04:37:12.325816: step 158000, loss 24.9801, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:37:13.822061: step 158000, loss 25.0659, acc 0.8875\n",
      "\n",
      "2019-05-08T04:37:42.946235: step 158500, loss 24.9894, acc 0.921875\n",
      "2019-05-08T04:38:12.054772: step 159000, loss 24.6612, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:38:13.554263: step 159000, loss 24.8745, acc 0.8873\n",
      "\n",
      "2019-05-08T04:38:42.678526: step 159500, loss 24.8785, acc 0.875\n",
      "2019-05-08T04:39:11.796646: step 160000, loss 24.73, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:39:13.286673: step 160000, loss 24.6651, acc 0.8857\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-08T04:39:42.407937: step 160500, loss 24.5367, acc 0.875\n",
      "2019-05-08T04:40:11.533353: step 161000, loss 24.5078, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:40:13.048555: step 161000, loss 24.4375, acc 0.8860\n",
      "\n",
      "2019-05-08T04:40:42.164108: step 161500, loss 24.2309, acc 0.9375\n",
      "2019-05-08T04:41:11.302693: step 162000, loss 24.1572, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:41:12.794676: step 162000, loss 24.1974, acc 0.8840\n",
      "\n",
      "2019-05-08T04:41:41.912345: step 162500, loss 24.0368, acc 0.875\n",
      "2019-05-08T04:42:11.028818: step 163000, loss 23.9437, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:42:12.543906: step 163000, loss 23.946, acc 0.8840\n",
      "\n",
      "2019-05-08T04:42:41.668303: step 163500, loss 23.8686, acc 0.90625\n",
      "2019-05-08T04:43:10.791583: step 164000, loss 23.6363, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:43:12.283879: step 164000, loss 23.6849, acc 0.8839\n",
      "\n",
      "2019-05-08T04:43:41.398538: step 164500, loss 23.7559, acc 0.859375\n",
      "2019-05-08T04:44:10.537587: step 165000, loss 23.1917, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:44:12.040195: step 165000, loss 23.4121, acc 0.8838\n",
      "\n",
      "2019-05-08T04:44:41.156320: step 165500, loss 23.4216, acc 0.828125\n",
      "2019-05-08T04:45:10.264252: step 166000, loss 22.9985, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:45:11.770085: step 166000, loss 23.1214, acc 0.8847\n",
      "\n",
      "2019-05-08T04:45:40.892396: step 166500, loss 22.8835, acc 0.890625\n",
      "2019-05-08T04:46:10.021772: step 167000, loss 22.856, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:46:11.530329: step 167000, loss 22.8079, acc 0.8838\n",
      "\n",
      "2019-05-08T04:46:40.663266: step 167500, loss 22.6449, acc 0.875\n",
      "2019-05-08T04:47:09.791829: step 168000, loss 22.3175, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2019-05-08T04:47:11.290734: step 168000, loss 22.4741, acc 0.8846\n",
      "\n",
      "2019-05-08T04:47:40.404351: step 168500, loss 22.2558, acc 0.921875\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = SelfAttention(\n",
    "        sequence_length=x_train.shape[1],\n",
    "        num_classes=y_train.shape[1],\n",
    "        vocab_size=len(vocab_processor.vocabulary_),\n",
    "        embedding_size=Config.embedding_dim,\n",
    "        hidden_size=Config.hidden_size,\n",
    "        d_a_size=Config.d_a_size,\n",
    "        r_size=Config.r_size,\n",
    "        fc_size=Config.fc_size,\n",
    "        p_coef=Config.p_coef\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.AdadeltaOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"28.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", model.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", model.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=Config.num_checkpoints)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    pretrain_W = load_word2vec(Config.embedding_dim, vocab_processor)\n",
    "    sess.run(model.W_text.assign(pretrain_W))\n",
    "    print(\"Success to load pre-trained word2vec model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(list(zip(x_train, y_train)), Config.batch_size,Config.num_epochs)\n",
    "    \n",
    "    # Training loop. For each batch...\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        # Train\n",
    "        feed_dict = {\n",
    "            model.input_text: x_batch,\n",
    "            model.input_y: y_batch,\n",
    "        }\n",
    "        _, step, summaries, loss, accuracy = sess.run(\n",
    "            [train_op, global_step, train_summary_op, model.loss, model.accuracy], feed_dict)\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            feed_dict = {\n",
    "                model.input_text: x_dev,\n",
    "                model.input_y: y_dev,\n",
    "            }\n",
    "            summaries, loss, accuracy, predictions = sess.run(\n",
    "                [dev_summary_op, model.loss, model.accuracy, model.predictions], feed_dict)\n",
    "            dev_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:.4f}\\n\".format(time_str, step, loss, accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./28.runs --host 0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
