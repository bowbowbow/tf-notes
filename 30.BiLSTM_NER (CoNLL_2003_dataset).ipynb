{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data loading params\n",
    "    max_sentence_length = 40\n",
    "    dev_sample_percentage = 0.1\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_dim = 300\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size = 300\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 40\n",
    "    num_epochs = 20\n",
    "    display_every = 200\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"O\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Load annotated corpus for named entity recognition\n",
    "\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-ORG O I-MISC O O O I-MISC O O</td>\n",
       "      <td>eu rejects german call to boycott british lamb .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-PER I-PER</td>\n",
       "      <td>peter blackburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-LOC O</td>\n",
       "      <td>brussels 1996-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O I-ORG I-ORG O O O O O O I-MISC O O O O O I-M...</td>\n",
       "      <td>the european commission said on thursday it di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-LOC O O O O I-ORG I-ORG O O O I-PER I-PER O ...</td>\n",
       "      <td>germany 's representative to the european unio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O I-ORG ...</td>\n",
       "      <td>\" we do n't support any such recommendation be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O I-...</td>\n",
       "      <td>he said further scientific study was required ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O O O O O O O I-ORG O O I-PER I-PER O O O O O ...</td>\n",
       "      <td>he said a proposal last month by eu farm commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PER O I-MISC O O O O I-LOC O I-LOC O O O O O...</td>\n",
       "      <td>fischler proposed eu-wide measures after repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O I-PER O O O O O O O I-ORG O O O O O O O O O ...</td>\n",
       "      <td>but fischler agreed to review his proposal aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0                    I-ORG O I-MISC O O O I-MISC O O   \n",
       "1                                        I-PER I-PER   \n",
       "2                                            I-LOC O   \n",
       "3  O I-ORG I-ORG O O O O O O I-MISC O O O O O I-M...   \n",
       "4  I-LOC O O O O I-ORG I-ORG O O O I-PER I-PER O ...   \n",
       "5  O O O O O O O O O O O O O O O O O O O O I-ORG ...   \n",
       "6  O O O O O O O O O O O O O O O O O O O O O O I-...   \n",
       "7  O O O O O O O I-ORG O O I-PER I-PER O O O O O ...   \n",
       "8  I-PER O I-MISC O O O O I-LOC O I-LOC O O O O O...   \n",
       "9  O I-PER O O O O O O O I-ORG O O O O O O O O O ...   \n",
       "\n",
       "                                               words  \n",
       "0   eu rejects german call to boycott british lamb .  \n",
       "1                                    peter blackburn  \n",
       "2                                brussels 1996-08-22  \n",
       "3  the european commission said on thursday it di...  \n",
       "4  germany 's representative to the european unio...  \n",
       "5  \" we do n't support any such recommendation be...  \n",
       "6  he said further scientific study was required ...  \n",
       "7  he said a proposal last month by eu farm commi...  \n",
       "8  fischler proposed eu-wide measures after repor...  \n",
       "9  but fischler agreed to review his proposal aft...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        words_col, tags_col = [], []\n",
    "        with open(path) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(words) != 0:\n",
    "                        words_col.append(' '.join(words))\n",
    "                        tags_col.append(' '.join(tags))\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, tag = ls[0],ls[3]\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_tags.append(tag)\n",
    "                    \n",
    "        return pd.DataFrame({'words': words_col, 'tags': tags_col})\n",
    "        \n",
    "    def download_and_load_datasets(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"CoNLL-2003.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/CoNLL-2003.zip\", \n",
    "          extract=True)\n",
    "        \n",
    "        dir_path = os.path.join(os.path.dirname(dataset), 'CoNLL-2003')\n",
    "        train_df = self.load_dataset(os.path.join(dir_path, 'eng.train'))\n",
    "        test_df = self.load_dataset(os.path.join(dir_path, 'eng.testb'))\n",
    "        return train_df, test_df\n",
    "\n",
    "dataset = Dataset()\n",
    "train_df, test_df = dataset.download_and_load_datasets()\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "x, lengths, y = [], [], []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    sentence = row['words'].split(' ')\n",
    "    tags = row['tags'].split(' ')\n",
    "    \n",
    "    sentence = sentence[:Config.max_sentence_length]\n",
    "    tags = tags[:Config.max_sentence_length]\n",
    "    \n",
    "    lengths.append(Config.max_sentence_length)\n",
    "    x_row, y_row = [], []\n",
    "    for word in sentence:\n",
    "        x_row.append(word2idx[word])\n",
    "    for tag in tags:\n",
    "        y_row.append(tag2idx[tag])\n",
    "    \n",
    "    if len(sentence) < Config.max_sentence_length:\n",
    "        lengths[-1] = len(sentence)\n",
    "        x_row += [word2idx[Config.PAD]]* (Config.max_sentence_length - len(sentence))\n",
    "        y_row += [tag2idx[Config.NONE]]* (Config.max_sentence_length - len(sentence))\n",
    "        \n",
    "    x.append(x_row)\n",
    "    y.append(y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 12637/1404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "# Randomly shuffle data to split into train and dev\n",
    "np.random.seed(11)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "x_shuffled, lengths_shuffled, y_shuffled = x[shuffle_indices], lengths[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "# Split train/dev set\n",
    "dev_sample_index = -1 * int(Config.dev_sample_percentage*float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:] \n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:] \n",
    "lengths_train, lengths_dev = lengths_shuffled[:dev_sample_index], lengths_shuffled[dev_sample_index:]\n",
    "\n",
    "print(\"Train/Dev split: {:d}/{:d}\\n\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               sequence_length, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               embedding_size, \n",
    "               hidden_size):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_y')\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('text-embedding'):\n",
    "            self.W_text = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -0.25, 0.25), name='W_text')\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_x) # [batch_size, sequence_length, embedding_size]\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "            self.rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.embedded_chars,\n",
    "                                                                  sequence_length= self._length(self.input_x), # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([self.rnn_outputs[0], self.rnn_outputs[1]], axis=2) # [batch_size, sequence_length, 2*hidden_size]\n",
    "            \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, sequence_length, num_classes]) # [batch_size, sequence_length, num_classes]\n",
    "            \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            mask = tf.sequence_mask(self.sequence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "        # Accuracy    \n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.predictions = tf.argmax(self.logits, 2, name='predictions')\n",
    "            correct_predictions = tf.equal(self.predictions, tf.cast(self.input_y, tf.int64))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-d8d7153872c7>:22: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-d8d7153872c7>:28: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/30.runs/1557932535\n",
      "\n",
      "2019-05-16T00:02:23.380068: step 200, loss 2.06631, acc 0.0525\n",
      "2019-05-16T00:02:30.399414: step 400, loss 2.05272, acc 0.17375\n",
      "2019-05-16T00:02:37.419910: step 600, loss 1.91376, acc 0.721875\n",
      "2019-05-16T00:02:44.428180: step 800, loss 1.8607, acc 0.861875\n",
      "2019-05-16T00:02:51.423989: step 1000, loss 1.95301, acc 0.905625\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         1\n",
      "      B-MISC       0.00      0.25      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.02      0.01      0.01       828\n",
      "      I-MISC       0.02      0.03      0.03       451\n",
      "       I-ORG       0.07      0.04      0.05      1056\n",
      "       I-PER       0.06      0.02      0.03      1045\n",
      "           O       0.95      0.93      0.94     52775\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     56160\n",
      "   macro avg       0.14      0.16      0.13     56160\n",
      "weighted avg       0.89      0.87      0.88     56160\n",
      "\n",
      "2019-05-16T00:02:51.887222: step 1000, loss 1.96696, acc 0.8725\n",
      "\n",
      "2019-05-16T00:02:58.910487: step 1200, loss 1.94205, acc 0.87875\n",
      "2019-05-16T00:03:05.921413: step 1400, loss 1.64166, acc 0.91\n",
      "2019-05-16T00:03:12.945419: step 1600, loss 1.66449, acc 0.896875\n",
      "2019-05-16T00:03:19.972608: step 1800, loss 1.79189, acc 0.920625\n",
      "2019-05-16T00:03:27.000608: step 2000, loss 1.53864, acc 0.928125\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         1\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.07      0.00      0.00       828\n",
      "      I-MISC       0.02      0.00      0.00       451\n",
      "       I-ORG       0.05      0.00      0.01      1056\n",
      "       I-PER       0.00      0.00      0.00      1045\n",
      "           O       0.94      0.99      0.97     52775\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     56160\n",
      "   macro avg       0.14      0.13      0.12     56160\n",
      "weighted avg       0.89      0.93      0.91     56160\n",
      "\n",
      "2019-05-16T00:03:27.364715: step 2000, loss 1.5735, acc 0.9344\n",
      "\n",
      "2019-05-16T00:03:34.394368: step 2200, loss 1.48999, acc 0.906875\n",
      "2019-05-16T00:03:41.425044: step 2400, loss 0.998195, acc 0.926875\n",
      "2019-05-16T00:03:48.438982: step 2600, loss 0.994388, acc 0.9375\n",
      "2019-05-16T00:03:55.469343: step 2800, loss 1.06388, acc 0.948125\n",
      "2019-05-16T00:04:02.490910: step 3000, loss 1.02004, acc 0.944375\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         1\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.00      0.00      0.00       828\n",
      "      I-MISC       0.00      0.00      0.00       451\n",
      "       I-ORG       0.06      0.00      0.00      1056\n",
      "       I-PER       0.00      0.00      0.00      1045\n",
      "           O       0.94      1.00      0.97     52775\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56160\n",
      "   macro avg       0.12      0.13      0.12     56160\n",
      "weighted avg       0.88      0.94      0.91     56160\n",
      "\n",
      "2019-05-16T00:04:02.848608: step 3000, loss 0.934148, acc 0.9389\n",
      "\n",
      "2019-05-16T00:04:09.875959: step 3200, loss 0.937411, acc 0.94375\n",
      "2019-05-16T00:04:16.894949: step 3400, loss 0.992391, acc 0.931875\n",
      "2019-05-16T00:04:23.923759: step 3600, loss 1.0096, acc 0.93\n",
      "2019-05-16T00:04:30.943808: step 3800, loss 0.672247, acc 0.93\n",
      "2019-05-16T00:04:37.967588: step 4000, loss 0.886904, acc 0.929375\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         1\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.00      0.00      0.00       828\n",
      "      I-MISC       0.00      0.00      0.00       451\n",
      "       I-ORG       0.00      0.00      0.00      1056\n",
      "       I-PER       0.00      0.00      0.00      1045\n",
      "           O       0.94      1.00      0.97     52775\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56160\n",
      "   macro avg       0.12      0.12      0.12     56160\n",
      "weighted avg       0.88      0.94      0.91     56160\n",
      "\n",
      "2019-05-16T00:04:38.345290: step 4000, loss 0.872785, acc 0.9394\n",
      "\n",
      "2019-05-16T00:04:45.367465: step 4200, loss 0.782565, acc 0.949375\n",
      "2019-05-16T00:04:52.399355: step 4400, loss 0.65894, acc 0.9375\n",
      "2019-05-16T00:04:59.430117: step 4600, loss 0.798701, acc 0.948125\n",
      "2019-05-16T00:05:06.450457: step 4800, loss 0.948015, acc 0.93375\n",
      "2019-05-16T00:05:13.462846: step 5000, loss 0.570328, acc 0.928125\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         1\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.00      0.00      0.00       828\n",
      "      I-MISC       0.00      0.00      0.00       451\n",
      "       I-ORG       0.00      0.00      0.00      1056\n",
      "       I-PER       0.00      0.00      0.00      1045\n",
      "           O       0.94      1.00      0.97     52775\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56160\n",
      "   macro avg       0.12      0.12      0.12     56160\n",
      "weighted avg       0.88      0.94      0.91     56160\n",
      "\n",
      "2019-05-16T00:05:13.836673: step 5000, loss 0.840884, acc 0.9395\n",
      "\n",
      "2019-05-16T00:05:20.865548: step 5200, loss 0.497096, acc 0.9575\n",
      "2019-05-16T00:05:27.879015: step 5400, loss 0.904952, acc 0.913125\n",
      "2019-05-16T00:05:34.903725: step 5600, loss 0.602273, acc 0.934375\n",
      "2019-05-16T00:05:41.919660: step 5800, loss 0.678024, acc 0.949375\n",
      "2019-05-16T00:05:48.938739: step 6000, loss 0.819424, acc 0.955\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         1\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.00      0.00      0.00       828\n",
      "      I-MISC       0.00      0.00      0.00       451\n",
      "       I-ORG       0.00      0.00      0.00      1056\n",
      "       I-PER       0.00      0.00      0.00      1045\n",
      "           O       0.94      1.00      0.97     52775\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     56160\n",
      "   macro avg       0.13      0.14      0.14     56160\n",
      "weighted avg       0.88      0.94      0.91     56160\n",
      "\n",
      "2019-05-16T00:05:49.308114: step 6000, loss 0.815848, acc 0.9396\n",
      "\n",
      "2019-05-16T00:05:56.343752: step 6200, loss 0.816262, acc 0.94\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        sequence_length=x_train.shape[1],\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        embedding_size=Config.embedding_dim,\n",
    "        hidden_size=Config.hidden_size\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.AdadeltaOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"30.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     pretrain_W = load_glove(Config.embedding_dim, word2idx)\n",
    "#     sess.run(model.W_text.assign(pretrain_W))\n",
    "#     print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(list(zip(x_train, lengths_train, y_train)), Config.batch_size, Config.num_epochs)\n",
    "    \n",
    "    for batch in batches:\n",
    "        x_batch, lengths_train, y_batch = zip(*batch)\n",
    "\n",
    "        # Train\n",
    "        feed_dict = {\n",
    "            model.input_x: x_batch,\n",
    "            model.input_y: y_batch,\n",
    "            model.sequence_lengths: lengths_train,\n",
    "        }\n",
    "\n",
    "        _, step, loss, accuracy = sess.run(\n",
    "            [train_op, global_step, model.loss, model.accuracy], feed_dict)\n",
    "\n",
    "\n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            feed_dict = {\n",
    "                model.input_x: x_dev,\n",
    "                model.input_y: y_dev,\n",
    "                model.sequence_lengths: lengths_dev,\n",
    "            }\n",
    "            loss, accuracy, predictions = sess.run(\n",
    "                [model.loss, model.accuracy, model.predictions], feed_dict)\n",
    "\n",
    "            from sklearn.metrics import classification_report\n",
    "            \n",
    "            print(classification_report(\n",
    "                [idx2tag[y] for y in np.array(y_dev).flatten()], \n",
    "                [idx2tag[y] for y in np.array(predictions).flatten()]))\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:.4f}\\n\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./30.runs --host 0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
