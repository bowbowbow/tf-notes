{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Embeddings\n",
    "    word_embedding_dim = 128\n",
    "    char_embedding_dim = 128\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size_word = 128\n",
    "    hidden_size_char = 128\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 20\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    \n",
    "    learning_rate = 1.0 \n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"-\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "nlp-challenge SRL dataset: https://github.com/naver/nlp-challenge/tree/master/missions/srl\n",
    "\n",
    "LeaderBoard and Tag description: http://air.changwon.ac.kr/?page_id=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARGM-LOC - - - - - ARG1 ARG1 - - - ARG1 - - -</td>\n",
       "      <td>인사동에 들어서면 다종다양의 창호지, 도자기 등 고미술품들이 진열장에 즐비하게 널려...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARG3 - ARG0 - - ARG1 - ARG1 - - ARG1 -</td>\n",
       "      <td>올림픽에 출진하는 선수라면 어떤 금전적 인유가 없더라도 최악을 다하겠다는 정신력, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARG0 - - - - - - - - - - -</td>\n",
       "      <td>젖먹이들은 비닐 찢어 우산살 일체 붙인 졸악한 것을 연이라 믿고 있을 터인데.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- ARG1 - ARG1 -</td>\n",
       "      <td>때로는 규범이 큰 무도화도 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- - - - ARG0 -</td>\n",
       "      <td>삶이란 끊임없는 취택의 불연속이라고 나는 요탁했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>- - ARG3 - ARG1 - -</td>\n",
       "      <td>천정, 짜임새 마저에 빨간 뼁끼가 칠해져 있었다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>- ARG0 - ARG1 - ARGM-DIR - ARG1 - ARGM-TMP - A...</td>\n",
       "      <td>여기에다가 권뢰가 자신의 양식을 되찾아 현대문화에로 불연속시키는 사역을 시작하기도 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>- ARGM-CAU - - - ARGM-DIR - - ARG1 -</td>\n",
       "      <td>리드미컬한 반동으로 흔들리는 내 교제 위로 노갱이들의 날카로운 발톱들이 보였다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>- - - - - - ARG0 - - - - - - ARG1 - ARGM-TMP -...</td>\n",
       "      <td>마지못해 받아들이는 듯 하면서도 아하스 페르츠의 손길이 미치기만 하면 따뜻해져 감겨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>- - - - ARG1 - -</td>\n",
       "      <td>에너지원으로서 학사당하던 인품도 교질 요막은 얼마든지 있었다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0      ARGM-LOC - - - - - ARG1 ARG1 - - - ARG1 - - -   \n",
       "1             ARG3 - ARG0 - - ARG1 - ARG1 - - ARG1 -   \n",
       "2                         ARG0 - - - - - - - - - - -   \n",
       "3                                    - ARG1 - ARG1 -   \n",
       "4                                     - - - - ARG0 -   \n",
       "5                                - - ARG3 - ARG1 - -   \n",
       "6  - ARG0 - ARG1 - ARGM-DIR - ARG1 - ARGM-TMP - A...   \n",
       "7               - ARGM-CAU - - - ARGM-DIR - - ARG1 -   \n",
       "8  - - - - - - ARG0 - - - - - - ARG1 - ARGM-TMP -...   \n",
       "9                                   - - - - ARG1 - -   \n",
       "\n",
       "                                               words  \n",
       "0  인사동에 들어서면 다종다양의 창호지, 도자기 등 고미술품들이 진열장에 즐비하게 널려...  \n",
       "1  올림픽에 출진하는 선수라면 어떤 금전적 인유가 없더라도 최악을 다하겠다는 정신력, ...  \n",
       "2        젖먹이들은 비닐 찢어 우산살 일체 붙인 졸악한 것을 연이라 믿고 있을 터인데.  \n",
       "3                                 때로는 규범이 큰 무도화도 있다.  \n",
       "4                       삶이란 끊임없는 취택의 불연속이라고 나는 요탁했다.  \n",
       "5                        천정, 짜임새 마저에 빨간 뼁끼가 칠해져 있었다.  \n",
       "6  여기에다가 권뢰가 자신의 양식을 되찾아 현대문화에로 불연속시키는 사역을 시작하기도 ...  \n",
       "7       리드미컬한 반동으로 흔들리는 내 교제 위로 노갱이들의 날카로운 발톱들이 보였다.  \n",
       "8  마지못해 받아들이는 듯 하면서도 아하스 페르츠의 손길이 미치기만 하면 따뜻해져 감겨...  \n",
       "9                 에너지원으로서 학사당하던 인품도 교질 요막은 얼마든지 있었다.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words, self.all_chars = [], [], []\n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        words_col, tags_col = [], []\n",
    "        with open(path) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    if len(words) != 0:\n",
    "                        words_col.append(' '.join(words))\n",
    "                        tags_col.append(' '.join(tags))\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split('\\t')\n",
    "                    word, tag = ls[1], ls[2]\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_tags.append(tag)\n",
    "                    self.all_chars.extend(list(word))\n",
    "                    \n",
    "                    \n",
    "        return pd.DataFrame({'words': words_col, 'tags': tags_col})\n",
    "        \n",
    "    def download_and_load_datasets(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"naver_challenge_srl_train_data.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/naver_challenge_srl_train_data.zip\", \n",
    "          extract=True)\n",
    "\n",
    "        train_df = self.load_dataset(os.path.join(os.path.dirname(dataset), 'naver_challenge_srl_train_data'))\n",
    "        return train_df\n",
    "\n",
    "dataset = Dataset()\n",
    "df = dataset.download_and_load_datasets()\n",
    "\n",
    "# shuffle \n",
    "shuffle_indices = np.random.permutation(np.arange(len(df)))\n",
    "shuffled_df= df.iloc[shuffle_indices]\n",
    "\n",
    "train_end = int(len(df) * 0.9)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "dev_df = df.iloc[train_end:]\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "char_list = list(set(dataset.all_chars)) + [Config.PAD, Config.UNK]\n",
    "char2idx = {w: i for i, w in enumerate(char_list)}\n",
    "idx2char = {i: w for i, w in enumerate(char_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               char_size,\n",
    "               word_embedding_dim, \n",
    "               char_embedding_dim,\n",
    "               hidden_size_word,\n",
    "               hidden_size_char):\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name='word_ids') \n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "        \n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name='char_ids') # [batch_size, max_sequence_length, max_word_length]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\") # [batch_size, max_sequence_length]\n",
    "        \n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('word-embedding'):\n",
    "            self._word_embeddings = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim], -0.25, 0.25), name='_word_embeddings')\n",
    "            self.word_embeddings = tf.nn.embedding_lookup(self._word_embeddings, self.word_ids) # [batch_size, max_sequence_length, word_embedding_dim]\n",
    "        \n",
    "        # Char Embedding Layer\n",
    "        with tf.variable_scope('char-embedding'):\n",
    "            self._char_embeddings = tf.get_variable(dtype=tf.float32, shape=[char_size, char_embedding_dim], name='_char_embeddings')\n",
    "            \n",
    "            # [batch_size, max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            self.char_embeddings = tf.nn.embedding_lookup(self._char_embeddings, self.char_ids) \n",
    "            \n",
    "            s = tf.shape(self.char_embeddings)\n",
    "            \n",
    "            # [batch_size*max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            char_embeddings = tf.reshape(self.char_embeddings, shape=[s[0]*s[1], s[2], char_embedding_dim])\n",
    "            word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "            \n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            \n",
    "            _, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, \n",
    "                                                                                   cell_bw=bw_cell, \n",
    "                                                                                   inputs=char_embeddings,\n",
    "                                                                                   sequence_length=word_lengths,\n",
    "                                                                                   dtype=tf.float32)\n",
    "            # shape: [batch_size*max_sequnce_length, 2*hidden_size_char]\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.reshape(output, shape=[s[0], s[1], 2*hidden_size_char])\n",
    "            \n",
    "            # shape: # [batch_size, max_sequence_length, word_embedding_dim + 2*hidden_size_char]\n",
    "            self.word_embeddings = tf.concat([self.word_embeddings, output], axis=-1) \n",
    "            # self.word_embeddings = tf.nn.dropout(self.word_embeddings, self.dropout)\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.word_embeddings,\n",
    "                                                                  sequence_length= self.sequence_lengths, # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, max_sequence_length, 2*hidden_size_word]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "        \n",
    "        \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size_word, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tf.shape(self.rnn_outputs)[1]\n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size_word])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, num_classes]) # [batch_size, max_sequence_length, num_classes]\n",
    "    \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params  # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "            \n",
    "#             When CRF is not in use\n",
    "#             self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "#             mask = tf.sequence_mask(self.sequence_lengths)\n",
    "#             losses = tf.boolean_mask(self.losses, mask)\n",
    "#             self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @staticmethod\n",
    "    def viterbi_decode(logits, trans_params):\n",
    "        # get tag scores and transition params of CRF\n",
    "        viterbi_sequences = []\n",
    "\n",
    "        # iterate over the sentences because no batching in vitervi_decode\n",
    "        for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "            logit = logit[:sequence_length]  # keep only the valid steps\n",
    "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                logit, trans_params)\n",
    "            viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "        return np.array(viterbi_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(word_embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), word_embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(df, batch_size, num_epochs, shuffle=True, tqdm_disable=False):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(df)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs), disable=tqdm_disable):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_df= df.iloc[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_df = df\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_df.iloc[start_index:end_index]    \n",
    "            \n",
    "\n",
    "def get_feed_dict(batch_df):\n",
    "    max_length = max(map(lambda x : len(x.split(' ')), batch_df['words'].tolist()))\n",
    "    \n",
    "    max_length_word = 0\n",
    "    for seq in batch_df['words'].tolist():\n",
    "        for word in seq.split(' '):\n",
    "            max_length_word = max(max_length_word, len(word))\n",
    "    \n",
    "    word_ids, sequence_lengths, labels, char_ids, word_lengths = [], [], [], [], []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        sentence = row['words'].split(' ')\n",
    "        tags = row['tags'].split(' ')\n",
    "\n",
    "        word_ids_row, labels_row, char_ids_row, word_lengths_row = [], [], [], []\n",
    "        for word in sentence:\n",
    "            word_ids_row.append(word2idx[word])\n",
    "        \n",
    "            char_ids_row.append([char2idx[char] for char in word] + [char2idx[Config.PAD]]* (max_length_word - len(word)) )\n",
    "            word_lengths_row.append(len(word))\n",
    "        \n",
    "        empty_char_ids = [char2idx[Config.PAD]]* max_length_word\n",
    "        char_ids_row += [empty_char_ids] * (max_length - len(char_ids_row))\n",
    "        word_lengths_row += [0] * (max_length - len(word_lengths_row))\n",
    "        \n",
    "        for tag in tags:\n",
    "            labels_row.append(tag2idx[tag])\n",
    "\n",
    "        if len(sentence) < max_length:\n",
    "            word_ids_row += [word2idx[Config.PAD]]* (max_length - len(sentence))\n",
    "            labels_row += [tag2idx[Config.NONE]]* (max_length - len(sentence))\n",
    "\n",
    "        word_ids.append(word_ids_row)\n",
    "        labels.append(labels_row)\n",
    "        sequence_lengths.append(len(sentence))\n",
    "        char_ids.append(char_ids_row)\n",
    "        word_lengths.append(word_lengths_row)\n",
    "    \n",
    "    word_ids = np.array(word_ids)\n",
    "    labels = np.array(labels)\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    char_ids = np.array(char_ids)\n",
    "    word_lengths = np.array(word_lengths)\n",
    "    \n",
    "    return word_ids, labels, sequence_lengths, char_ids, word_lengths\n",
    "\n",
    "def evaluation(y, preds, lengths):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answers, arg_preds = [], []\n",
    "    \n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0.0, 0.0, 0.0\n",
    "    for i in range(len(y)):\n",
    "        sent_answers,sent_preds = [], []\n",
    "        sent_answer_chunks, sent_pred_chunks = [], []\n",
    "        \n",
    "        for j in range(lengths[i]):\n",
    "            sent_answers.append(idx2tag[y[i][j]])\n",
    "            sent_preds.append(idx2tag[preds[i][j]])\n",
    "            \n",
    "            if idx2tag[y[i][j]] != Config.NONE:\n",
    "                sent_answer_chunks.append(idx2tag[y[i][j]] + '-' + str(j))\n",
    "            if idx2tag[preds[i][j]] != Config.NONE:\n",
    "                sent_pred_chunks.append(idx2tag[preds[i][j]] + '-' + str(j))\n",
    "    \n",
    "        arg_answers.extend(sent_answers)\n",
    "        arg_preds.extend(sent_preds)\n",
    "        \n",
    "        accs += [a == b for (a, b) in zip(sent_answers, sent_preds)]\n",
    "        \n",
    "        sent_answer_chunks = set(sent_answer_chunks)\n",
    "        sent_pred_chunks = set(sent_pred_chunks)\n",
    "\n",
    "        correct_preds += len(sent_answer_chunks & sent_pred_chunks)\n",
    "        total_preds += len(sent_pred_chunks)\n",
    "        total_correct += len(sent_answer_chunks)\n",
    "    \n",
    "    p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "        \n",
    "    print(classification_report(arg_answers, arg_preds))\n",
    "    \n",
    "    print('Tag based evaluation: acc: {}, f1: {}'.format(acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-a777bdb640ad>:41: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-a777bdb640ad>:48: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-a777bdb640ad>:68: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/34.runs/1559130469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:01<19:22, 61.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:48:53.214938: step 500, loss 5.11227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.95      0.94       499\n",
      "        ARG0       0.69      0.82      0.75        22\n",
      "        ARG1       0.80      0.84      0.82       125\n",
      "        ARG2       0.67      0.55      0.60        11\n",
      "        ARG3       0.27      0.32      0.29        19\n",
      "    ARGM-CAU       0.00      0.00      0.00         1\n",
      "    ARGM-DIR       0.00      0.00      0.00         3\n",
      "    ARGM-EXT       1.00      0.25      0.40         4\n",
      "    ARGM-INS       0.00      0.00      0.00         3\n",
      "    ARGM-LOC       0.38      0.25      0.30        12\n",
      "    ARGM-MNR       0.17      0.14      0.15         7\n",
      "    ARGM-PRP       0.00      0.00      0.00         1\n",
      "    ARGM-TMP       1.00      0.25      0.40         4\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       711\n",
      "   macro avg       0.45      0.34      0.36       711\n",
      "weighted avg       0.85      0.86      0.85       711\n",
      "\n",
      "Tag based evaluation: acc: 0.8621659634317862, f1: 0.6746411483253588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [02:02<18:20, 61.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:49:55.443672: step 1000, loss 4.44164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.91      0.96      0.93       524\n",
      "        ARG0       0.76      0.52      0.62        25\n",
      "        ARG1       0.78      0.78      0.78       120\n",
      "        ARG2       0.75      0.50      0.60         6\n",
      "        ARG3       0.27      0.19      0.22        21\n",
      "    ARGM-CAU       0.00      0.00      0.00         1\n",
      "    ARGM-DIR       1.00      0.50      0.67         2\n",
      "    ARGM-EXT       0.00      0.00      0.00         3\n",
      "    ARGM-LOC       0.53      0.40      0.46        20\n",
      "    ARGM-MNR       0.64      0.64      0.64        11\n",
      "    ARGM-TMP       1.00      0.29      0.44         7\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       740\n",
      "   macro avg       0.60      0.43      0.49       740\n",
      "weighted avg       0.85      0.86      0.85       740\n",
      "\n",
      "Tag based evaluation: acc: 0.8581081081081081, f1: 0.6534653465346535\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T20:49:58.628666: loss 0.0742247\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.95      0.95     29362\n",
      "        ARG0       0.76      0.66      0.71      1807\n",
      "        ARG1       0.79      0.85      0.82      6967\n",
      "        ARG2       0.64      0.19      0.29       474\n",
      "        ARG3       0.46      0.46      0.46      1151\n",
      "    ARGM-CAU       0.52      0.16      0.24       183\n",
      "    ARGM-DIR       0.57      0.21      0.30       136\n",
      "    ARGM-EXT       0.82      0.36      0.50       292\n",
      "    ARGM-INS       1.00      0.01      0.01       133\n",
      "    ARGM-LOC       0.34      0.69      0.46       618\n",
      "    ARGM-MNR       0.41      0.41      0.41       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.50      0.20      0.29       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.60      0.40      0.42     41882\n",
      "weighted avg       0.87      0.87      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8707081801251134, f1: 0.6895608814540775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [03:06<17:34, 62.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:51:00.504603: step 1500, loss 4.4903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.92      0.95      0.93       500\n",
      "        ARG0       0.77      0.73      0.75        33\n",
      "        ARG1       0.78      0.76      0.77       118\n",
      "        ARG2       0.60      0.67      0.63         9\n",
      "        ARG3       0.50      0.48      0.49        21\n",
      "    ARGM-CAU       0.00      0.00      0.00         1\n",
      "    ARGM-DIR       0.33      1.00      0.50         1\n",
      "    ARGM-EXT       0.67      0.80      0.73         5\n",
      "    ARGM-INS       0.00      0.00      0.00         2\n",
      "    ARGM-LOC       0.38      0.25      0.30        12\n",
      "    ARGM-MNR       0.50      0.20      0.29         5\n",
      "    ARGM-TMP       0.00      0.00      0.00         5\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       712\n",
      "   macro avg       0.45      0.49      0.45       712\n",
      "weighted avg       0.85      0.86      0.85       712\n",
      "\n",
      "Tag based evaluation: acc: 0.8609550561797753, f1: 0.6780487804878048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [04:06<16:25, 61.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:52:02.417652: step 2000, loss 3.62751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.97      0.95       529\n",
      "        ARG0       0.85      0.82      0.84        34\n",
      "        ARG1       0.90      0.83      0.86       138\n",
      "        ARG2       0.71      0.62      0.67         8\n",
      "        ARG3       0.52      0.65      0.58        20\n",
      "    ARGM-CAU       0.00      0.00      0.00         2\n",
      "    ARGM-DIR       0.57      0.67      0.62         6\n",
      "    ARGM-EXT       0.83      0.83      0.83         6\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.86      0.46      0.60        13\n",
      "    ARGM-MNR       0.71      0.50      0.59        10\n",
      "    ARGM-PRP       0.00      0.00      0.00         1\n",
      "    ARGM-TMP       0.50      0.33      0.40         3\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       771\n",
      "   macro avg       0.57      0.51      0.53       771\n",
      "weighted avg       0.90      0.90      0.90       771\n",
      "\n",
      "Tag based evaluation: acc: 0.9014267185473411, f1: 0.7835497835497834\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T20:52:05.451344: loss 0.0682462\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.95      0.95     29362\n",
      "        ARG0       0.74      0.74      0.74      1807\n",
      "        ARG1       0.81      0.86      0.83      6967\n",
      "        ARG2       0.53      0.51      0.52       474\n",
      "        ARG3       0.48      0.56      0.52      1151\n",
      "    ARGM-CAU       0.55      0.18      0.27       183\n",
      "    ARGM-DIR       0.56      0.47      0.51       136\n",
      "    ARGM-EXT       0.77      0.58      0.66       292\n",
      "    ARGM-INS       0.50      0.08      0.14       133\n",
      "    ARGM-LOC       0.49      0.50      0.49       618\n",
      "    ARGM-MNR       0.46      0.45      0.46       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.61      0.34      0.44       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.57      0.48      0.50     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.882216704073349, f1: 0.7231210647963657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [05:11<15:38, 62.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:53:08.134743: step 2500, loss 3.86079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.96      0.95       499\n",
      "        ARG0       0.84      0.84      0.84        37\n",
      "        ARG1       0.87      0.88      0.88       137\n",
      "        ARG2       0.67      0.29      0.40         7\n",
      "        ARG3       0.43      0.53      0.47        17\n",
      "    ARGM-CAU       1.00      0.50      0.67         2\n",
      "    ARGM-DIR       0.00      0.00      0.00         6\n",
      "    ARGM-EXT       0.75      0.50      0.60         6\n",
      "    ARGM-INS       0.00      0.00      0.00         3\n",
      "    ARGM-LOC       0.58      0.58      0.58        12\n",
      "    ARGM-MNR       0.60      0.43      0.50         7\n",
      "    ARGM-TMP       0.20      0.17      0.18         6\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       739\n",
      "   macro avg       0.57      0.47      0.51       739\n",
      "weighted avg       0.88      0.89      0.88       739\n",
      "\n",
      "Tag based evaluation: acc: 0.8890392422192152, f1: 0.7590618336886994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [06:12<14:27, 61.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:54:09.757189: step 3000, loss 5.3028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.94      0.96       698\n",
      "        ARG0       0.64      0.74      0.69        43\n",
      "        ARG1       0.85      0.90      0.88       149\n",
      "        ARG2       0.43      0.38      0.40         8\n",
      "        ARG3       0.38      0.60      0.46        15\n",
      "    ARGM-CAU       0.00      0.00      0.00         6\n",
      "    ARGM-DIR       0.50      0.50      0.50         2\n",
      "    ARGM-EXT       0.83      0.56      0.67         9\n",
      "    ARGM-INS       0.40      0.50      0.44         4\n",
      "    ARGM-LOC       0.45      0.45      0.45        11\n",
      "    ARGM-MNR       0.36      0.44      0.40         9\n",
      "    ARGM-TMP       0.50      0.43      0.46         7\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       961\n",
      "   macro avg       0.53      0.54      0.53       961\n",
      "weighted avg       0.89      0.89      0.89       961\n",
      "\n",
      "Tag based evaluation: acc: 0.8917793964620188, f1: 0.7306273062730627\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T20:54:12.767479: loss 0.0696927\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.96      0.95     29362\n",
      "        ARG0       0.80      0.70      0.74      1807\n",
      "        ARG1       0.82      0.83      0.83      6967\n",
      "        ARG2       0.60      0.43      0.50       474\n",
      "        ARG3       0.53      0.42      0.47      1151\n",
      "    ARGM-CAU       0.51      0.21      0.30       183\n",
      "    ARGM-DIR       0.71      0.37      0.49       136\n",
      "    ARGM-EXT       0.76      0.59      0.67       292\n",
      "    ARGM-INS       0.44      0.14      0.22       133\n",
      "    ARGM-LOC       0.58      0.42      0.48       618\n",
      "    ARGM-MNR       0.44      0.46      0.45       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.56      0.33      0.41       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.59      0.45      0.50     41882\n",
      "weighted avg       0.87      0.88      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8804020820400172, f1: 0.7177703269069572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [07:16<13:35, 62.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:55:15.287656: step 3500, loss 2.95726\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.97      0.97       549\n",
      "        ARG0       0.94      0.86      0.90        37\n",
      "        ARG1       0.87      0.89      0.88       123\n",
      "        ARG2       0.62      0.57      0.59        14\n",
      "        ARG3       0.59      0.87      0.70        23\n",
      "    ARGM-CAU       0.75      0.60      0.67         5\n",
      "    ARGM-DIR       0.75      0.75      0.75         4\n",
      "    ARGM-EXT       1.00      0.88      0.93         8\n",
      "    ARGM-INS       1.00      0.67      0.80         3\n",
      "    ARGM-LOC       0.62      0.50      0.56        10\n",
      "    ARGM-MNR       0.67      0.46      0.55        13\n",
      "    ARGM-TMP       0.86      0.86      0.86         7\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       796\n",
      "   macro avg       0.80      0.74      0.76       796\n",
      "weighted avg       0.92      0.92      0.92       796\n",
      "\n",
      "Tag based evaluation: acc: 0.9208542713567839, f1: 0.8161616161616162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [08:17<12:26, 62.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:56:17.702489: step 4000, loss 2.75664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.96      0.96       490\n",
      "        ARG0       0.87      0.97      0.92        34\n",
      "        ARG1       0.86      0.98      0.91       121\n",
      "        ARG2       0.73      0.53      0.62        15\n",
      "        ARG3       0.68      0.87      0.76        15\n",
      "    ARGM-CAU       0.00      0.00      0.00         4\n",
      "    ARGM-DIR       0.67      0.67      0.67         3\n",
      "    ARGM-EXT       0.80      0.80      0.80         5\n",
      "    ARGM-INS       1.00      0.40      0.57         5\n",
      "    ARGM-LOC       0.69      0.56      0.62        16\n",
      "    ARGM-MNR       0.75      0.43      0.55         7\n",
      "    ARGM-TMP       0.75      0.75      0.75         4\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       719\n",
      "   macro avg       0.73      0.66      0.68       719\n",
      "weighted avg       0.92      0.92      0.92       719\n",
      "\n",
      "Tag based evaluation: acc: 0.9221140472878998, f1: 0.8387096774193549\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T20:56:20.709760: loss 0.0735964\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.96      0.95     29362\n",
      "        ARG0       0.77      0.71      0.74      1807\n",
      "        ARG1       0.81      0.84      0.82      6967\n",
      "        ARG2       0.55      0.47      0.50       474\n",
      "        ARG3       0.51      0.45      0.48      1151\n",
      "    ARGM-CAU       0.49      0.23      0.32       183\n",
      "    ARGM-DIR       0.64      0.43      0.51       136\n",
      "    ARGM-EXT       0.67      0.66      0.66       292\n",
      "    ARGM-INS       0.43      0.24      0.31       133\n",
      "    ARGM-LOC       0.50      0.51      0.51       618\n",
      "    ARGM-MNR       0.49      0.41      0.44       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.55      0.37      0.44       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.56      0.48      0.51     41882\n",
      "weighted avg       0.87      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8794231412062461, f1: 0.7162002525561123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [09:21<11:30, 62.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:57:22.596597: step 4500, loss 2.45982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.97      0.97       515\n",
      "        ARG0       0.90      0.90      0.90        31\n",
      "        ARG1       0.88      0.94      0.91       145\n",
      "        ARG2       0.58      0.70      0.64        10\n",
      "        ARG3       0.87      0.72      0.79        18\n",
      "    ARGM-CAU       0.75      0.33      0.46         9\n",
      "    ARGM-DIR       1.00      1.00      1.00         2\n",
      "    ARGM-EXT       1.00      0.57      0.73         7\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.75      0.60      0.67        10\n",
      "    ARGM-MNR       0.62      0.71      0.67         7\n",
      "    ARGM-TMP       0.86      0.75      0.80         8\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       763\n",
      "   macro avg       0.77      0.68      0.71       763\n",
      "weighted avg       0.93      0.93      0.93       763\n",
      "\n",
      "Tag based evaluation: acc: 0.9318479685452162, f1: 0.8484848484848485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [10:22<10:21, 62.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:58:24.545360: step 5000, loss 1.69325\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.98      0.98      0.98       530\n",
      "        ARG0       0.97      0.88      0.92        33\n",
      "        ARG1       0.90      0.97      0.94       117\n",
      "        ARG2       0.67      0.40      0.50        10\n",
      "        ARG3       0.38      0.55      0.44        11\n",
      "    ARGM-CAU       1.00      0.33      0.50         3\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      0.89      0.94         9\n",
      "    ARGM-INS       0.33      0.50      0.40         2\n",
      "    ARGM-LOC       0.73      0.57      0.64        14\n",
      "    ARGM-MNR       0.67      0.57      0.62         7\n",
      "    ARGM-TMP       0.67      0.86      0.75         7\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       744\n",
      "   macro avg       0.77      0.71      0.72       744\n",
      "weighted avg       0.95      0.94      0.94       744\n",
      "\n",
      "Tag based evaluation: acc: 0.9422043010752689, f1: 0.8445475638051043\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T20:58:27.542134: loss 0.0863961\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.95      0.94     29362\n",
      "        ARG0       0.71      0.78      0.74      1807\n",
      "        ARG1       0.83      0.79      0.81      6967\n",
      "        ARG2       0.51      0.47      0.49       474\n",
      "        ARG3       0.51      0.46      0.48      1151\n",
      "    ARGM-CAU       0.36      0.28      0.32       183\n",
      "    ARGM-DIR       0.51      0.51      0.51       136\n",
      "    ARGM-EXT       0.62      0.64      0.63       292\n",
      "    ARGM-INS       0.33      0.26      0.29       133\n",
      "    ARGM-LOC       0.48      0.50      0.49       618\n",
      "    ARGM-MNR       0.43      0.50      0.46       383\n",
      "    ARGM-PRP       0.11      0.02      0.04        41\n",
      "    ARGM-TMP       0.46      0.46      0.46       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.52      0.51      0.51     41882\n",
      "weighted avg       0.87      0.87      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8730003342724798, f1: 0.7021697846377633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [11:26<09:25, 62.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T20:59:30.183571: step 5500, loss 1.5396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       527\n",
      "        ARG0       0.94      0.97      0.95        31\n",
      "        ARG1       0.94      0.99      0.96       138\n",
      "        ARG2       0.50      0.20      0.29         5\n",
      "        ARG3       0.76      0.80      0.78        20\n",
      "    ARGM-CAU       0.83      0.71      0.77         7\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      0.80      0.89         5\n",
      "    ARGM-INS       0.00      0.00      0.00         2\n",
      "    ARGM-LOC       0.50      0.75      0.60         4\n",
      "    ARGM-MNR       0.83      0.71      0.77         7\n",
      "    ARGM-TMP       1.00      0.80      0.89         5\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       752\n",
      "   macro avg       0.77      0.73      0.74       752\n",
      "weighted avg       0.96      0.96      0.96       752\n",
      "\n",
      "Tag based evaluation: acc: 0.964095744680851, f1: 0.9070796460176991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [12:28<08:19, 62.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:00:33.209292: step 6000, loss 1.77701\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.98      0.99      0.99       544\n",
      "        ARG0       0.92      0.98      0.95        49\n",
      "        ARG1       0.93      0.93      0.93       121\n",
      "        ARG2       1.00      0.33      0.50         9\n",
      "        ARG3       0.78      0.61      0.68        23\n",
      "    ARGM-CAU       0.25      0.25      0.25         4\n",
      "    ARGM-DIR       0.00      0.00      0.00         2\n",
      "    ARGM-EXT       1.00      0.83      0.91         6\n",
      "    ARGM-INS       0.33      1.00      0.50         1\n",
      "    ARGM-LOC       0.71      1.00      0.83        15\n",
      "    ARGM-MNR       0.88      0.70      0.78        10\n",
      "    ARGM-TMP       0.92      0.85      0.88        13\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       797\n",
      "   macro avg       0.73      0.71      0.68       797\n",
      "weighted avg       0.95      0.95      0.95       797\n",
      "\n",
      "Tag based evaluation: acc: 0.9510664993726474, f1: 0.872\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:00:36.214371: loss 0.105746\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.94      0.94     29362\n",
      "        ARG0       0.71      0.75      0.73      1807\n",
      "        ARG1       0.80      0.81      0.80      6967\n",
      "        ARG2       0.48      0.43      0.45       474\n",
      "        ARG3       0.49      0.50      0.50      1151\n",
      "    ARGM-CAU       0.43      0.25      0.32       183\n",
      "    ARGM-DIR       0.50      0.46      0.48       136\n",
      "    ARGM-EXT       0.65      0.58      0.61       292\n",
      "    ARGM-INS       0.31      0.28      0.29       133\n",
      "    ARGM-LOC       0.46      0.50      0.48       618\n",
      "    ARGM-MNR       0.45      0.46      0.46       383\n",
      "    ARGM-PRP       0.25      0.02      0.04        41\n",
      "    ARGM-TMP       0.44      0.42      0.43       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.53      0.49      0.50     41882\n",
      "weighted avg       0.87      0.87      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8683921493720452, f1: 0.6954304609378112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [13:33<07:22, 63.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:01:38.916301: step 6500, loss 1.47846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       572\n",
      "        ARG0       0.92      1.00      0.96        35\n",
      "        ARG1       0.95      0.97      0.96       141\n",
      "        ARG2       0.77      0.77      0.77        13\n",
      "        ARG3       0.95      0.79      0.86        24\n",
      "    ARGM-CAU       1.00      1.00      1.00         1\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       0.67      1.00      0.80         6\n",
      "    ARGM-INS       1.00      0.67      0.80         3\n",
      "    ARGM-LOC       1.00      0.87      0.93        15\n",
      "    ARGM-MNR       1.00      0.71      0.83         7\n",
      "    ARGM-TMP       0.83      1.00      0.91         5\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       823\n",
      "   macro avg       0.92      0.90      0.90       823\n",
      "weighted avg       0.97      0.97      0.97       823\n",
      "\n",
      "Tag based evaluation: acc: 0.9720534629404617, f1: 0.9304174950298212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [14:34<06:14, 62.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:02:40.956683: step 7000, loss 0.911424\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       534\n",
      "        ARG0       0.97      1.00      0.99        36\n",
      "        ARG1       0.97      0.97      0.97       123\n",
      "        ARG2       0.67      0.86      0.75         7\n",
      "        ARG3       0.91      1.00      0.95        21\n",
      "    ARGM-CAU       1.00      0.60      0.75         5\n",
      "    ARGM-DIR       1.00      1.00      1.00         3\n",
      "    ARGM-EXT       1.00      1.00      1.00         6\n",
      "    ARGM-INS       0.50      0.50      0.50         2\n",
      "    ARGM-LOC       0.92      0.79      0.85        14\n",
      "    ARGM-MNR       0.80      0.57      0.67         7\n",
      "    ARGM-PRP       1.00      0.50      0.67         2\n",
      "    ARGM-TMP       0.67      0.80      0.73         5\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       765\n",
      "   macro avg       0.88      0.81      0.83       765\n",
      "weighted avg       0.98      0.98      0.97       765\n",
      "\n",
      "Tag based evaluation: acc: 0.9751633986928104, f1: 0.9327548806941431\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:02:44.008057: loss 0.129639\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.94      0.94     29362\n",
      "        ARG0       0.73      0.71      0.72      1807\n",
      "        ARG1       0.79      0.81      0.80      6967\n",
      "        ARG2       0.49      0.46      0.47       474\n",
      "        ARG3       0.50      0.42      0.46      1151\n",
      "    ARGM-CAU       0.36      0.27      0.31       183\n",
      "    ARGM-DIR       0.58      0.39      0.46       136\n",
      "    ARGM-EXT       0.65      0.57      0.60       292\n",
      "    ARGM-INS       0.25      0.23      0.24       133\n",
      "    ARGM-LOC       0.49      0.46      0.47       618\n",
      "    ARGM-MNR       0.39      0.51      0.45       383\n",
      "    ARGM-PRP       0.19      0.07      0.11        41\n",
      "    ARGM-TMP       0.45      0.40      0.42       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.52      0.48      0.50     41882\n",
      "weighted avg       0.86      0.87      0.86     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8650255479681008, f1: 0.6877649053404917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [15:39<05:15, 63.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:03:46.805434: step 7500, loss 0.666215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       444\n",
      "        ARG0       0.97      1.00      0.98        29\n",
      "        ARG1       0.97      1.00      0.99       110\n",
      "        ARG2       1.00      0.89      0.94         9\n",
      "        ARG3       1.00      0.90      0.95        21\n",
      "    ARGM-CAU       1.00      0.75      0.86         4\n",
      "    ARGM-DIR       0.50      1.00      0.67         1\n",
      "    ARGM-EXT       1.00      1.00      1.00        12\n",
      "    ARGM-INS       0.75      0.75      0.75         4\n",
      "    ARGM-LOC       0.87      1.00      0.93        13\n",
      "    ARGM-MNR       1.00      0.71      0.83         7\n",
      "    ARGM-TMP       0.80      1.00      0.89         4\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       658\n",
      "   macro avg       0.90      0.92      0.90       658\n",
      "weighted avg       0.98      0.98      0.98       658\n",
      "\n",
      "Tag based evaluation: acc: 0.9817629179331308, f1: 0.9627906976744186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [16:40<04:10, 62.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:04:48.943861: step 8000, loss 0.488135\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       471\n",
      "        ARG0       1.00      0.97      0.99        38\n",
      "        ARG1       0.97      0.99      0.98        98\n",
      "        ARG2       0.80      0.80      0.80         5\n",
      "        ARG3       0.93      0.81      0.87        16\n",
      "    ARGM-CAU       1.00      1.00      1.00         4\n",
      "    ARGM-DIR       0.75      1.00      0.86         3\n",
      "    ARGM-EXT       1.00      0.75      0.86         8\n",
      "    ARGM-INS       1.00      1.00      1.00         3\n",
      "    ARGM-LOC       0.92      0.92      0.92        12\n",
      "    ARGM-MNR       0.80      1.00      0.89         4\n",
      "    ARGM-PRP       1.00      1.00      1.00         1\n",
      "    ARGM-TMP       0.67      1.00      0.80         2\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       665\n",
      "   macro avg       0.91      0.94      0.92       665\n",
      "weighted avg       0.98      0.98      0.98       665\n",
      "\n",
      "Tag based evaluation: acc: 0.9819548872180451, f1: 0.9536082474226805\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:04:51.948162: loss 0.158535\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.94      0.94     29362\n",
      "        ARG0       0.73      0.70      0.72      1807\n",
      "        ARG1       0.79      0.80      0.79      6967\n",
      "        ARG2       0.48      0.46      0.47       474\n",
      "        ARG3       0.46      0.49      0.47      1151\n",
      "    ARGM-CAU       0.40      0.25      0.30       183\n",
      "    ARGM-DIR       0.51      0.43      0.47       136\n",
      "    ARGM-EXT       0.62      0.56      0.59       292\n",
      "    ARGM-INS       0.22      0.25      0.23       133\n",
      "    ARGM-LOC       0.49      0.45      0.47       618\n",
      "    ARGM-MNR       0.42      0.40      0.41       383\n",
      "    ARGM-PRP       0.38      0.07      0.12        41\n",
      "    ARGM-TMP       0.47      0.35      0.40       335\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     41882\n",
      "   macro avg       0.53      0.47      0.49     41882\n",
      "weighted avg       0.86      0.86      0.86     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8640466071343298, f1: 0.6822124500746358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [17:44<03:09, 63.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:05:54.083975: step 8500, loss 0.28223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       1.00      1.00      1.00       557\n",
      "        ARG0       1.00      0.98      0.99        41\n",
      "        ARG1       1.00      0.99      1.00       140\n",
      "        ARG2       0.86      1.00      0.92         6\n",
      "        ARG3       0.94      1.00      0.97        15\n",
      "    ARGM-CAU       1.00      1.00      1.00         2\n",
      "    ARGM-DIR       1.00      1.00      1.00         3\n",
      "    ARGM-EXT       1.00      1.00      1.00         4\n",
      "    ARGM-INS       1.00      1.00      1.00         4\n",
      "    ARGM-LOC       1.00      0.93      0.97        15\n",
      "    ARGM-MNR       0.75      1.00      0.86         3\n",
      "    ARGM-TMP       1.00      0.83      0.91         6\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       796\n",
      "   macro avg       0.96      0.98      0.97       796\n",
      "weighted avg       0.99      0.99      0.99       796\n",
      "\n",
      "Tag based evaluation: acc: 0.9937185929648241, f1: 0.9853249475890986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [18:45<02:05, 62.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:06:56.945488: step 9000, loss 0.20072\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       1.00      1.00      1.00       561\n",
      "        ARG0       1.00      0.98      0.99        45\n",
      "        ARG1       0.98      0.99      0.99       105\n",
      "        ARG2       1.00      1.00      1.00         4\n",
      "        ARG3       0.94      0.94      0.94        17\n",
      "    ARGM-CAU       1.00      1.00      1.00         1\n",
      "    ARGM-DIR       1.00      1.00      1.00         4\n",
      "    ARGM-EXT       1.00      1.00      1.00         7\n",
      "    ARGM-INS       1.00      1.00      1.00         2\n",
      "    ARGM-LOC       1.00      1.00      1.00        11\n",
      "    ARGM-MNR       1.00      1.00      1.00         4\n",
      "    ARGM-PRP       1.00      1.00      1.00         1\n",
      "    ARGM-TMP       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       768\n",
      "   macro avg       0.99      0.99      0.99       768\n",
      "weighted avg       1.00      1.00      1.00       768\n",
      "\n",
      "Tag based evaluation: acc: 0.99609375, f1: 0.9855072463768116\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:06:59.932038: loss 0.185022\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.94      0.94     29362\n",
      "        ARG0       0.74      0.69      0.71      1807\n",
      "        ARG1       0.81      0.79      0.80      6967\n",
      "        ARG2       0.45      0.48      0.46       474\n",
      "        ARG3       0.46      0.46      0.46      1151\n",
      "    ARGM-CAU       0.32      0.26      0.29       183\n",
      "    ARGM-DIR       0.49      0.49      0.49       136\n",
      "    ARGM-EXT       0.60      0.59      0.59       292\n",
      "    ARGM-INS       0.20      0.21      0.20       133\n",
      "    ARGM-LOC       0.45      0.51      0.48       618\n",
      "    ARGM-MNR       0.38      0.37      0.37       383\n",
      "    ARGM-PRP       0.17      0.07      0.10        41\n",
      "    ARGM-TMP       0.39      0.41      0.40       335\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     41882\n",
      "   macro avg       0.49      0.48      0.48     41882\n",
      "weighted avg       0.86      0.86      0.86     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8615873167470512, f1: 0.6753937245740524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [19:50<01:03, 63.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:08:02.542936: step 9500, loss 0.260349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       1.00      1.00      1.00       546\n",
      "        ARG0       1.00      1.00      1.00        31\n",
      "        ARG1       1.00      0.98      0.99       114\n",
      "        ARG2       0.92      1.00      0.96        11\n",
      "        ARG3       1.00      1.00      1.00        21\n",
      "    ARGM-CAU       1.00      1.00      1.00         3\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      1.00      1.00         3\n",
      "    ARGM-INS       1.00      0.75      0.86         4\n",
      "    ARGM-LOC       1.00      0.88      0.93        16\n",
      "    ARGM-MNR       0.93      1.00      0.96        13\n",
      "    ARGM-TMP       0.88      1.00      0.93         7\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       770\n",
      "   macro avg       0.98      0.97      0.97       770\n",
      "weighted avg       0.99      0.99      0.99       770\n",
      "\n",
      "Tag based evaluation: acc: 0.9935064935064936, f1: 0.9820627802690582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:51<00:00, 62.51s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        char_size=len(char_list),\n",
    "        word_embedding_dim=Config.word_embedding_dim,\n",
    "        char_embedding_dim=Config.char_embedding_dim,\n",
    "        hidden_size_word=Config.hidden_size_word,\n",
    "        hidden_size_char=Config.hidden_size_char\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # train_op = tf.train.AdamOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(Config.learning_rate, Config.decay_rate, 1e-6)\n",
    "    gvs = optimizer.compute_gradients(model.loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"34.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     pretrain_W = load_glove(Config.word_embedding_dim, word2idx)\n",
    "#     sess.run(model._word_embeddings.assign(pretrain_W))\n",
    "#     print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(train_df, Config.batch_size, Config.num_epochs)\n",
    "    for batch_df in batches:\n",
    "        word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "        feed_dict = {\n",
    "            model.word_ids: word_ids,\n",
    "            model.labels: labels,\n",
    "            model.sequence_lengths: sequence_lengths,\n",
    "            model.char_ids: char_ids,\n",
    "            model.word_lengths: word_lengths,\n",
    "            model.dropout: 0.5,\n",
    "        }\n",
    "        _, step, loss, logits, trans_params = sess.run([\n",
    "            train_op, global_step, model.loss, model.logits, model.trans_params], feed_dict)\n",
    "        \n",
    "        predictions = model.viterbi_decode(logits, trans_params)\n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"Train Evaluation {}: step {}, loss {:g}\".format(time_str, step, loss))\n",
    "            evaluation(labels, predictions, sequence_lengths)\n",
    "            \n",
    "            \n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            batches = batch_iter(dev_df, Config.batch_size, 1, tqdm_disable=True)\n",
    "            \n",
    "            total_loss, predictions_all, labels_all, sequence_lengths_all  = 0, [], [], []\n",
    "            for batch_df in batches:\n",
    "                word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "                feed_dict = {\n",
    "                    model.word_ids: word_ids,\n",
    "                    model.labels: labels,\n",
    "                    model.sequence_lengths: sequence_lengths,\n",
    "                    model.char_ids: char_ids,\n",
    "                    model.word_lengths: word_lengths,\n",
    "                    model.dropout: 1.0,\n",
    "                }\n",
    "                loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "                predictions = model.viterbi_decode(logits, trans_params)\n",
    "                \n",
    "                total_loss += loss\n",
    "                predictions_all += predictions.tolist()\n",
    "                labels_all += labels.tolist()\n",
    "                sequence_lengths_all += sequence_lengths.tolist()\n",
    "        \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"\\nDev Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "            evaluation(labels_all, predictions_all, sequence_lengths_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
