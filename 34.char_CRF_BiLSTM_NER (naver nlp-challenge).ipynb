{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Embeddings\n",
    "    word_embedding_dim = 300\n",
    "    char_embedding_dim = 100\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size_word = 300\n",
    "    hidden_size_char = 100\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 20\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    learning_rate = 1.0\n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"-\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "nlp-challenge NER dataset: https://github.com/naver/nlp-challenge/tree/master/missions/ner\n",
    "\n",
    "LeaderBoard and Tag description: http://air.changwon.ac.kr/?page_id=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PER_B DAT_B - ORG_B CVL_B - - - - -</td>\n",
       "      <td>비토리오 양일 만에 영사관 감호 용퇴, 항룡 압력설 의심만 가율</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- - - NUM_B NUM_B -</td>\n",
       "      <td>이 음경동맥의 직경이 $NUM$ 19mm입니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NUM_B - NUM_B ORG_B PER_B - NUM_B - - NUM_B - ...</td>\n",
       "      <td>9세이브로 구완 30위인 lg 박찬형은 평균자책점이 16.45로 준수한 편이지만 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NUM_B NUM_B LOC_B - EVT_B - - - - -</td>\n",
       "      <td>7승 25패는 상트페테르부르크가 역대 월드리그에 출진한 분별 최선의 성적이다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- PER_B CVL_B -</td>\n",
       "      <td>▲ 퍼거슨 씨족의 꾀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EVT_B CVL_B - - CVL_B - - - -</td>\n",
       "      <td>[유로2008] '공인구가 변할 기록 시정조치는 죽을 맛 ? '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EVT_B TRM_B TRM_I CVL_B - PER_B DAT_B EVT_B EV...</td>\n",
       "      <td>로마올림픽에서 육미지황탕 이남지역으로 동메달에 머문 추경대는 차년 파리오픈 결승전에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>- CVL_B CVL_I NUM_B NUM_I DAT_B TIM_B TIM_I TI...</td>\n",
       "      <td>금반 명기 통합우승 24, 10회차는 8일 상오 6시 50분, 상오 11시 50분에...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>- - - - - - -</td>\n",
       "      <td>권뢰가 있는 곳에 직경에 따라 달라지는데요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>- - - - - - -</td>\n",
       "      <td>때로는은 귀여운 가스나기인 비담, 세상일에는 무관심 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0                PER_B DAT_B - ORG_B CVL_B - - - - -   \n",
       "1                                - - - NUM_B NUM_B -   \n",
       "2  NUM_B - NUM_B ORG_B PER_B - NUM_B - - NUM_B - ...   \n",
       "3                NUM_B NUM_B LOC_B - EVT_B - - - - -   \n",
       "4                                    - PER_B CVL_B -   \n",
       "5                      EVT_B CVL_B - - CVL_B - - - -   \n",
       "6  EVT_B TRM_B TRM_I CVL_B - PER_B DAT_B EVT_B EV...   \n",
       "7  - CVL_B CVL_I NUM_B NUM_I DAT_B TIM_B TIM_I TI...   \n",
       "8                                      - - - - - - -   \n",
       "9                                      - - - - - - -   \n",
       "\n",
       "                                               words  \n",
       "0                비토리오 양일 만에 영사관 감호 용퇴, 항룡 압력설 의심만 가율  \n",
       "1                        이 음경동맥의 직경이 $NUM$ 19mm입니다 .  \n",
       "2  9세이브로 구완 30위인 lg 박찬형은 평균자책점이 16.45로 준수한 편이지만 2...  \n",
       "3       7승 25패는 상트페테르부르크가 역대 월드리그에 출진한 분별 최선의 성적이다 .  \n",
       "4                                        ▲ 퍼거슨 씨족의 꾀  \n",
       "5                [유로2008] '공인구가 변할 기록 시정조치는 죽을 맛 ? '  \n",
       "6  로마올림픽에서 육미지황탕 이남지역으로 동메달에 머문 추경대는 차년 파리오픈 결승전에...  \n",
       "7  금반 명기 통합우승 24, 10회차는 8일 상오 6시 50분, 상오 11시 50분에...  \n",
       "8                          권뢰가 있는 곳에 직경에 따라 달라지는데요 .  \n",
       "9                     때로는은 귀여운 가스나기인 비담, 세상일에는 무관심 .  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words, self.all_chars = [], [], []\n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        words_col, tags_col = [], []\n",
    "        with open(path) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    if len(words) != 0:\n",
    "                        words_col.append(' '.join(words))\n",
    "                        tags_col.append(' '.join(tags))\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split('\\t')\n",
    "                    word, tag = ls[1], ls[2]\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_tags.append(tag)\n",
    "                    self.all_chars.extend(list(word))\n",
    "                    \n",
    "                    \n",
    "        return pd.DataFrame({'words': words_col, 'tags': tags_col})\n",
    "        \n",
    "    def download_and_load_datasets(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"naver_challenge_ner_train_data.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/naver_challenge_ner_train_data.zip\", \n",
    "          extract=True)\n",
    "\n",
    "        train_df = self.load_dataset(os.path.join(os.path.dirname(dataset), 'naver_challenge_ner_train_data'))\n",
    "        return train_df\n",
    "\n",
    "dataset = Dataset()\n",
    "df = dataset.download_and_load_datasets()\n",
    "\n",
    "train_end = int(len(df) * 0.9)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "dev_df = df.iloc[train_end:]\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "char_list = list(set(dataset.all_chars)) + [Config.PAD, Config.UNK]\n",
    "char2idx = {w: i for i, w in enumerate(char_list)}\n",
    "idx2char = {i: w for i, w in enumerate(char_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               char_size,\n",
    "               word_embedding_dim, \n",
    "               char_embedding_dim,\n",
    "               hidden_size_word,\n",
    "               hidden_size_char):\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name='word_ids') \n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "        \n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name='char_ids') # [batch_size, max_sequence_length, max_word_length]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\") # [batch_size, max_sequence_length]\n",
    "        \n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('word-embedding'):\n",
    "            self._word_embeddings = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim], -0.25, 0.25), name='_word_embeddings', trainable=False)\n",
    "            self.word_embeddings = tf.nn.embedding_lookup(self._word_embeddings, self.word_ids) # [batch_size, max_sequence_length, word_embedding_dim]\n",
    "        \n",
    "        # Char Embedding Layer\n",
    "        with tf.variable_scope('char-embedding'):\n",
    "            self._char_embeddings = tf.get_variable(dtype=tf.float32, shape=[char_size, char_embedding_dim], name='_char_embeddings')\n",
    "            \n",
    "            # [batch_size, max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            self.char_embeddings = tf.nn.embedding_lookup(self._char_embeddings, self.char_ids) \n",
    "            \n",
    "            s = tf.shape(self.char_embeddings)\n",
    "            \n",
    "            # [batch_size*max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            char_embeddings = tf.reshape(self.char_embeddings, shape=[s[0]*s[1], s[2], char_embedding_dim])\n",
    "            word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "            \n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            \n",
    "            _, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, \n",
    "                                                                                   cell_bw=bw_cell, \n",
    "                                                                                   inputs=char_embeddings,\n",
    "                                                                                   sequence_length=word_lengths,\n",
    "                                                                                   dtype=tf.float32)\n",
    "            # shape: [batch_size*max_sequnce_length, 2*hidden_size_char]\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.reshape(output, shape=[s[0], s[1], 2*hidden_size_char])\n",
    "            \n",
    "            # shape: # [batch_size, max_sequence_length, word_embedding_dim + 2*hidden_size_char]\n",
    "            self.word_embeddings = tf.concat([self.word_embeddings, output], axis=-1) \n",
    "            self.word_embeddings = tf.nn.dropout(self.word_embeddings, self.dropout)\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.word_embeddings,\n",
    "                                                                  sequence_length= self.sequence_lengths, # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, max_sequence_length, 2*hidden_size_word]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "        \n",
    "        \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size_word, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tf.shape(self.rnn_outputs)[1]\n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size_word])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, num_classes]) # [batch_size, max_sequence_length, num_classes]\n",
    "    \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params  # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "            \n",
    "#             When CRF is not in use\n",
    "#             self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "#             mask = tf.sequence_mask(self.sequence_lengths)\n",
    "#             losses = tf.boolean_mask(self.losses, mask)\n",
    "#             self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @staticmethod\n",
    "    def viterbi_decode(logits, trans_params):\n",
    "        # get tag scores and transition params of CRF\n",
    "        viterbi_sequences = []\n",
    "\n",
    "        # iterate over the sentences because no batching in vitervi_decode\n",
    "        for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "            logit = logit[:sequence_length]  # keep only the valid steps\n",
    "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                logit, trans_params)\n",
    "            viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "        return np.array(viterbi_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(word_embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), word_embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(df, batch_size, num_epochs, shuffle=True, tqdm_disable=False):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(df)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs), disable=tqdm_disable):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_df= df.iloc[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_df = df\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_df.iloc[start_index:end_index]    \n",
    "            \n",
    "\n",
    "def get_feed_dict(batch_df):\n",
    "    max_length = max(map(lambda x : len(x.split(' ')), batch_df['words'].tolist()))\n",
    "    \n",
    "    max_length_word = 0\n",
    "    for seq in batch_df['words'].tolist():\n",
    "        for word in seq.split(' '):\n",
    "            max_length_word = max(max_length_word, len(word))\n",
    "    \n",
    "    word_ids, sequence_lengths, labels, char_ids, word_lengths = [], [], [], [], []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        sentence = row['words'].split(' ')\n",
    "        tags = row['tags'].split(' ')\n",
    "\n",
    "        word_ids_row, labels_row, char_ids_row, word_lengths_row = [], [], [], []\n",
    "        for word in sentence:\n",
    "            word_ids_row.append(word2idx[word])\n",
    "        \n",
    "            char_ids_row.append([char2idx[char] for char in word] + [char2idx[Config.PAD]]* (max_length_word - len(word)) )\n",
    "            word_lengths_row.append(len(word))\n",
    "        \n",
    "        empty_char_ids = [char2idx[Config.PAD]]* max_length_word\n",
    "        char_ids_row += [empty_char_ids] * (max_length - len(char_ids_row))\n",
    "        word_lengths_row += [0] * (max_length - len(word_lengths_row))\n",
    "        \n",
    "        for tag in tags:\n",
    "            labels_row.append(tag2idx[tag])\n",
    "\n",
    "        if len(sentence) < max_length:\n",
    "            word_ids_row += [word2idx[Config.PAD]]* (max_length - len(sentence))\n",
    "            labels_row += [tag2idx[Config.NONE]]* (max_length - len(sentence))\n",
    "\n",
    "        word_ids.append(word_ids_row)\n",
    "        labels.append(labels_row)\n",
    "        sequence_lengths.append(len(sentence))\n",
    "        char_ids.append(char_ids_row)\n",
    "        word_lengths.append(word_lengths_row)\n",
    "    \n",
    "    word_ids = np.array(word_ids)\n",
    "    labels = np.array(labels)\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    char_ids = np.array(char_ids)\n",
    "    word_lengths = np.array(word_lengths)\n",
    "    \n",
    "    return word_ids, labels, sequence_lengths, char_ids, word_lengths\n",
    "\n",
    "\n",
    "def get_chunk_type(tok):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tok: id of token, ex 4\n",
    "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
    "\n",
    "    Returns:\n",
    "        tuple: \"B\", \"PER\"\n",
    "\n",
    "    \"\"\"\n",
    "    tag_class = tok.split('-')[0]\n",
    "    tag_type = tok.split('-')[-1]\n",
    "    return tag_class, tag_type\n",
    "\n",
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    default = Config.NONE\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def evaluation(y, preds, lengths):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answers, arg_preds = [], []\n",
    "    \n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0.0, 0.0, 0.0\n",
    "    for i in range(len(y)):\n",
    "        sent_answers = []\n",
    "        sent_preds = []\n",
    "        for j in range(lengths[i]):\n",
    "            sent_answers.append(idx2tag[y[i][j]])\n",
    "            sent_preds.append(idx2tag[preds[i][j]])\n",
    "    \n",
    "        arg_answers.extend(sent_answers)\n",
    "        arg_preds.extend(sent_preds)\n",
    "        \n",
    "        accs += [a == b for (a, b) in zip(sent_answers, sent_preds)]\n",
    "        \n",
    "        sent_answer_chunks = set(get_chunks(sent_answers, tag_list))\n",
    "        sent_pred_chunks = set(get_chunks(sent_preds, tag_list))\n",
    "\n",
    "        correct_preds += len(sent_answer_chunks & sent_pred_chunks)\n",
    "        total_preds += len(sent_pred_chunks)\n",
    "        total_correct += len(sent_answer_chunks)\n",
    "    \n",
    "    p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "        \n",
    "    print(classification_report(arg_answers, arg_preds))\n",
    "    \n",
    "    print('Chunk based evaluation: acc: {}, f1: {}'.format(acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-5e6a4e6653fd>:41: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-5e6a4e6653fd>:48: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-5e6a4e6653fd>:55: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/34.runs/1559044976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        char_size=len(char_list),\n",
    "        word_embedding_dim=Config.word_embedding_dim,\n",
    "        char_embedding_dim=Config.char_embedding_dim,\n",
    "        hidden_size_word=Config.hidden_size_word,\n",
    "        hidden_size_char=Config.hidden_size_char\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # train_op = tf.train.AdamOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(Config.learning_rate, Config.decay_rate, 1e-6)\n",
    "    gvs = optimizer.compute_gradients(model.loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"34.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     pretrain_W = load_glove(Config.word_embedding_dim, word2idx)\n",
    "#     sess.run(model._word_embeddings.assign(pretrain_W))\n",
    "#     print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(train_df, Config.batch_size, Config.num_epochs)\n",
    "    for batch_df in batches:\n",
    "        word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "        feed_dict = {\n",
    "            model.word_ids: word_ids,\n",
    "            model.labels: labels,\n",
    "            model.sequence_lengths: sequence_lengths,\n",
    "            model.char_ids: char_ids,\n",
    "            model.word_lengths: word_lengths,\n",
    "            model.dropout: 0.5,\n",
    "        }\n",
    "        _, step, loss, logits, trans_params = sess.run([\n",
    "            train_op, global_step, model.loss, model.logits, model.trans_params], feed_dict)\n",
    "        \n",
    "        predictions = model.viterbi_decode(logits, trans_params)\n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}\".format(time_str, step, loss))\n",
    "            \n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            batches = batch_iter(dev_df, Config.batch_size, 1, tqdm_disable=True)\n",
    "            \n",
    "            total_loss, predictions_all, labels_all, sequence_lengths_all  = 0, [], [], []\n",
    "            for batch_df in batches:\n",
    "                word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "                feed_dict = {\n",
    "                    model.word_ids: word_ids,\n",
    "                    model.labels: labels,\n",
    "                    model.sequence_lengths: sequence_lengths,\n",
    "                    model.char_ids: char_ids,\n",
    "                    model.word_lengths: word_lengths,\n",
    "                    model.dropout: 1.0,\n",
    "                }\n",
    "                loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "                predictions = model.viterbi_decode(logits, trans_params)\n",
    "                \n",
    "                total_loss += loss\n",
    "                predictions_all += predictions.tolist()\n",
    "                labels_all += labels.tolist()\n",
    "                sequence_lengths_all += sequence_lengths.tolist()\n",
    "        \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"\\nDev Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "            evaluation(labels_all, predictions_all, sequence_lengths_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
