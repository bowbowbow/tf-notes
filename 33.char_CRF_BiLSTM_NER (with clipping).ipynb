{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Embeddings\n",
    "    word_embedding_dim = 300\n",
    "    char_embedding_dim = 100\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size_word = 300\n",
    "    hidden_size_char = 100\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 20\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    learning_rate = 1.0\n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"O\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Load annotated corpus for named entity recognition\n",
    "\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-ORG O I-MISC O O O I-MISC O O</td>\n",
       "      <td>eu rejects german call to boycott british lamb .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-PER I-PER</td>\n",
       "      <td>peter blackburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-LOC O</td>\n",
       "      <td>brussels 1996-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O I-ORG I-ORG O O O O O O I-MISC O O O O O I-M...</td>\n",
       "      <td>the european commission said on thursday it di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-LOC O O O O I-ORG I-ORG O O O I-PER I-PER O ...</td>\n",
       "      <td>germany 's representative to the european unio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O I-ORG ...</td>\n",
       "      <td>\" we do n't support any such recommendation be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O I-...</td>\n",
       "      <td>he said further scientific study was required ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O O O O O O O I-ORG O O I-PER I-PER O O O O O ...</td>\n",
       "      <td>he said a proposal last month by eu farm commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PER O I-MISC O O O O I-LOC O I-LOC O O O O O...</td>\n",
       "      <td>fischler proposed eu-wide measures after repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O I-PER O O O O O O O I-ORG O O O O O O O O O ...</td>\n",
       "      <td>but fischler agreed to review his proposal aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0                    I-ORG O I-MISC O O O I-MISC O O   \n",
       "1                                        I-PER I-PER   \n",
       "2                                            I-LOC O   \n",
       "3  O I-ORG I-ORG O O O O O O I-MISC O O O O O I-M...   \n",
       "4  I-LOC O O O O I-ORG I-ORG O O O I-PER I-PER O ...   \n",
       "5  O O O O O O O O O O O O O O O O O O O O I-ORG ...   \n",
       "6  O O O O O O O O O O O O O O O O O O O O O O I-...   \n",
       "7  O O O O O O O I-ORG O O I-PER I-PER O O O O O ...   \n",
       "8  I-PER O I-MISC O O O O I-LOC O I-LOC O O O O O...   \n",
       "9  O I-PER O O O O O O O I-ORG O O O O O O O O O ...   \n",
       "\n",
       "                                               words  \n",
       "0   eu rejects german call to boycott british lamb .  \n",
       "1                                    peter blackburn  \n",
       "2                                brussels 1996-08-22  \n",
       "3  the european commission said on thursday it di...  \n",
       "4  germany 's representative to the european unio...  \n",
       "5  \" we do n't support any such recommendation be...  \n",
       "6  he said further scientific study was required ...  \n",
       "7  he said a proposal last month by eu farm commi...  \n",
       "8  fischler proposed eu-wide measures after repor...  \n",
       "9  but fischler agreed to review his proposal aft...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words, self.all_chars = [], [], []\n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        words_col, tags_col = [], []\n",
    "        with open(path) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(words) != 0:\n",
    "                        words_col.append(' '.join(words))\n",
    "                        tags_col.append(' '.join(tags))\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, tag = ls[0], ls[3]\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_tags.append(tag)\n",
    "                    self.all_chars.extend(list(word))\n",
    "                    \n",
    "                    \n",
    "        return pd.DataFrame({'words': words_col, 'tags': tags_col})\n",
    "        \n",
    "    def download_and_load_datasets(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"CoNLL-2003.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/CoNLL-2003.zip\", \n",
    "          extract=True)\n",
    "        \n",
    "        dir_path = os.path.join(os.path.dirname(dataset), 'CoNLL-2003')\n",
    "        train_df = self.load_dataset(os.path.join(dir_path, 'eng.train'))\n",
    "        dev_df = self.load_dataset(os.path.join(dir_path, 'eng.testa'))\n",
    "        test_df = self.load_dataset(os.path.join(dir_path, 'eng.testb'))\n",
    "        return train_df, dev_df, test_df\n",
    "\n",
    "dataset = Dataset()\n",
    "train_df, dev_df, test_df = dataset.download_and_load_datasets()\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "char_list = list(set(dataset.all_chars)) + [Config.PAD, Config.UNK]\n",
    "char2idx = {w: i for i, w in enumerate(char_list)}\n",
    "idx2char = {i: w for i, w in enumerate(char_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               char_size,\n",
    "               word_embedding_dim, \n",
    "               char_embedding_dim,\n",
    "               hidden_size_word,\n",
    "               hidden_size_char):\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name='word_ids') \n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "        \n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name='char_ids') # [batch_size, max_sequence_length, max_word_length]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\") # [batch_size, max_sequence_length]\n",
    "        \n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('word-embedding'):\n",
    "            self._word_embeddings = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim], -0.25, 0.25), name='_word_embeddings', trainable=False)\n",
    "            self.word_embeddings = tf.nn.embedding_lookup(self._word_embeddings, self.word_ids) # [batch_size, max_sequence_length, word_embedding_dim]\n",
    "        \n",
    "        # Char Embedding Layer\n",
    "        with tf.variable_scope('char-embedding'):\n",
    "            self._char_embeddings = tf.get_variable(dtype=tf.float32, shape=[char_size, char_embedding_dim], name='_char_embeddings')\n",
    "            \n",
    "            # [batch_size, max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            self.char_embeddings = tf.nn.embedding_lookup(self._char_embeddings, self.char_ids) \n",
    "            \n",
    "            s = tf.shape(self.char_embeddings)\n",
    "            \n",
    "            # [batch_size*max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            char_embeddings = tf.reshape(self.char_embeddings, shape=[s[0]*s[1], s[2], char_embedding_dim])\n",
    "            word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "            \n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            \n",
    "            _, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, \n",
    "                                                                                   cell_bw=bw_cell, \n",
    "                                                                                   inputs=char_embeddings,\n",
    "                                                                                   sequence_length=word_lengths,\n",
    "                                                                                   dtype=tf.float32)\n",
    "            # shape: [batch_size*max_sequnce_length, 2*hidden_size_char]\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.reshape(output, shape=[s[0], s[1], 2*hidden_size_char])\n",
    "            \n",
    "            # shape: # [batch_size, max_sequence_length, word_embedding_dim + 2*hidden_size_char]\n",
    "            self.word_embeddings = tf.concat([self.word_embeddings, output], axis=-1) \n",
    "            self.word_embeddings = tf.nn.dropout(self.word_embeddings, self.dropout)\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.word_embeddings,\n",
    "                                                                  sequence_length= self.sequence_lengths, # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, max_sequence_length, 2*hidden_size_word]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "        \n",
    "        \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size_word, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tf.shape(self.rnn_outputs)[1]\n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size_word])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, num_classes]) # [batch_size, max_sequence_length, num_classes]\n",
    "    \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params  # need to evaluate it for decoding\n",
    "            self.loss = tf.reduce_mean(-log_likelihood)\n",
    "            \n",
    "#             When CRF is not in use\n",
    "#             self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "#             mask = tf.sequence_mask(self.sequence_lengths)\n",
    "#             losses = tf.boolean_mask(self.losses, mask)\n",
    "#             self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @staticmethod\n",
    "    def viterbi_decode(logits, trans_params):\n",
    "        # get tag scores and transition params of CRF\n",
    "        viterbi_sequences = []\n",
    "\n",
    "        # iterate over the sentences because no batching in vitervi_decode\n",
    "        for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "            logit = logit[:sequence_length]  # keep only the valid steps\n",
    "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                logit, trans_params)\n",
    "            viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "        return np.array(viterbi_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(word_embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), word_embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(df, batch_size, num_epochs, shuffle=True, tqdm_disable=False):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(df)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs), disable=tqdm_disable):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_df= df.iloc[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_df = df\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_df.iloc[start_index:end_index]    \n",
    "            \n",
    "\n",
    "def get_feed_dict(batch_df):\n",
    "    max_length = max(map(lambda x : len(x.split(' ')), batch_df['words'].tolist()))\n",
    "    \n",
    "    max_length_word = 0\n",
    "    for seq in batch_df['words'].tolist():\n",
    "        for word in seq.split(' '):\n",
    "            max_length_word = max(max_length_word, len(word))\n",
    "    \n",
    "    word_ids, sequence_lengths, labels, char_ids, word_lengths = [], [], [], [], []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        sentence = row['words'].split(' ')\n",
    "        tags = row['tags'].split(' ')\n",
    "\n",
    "        word_ids_row, labels_row, char_ids_row, word_lengths_row = [], [], [], []\n",
    "        for word in sentence:\n",
    "            word_ids_row.append(word2idx[word])\n",
    "        \n",
    "            char_ids_row.append([char2idx[char] for char in word] + [char2idx[Config.PAD]]* (max_length_word - len(word)) )\n",
    "            word_lengths_row.append(len(word))\n",
    "        \n",
    "        empty_char_ids = [char2idx[Config.PAD]]* max_length_word\n",
    "        char_ids_row += [empty_char_ids] * (max_length - len(char_ids_row))\n",
    "        word_lengths_row += [0] * (max_length - len(word_lengths_row))\n",
    "        \n",
    "        for tag in tags:\n",
    "            labels_row.append(tag2idx[tag])\n",
    "\n",
    "        if len(sentence) < max_length:\n",
    "            word_ids_row += [word2idx[Config.PAD]]* (max_length - len(sentence))\n",
    "            labels_row += [tag2idx[Config.NONE]]* (max_length - len(sentence))\n",
    "\n",
    "        word_ids.append(word_ids_row)\n",
    "        labels.append(labels_row)\n",
    "        sequence_lengths.append(len(sentence))\n",
    "        char_ids.append(char_ids_row)\n",
    "        word_lengths.append(word_lengths_row)\n",
    "    \n",
    "    word_ids = np.array(word_ids)\n",
    "    labels = np.array(labels)\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    char_ids = np.array(char_ids)\n",
    "    word_lengths = np.array(word_lengths)\n",
    "    \n",
    "    return word_ids, labels, sequence_lengths, char_ids, word_lengths\n",
    "\n",
    "def evaluation(y, preds, lengths):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answer, arg_pred = [], []\n",
    "    for i in range(len(y)):\n",
    "        for j in range(lengths[i]):\n",
    "            arg_answer.append(idx2tag[y[i][j]])\n",
    "            arg_pred.append(idx2tag[preds[i][j]])\n",
    "\n",
    "    print(classification_report(arg_answer, arg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-5e6a4e6653fd>:41: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-5e6a4e6653fd>:48: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-5e6a4e6653fd>:55: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/30.runs/1559032651\n",
      "\n",
      "embedding_path : /home/seungwon/.keras/datasets/glove.6B.300d.txt\n",
      "Load glove file /home/seungwon/.keras/datasets/glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to load pre-trained glove model!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:07<10:04, 33.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:39:28.042765: step 500, loss 1.36262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [02:12<08:52, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:40:43.046972: step 1000, loss 1.29089\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-28T17:40:47.463866: loss 0.0177502\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.92      0.93      0.93      2094\n",
      "      I-MISC       0.94      0.73      0.82      1264\n",
      "       I-ORG       0.91      0.75      0.82      2092\n",
      "       I-PER       0.96      0.97      0.97      3149\n",
      "           O       0.98      1.00      0.99     42759\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     51362\n",
      "   macro avg       0.79      0.73      0.75     51362\n",
      "weighted avg       0.97      0.97      0.97     51362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [03:23<07:58, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:42:02.913528: step 1500, loss 0.855374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [05:03<06:07, 33.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:43:17.997388: step 2000, loss 0.66434\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-28T17:43:22.101042: loss 0.0144444\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       1.00      0.25      0.40         4\n",
      "       I-LOC       0.94      0.94      0.94      2094\n",
      "      I-MISC       0.93      0.78      0.85      1264\n",
      "       I-ORG       0.92      0.82      0.87      2092\n",
      "       I-PER       0.98      0.97      0.98      3149\n",
      "           O       0.99      1.00      0.99     42759\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     51362\n",
      "   macro avg       0.96      0.79      0.84     51362\n",
      "weighted avg       0.98      0.98      0.98     51362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [06:13<05:07, 34.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:44:37.766813: step 2500, loss 0.671433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [07:19<03:55, 33.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:45:52.845870: step 3000, loss 0.297675\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-28T17:45:56.921773: loss 0.0147202\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       1.00      0.25      0.40         4\n",
      "       I-LOC       0.94      0.94      0.94      2094\n",
      "      I-MISC       0.92      0.80      0.86      1264\n",
      "       I-ORG       0.94      0.79      0.86      2092\n",
      "       I-PER       0.97      0.98      0.97      3149\n",
      "           O       0.99      1.00      0.99     42759\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     51362\n",
      "   macro avg       0.96      0.79      0.84     51362\n",
      "weighted avg       0.98      0.98      0.98     51362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [08:30<02:51, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:47:12.183949: step 3500, loss 0.497705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [10:09<01:07, 33.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28T17:48:27.596385: step 4000, loss 0.47758\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-28T17:48:31.699727: loss 0.0142638\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.33      0.25      0.29         4\n",
      "       I-LOC       0.94      0.95      0.95      2094\n",
      "      I-MISC       0.88      0.82      0.85      1264\n",
      "       I-ORG       0.90      0.86      0.88      2092\n",
      "       I-PER       0.97      0.98      0.97      3149\n",
      "           O       0.99      0.99      0.99     42759\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     51362\n",
      "   macro avg       0.84      0.81      0.82     51362\n",
      "weighted avg       0.98      0.98      0.98     51362\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [11:19<00:00, 34.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        char_size=len(char_list),\n",
    "        word_embedding_dim=Config.word_embedding_dim,\n",
    "        char_embedding_dim=Config.char_embedding_dim,\n",
    "        hidden_size_word=Config.hidden_size_word,\n",
    "        hidden_size_char=Config.hidden_size_char\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # train_op = tf.train.AdamOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(Config.learning_rate, Config.decay_rate, 1e-6)\n",
    "    gvs = optimizer.compute_gradients(model.loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"30.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    pretrain_W = load_glove(Config.word_embedding_dim, word2idx)\n",
    "    sess.run(model._word_embeddings.assign(pretrain_W))\n",
    "    print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(train_df, Config.batch_size, Config.num_epochs)\n",
    "    for batch_df in batches:\n",
    "        word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "        feed_dict = {\n",
    "            model.word_ids: word_ids,\n",
    "            model.labels: labels,\n",
    "            model.sequence_lengths: sequence_lengths,\n",
    "            model.char_ids: char_ids,\n",
    "            model.word_lengths: word_lengths,\n",
    "            model.dropout: 0.5,\n",
    "        }\n",
    "        _, step, loss, logits, trans_params = sess.run([\n",
    "            train_op, global_step, model.loss, model.logits, model.trans_params], feed_dict)\n",
    "        \n",
    "        predictions = model.viterbi_decode(logits, trans_params)\n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}\".format(time_str, step, loss))\n",
    "            \n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            batches = batch_iter(dev_df, Config.batch_size, 1, tqdm_disable=True)\n",
    "            \n",
    "            total_loss, predictions_all, labels_all, sequence_lengths_all  = 0, [], [], []\n",
    "            for batch_df in batches:\n",
    "                word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "                feed_dict = {\n",
    "                    model.word_ids: word_ids,\n",
    "                    model.labels: labels,\n",
    "                    model.sequence_lengths: sequence_lengths,\n",
    "                    model.char_ids: char_ids,\n",
    "                    model.word_lengths: word_lengths,\n",
    "                    model.dropout: 1.0,\n",
    "                }\n",
    "                loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "                predictions = model.viterbi_decode(logits, trans_params)\n",
    "                \n",
    "                total_loss += loss\n",
    "                predictions_all += predictions.tolist()\n",
    "                labels_all += labels.tolist()\n",
    "                sequence_lengths_all += sequence_lengths.tolist()\n",
    "        \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"\\nDev Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "            evaluation(labels_all, predictions_all, sequence_lengths_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Evaluation\n",
      "2019-05-28T17:49:35.939686: loss 0.000532494\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       1.00      0.11      0.20         9\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.90      0.90      0.90      1919\n",
      "      I-MISC       0.78      0.78      0.78       909\n",
      "       I-ORG       0.86      0.85      0.85      2491\n",
      "       I-PER       0.97      0.96      0.97      2773\n",
      "           O       0.99      0.99      0.99     38323\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     46435\n",
      "   macro avg       0.69      0.57      0.59     46435\n",
      "weighted avg       0.97      0.97      0.97     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batches = batch_iter(test_df, Config.batch_size, 1)\n",
    "total_loss, predictions_all, labels_all, sequence_lengths_all  = 0, [], [], []\n",
    "for batch_df in batches:\n",
    "    word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "    feed_dict = {\n",
    "        model.word_ids: word_ids,\n",
    "        model.labels: labels,\n",
    "        model.sequence_lengths: sequence_lengths,\n",
    "        model.char_ids: char_ids,\n",
    "        model.word_lengths: word_lengths,\n",
    "        model.dropout: 1.0,\n",
    "    }\n",
    "    \n",
    "    total_loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "    predictions = model.viterbi_decode(logits, trans_params)\n",
    "    \n",
    "    total_loss += loss\n",
    "    predictions_all += predictions.tolist()\n",
    "    labels_all += labels.tolist()\n",
    "    sequence_lengths_all += sequence_lengths.tolist()\n",
    "\n",
    "time_str = datetime.datetime.now().isoformat()\n",
    "print(\"\\nTest Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "evaluation(labels_all, predictions_all, sequence_lengths_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
