{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data loading params\n",
    "    max_sentence_length = 40\n",
    "    dev_sample_percentage = 0.1\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_dim = 300\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size = 300\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 40\n",
    "    num_epochs = 15\n",
    "    display_every = 500\n",
    "    evaluate_every = 1500\n",
    "    num_checkpoints = 5\n",
    "    learning_rate = 1.0\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Load annotated corpus for named entity recognition\n",
    "\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O\n",
       "5          NaN        through   IN      O\n",
       "6          NaN         London  NNP  B-geo\n",
       "7          NaN             to   TO      O\n",
       "8          NaN        protest   VB      O\n",
       "9          NaN            the   DT      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def download_and_load_datasets(self):\n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"entity-annotated-corpus.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/entity-annotated-corpus.zip\", \n",
    "          extract=True)\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(os.path.dirname(dataset), 'ner_dataset.csv'), encoding=\"latin1\")\n",
    "        return df\n",
    "\n",
    "dataset = Dataset()\n",
    "df = dataset.download_and_load_datasets()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  1048575\n",
      "Number of unique items: 47959 35178 17\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size: ', len(df))\n",
    "print('Number of unique items:', df['Sentence #'].nunique(), df.Word.nunique(), df.Tag.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag distribution\n",
      "      Tag   count\n",
      "0   B-art     402\n",
      "1   B-eve     308\n",
      "2   B-geo   37644\n",
      "3   B-gpe   15870\n",
      "4   B-nat     201\n",
      "5   B-org   20143\n",
      "6   B-per   16990\n",
      "7   B-tim   20333\n",
      "8   I-art     297\n",
      "9   I-eve     253\n",
      "10  I-geo    7414\n",
      "11  I-gpe     198\n",
      "12  I-nat      51\n",
      "13  I-org   16784\n",
      "14  I-per   17251\n",
      "15  I-tim    6528\n",
      "16      O  887908\n"
     ]
    }
   ],
   "source": [
    "print('Tag distribution')\n",
    "print(df.groupby('Tag').size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O\n",
       "3  Sentence: 1           have  VBP   O\n",
       "4  Sentence: 1        marched  VBN   O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill NaN by value of preceding row\n",
    "df = df.fillna(method=\"ffill\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words(before) : 35178\n",
      "The number of unique words(after) : 31817\n"
     ]
    }
   ],
   "source": [
    "# To remove duplicate words \n",
    "print('The number of unique words(before) :', df.Word.nunique())\n",
    "df['Word'] = df['Word'].str.lower()\n",
    "print('The number of unique words(after) :', df.Word.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('thousands', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('demonstrators', 'O'),\n",
       "  ('have', 'O'),\n",
       "  ('marched', 'O'),\n",
       "  ('through', 'O'),\n",
       "  ('london', 'B-geo'),\n",
       "  ('to', 'O'),\n",
       "  ('protest', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('war', 'O'),\n",
       "  ('in', 'O'),\n",
       "  ('iraq', 'B-geo'),\n",
       "  ('and', 'O'),\n",
       "  ('demand', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('withdrawal', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('british', 'B-gpe'),\n",
       "  ('troops', 'O'),\n",
       "  ('from', 'O'),\n",
       "  ('that', 'O'),\n",
       "  ('country', 'O'),\n",
       "  ('.', 'O')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by same sentence\n",
    "data = []\n",
    "\n",
    "for key, group in df.groupby('Sentence #'):\n",
    "    words = []\n",
    "    for w, t in zip(group['Word'].values.tolist(), group['Tag'].values.tolist()):\n",
    "        words.append((w,t))\n",
    "    data.append(words)\n",
    "\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Length distribution\n",
    "# This gives insight to determine the maximum sentence length.\n",
    "plt.hist([len(s) for s in data], bins=50)\n",
    "plt.xlabel('length of data')\n",
    "plt.ylabel('number of data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47959/47959 [00:00<00:00, 73637.13it/s]\n"
     ]
    }
   ],
   "source": [
    "word_list = list(set(df['Word'].values.tolist() )) + ['<PAD>', '<UNK>']\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(df['Tag'].values.tolist()))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "x, lengths, y = [], [], []\n",
    "for sentence in tqdm(data):\n",
    "    sentence = sentence[:Config.max_sentence_length]\n",
    "    lengths.append(Config.max_sentence_length)\n",
    "    if len(sentence) < Config.max_sentence_length:\n",
    "        lengths[-1] = len(sentence)\n",
    "        sentence += [('<PAD>', 'O')] * (Config.max_sentence_length - len(sentence))\n",
    "    \n",
    "    x_row, y_row = [], []\n",
    "    for word, tag in sentence:\n",
    "        x_row.append(word2idx[word])\n",
    "        y_row.append(tag2idx[tag])\n",
    "    \n",
    "    x.append(x_row)\n",
    "    y.append(y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Dev split: 43164/4795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "lengths = np.array(lengths)\n",
    "\n",
    "# Randomly shuffle data to split into train and dev\n",
    "np.random.seed(11)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "\n",
    "x_shuffled, lengths_shuffled, y_shuffled = x[shuffle_indices], lengths[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "# Split train/dev set\n",
    "dev_sample_index = -1 * int(Config.dev_sample_percentage*float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:] \n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:] \n",
    "lengths_train, lengths_dev = lengths_shuffled[:dev_sample_index], lengths_shuffled[dev_sample_index:]\n",
    "\n",
    "print(\"Train/Dev split: {:d}/{:d}\\n\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               sequence_length, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               embedding_size, \n",
    "               hidden_size):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_y')\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('text-embedding'):\n",
    "            self.W_text = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -0.25, 0.25), name='W_text')\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_x) # [batch_size, sequence_length, embedding_size]\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, initializer=initializer)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size, initializer=initializer)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.embedded_chars,\n",
    "                                                                  sequence_length= self._length(self.input_x), # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, sequence_length, 2*hidden_size]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "            \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size, num_classes],  dtype=tf.float32, initializer=initializer)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, sequence_length, num_classes]) # [batch_size, sequence_length, num_classes]\n",
    "            \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            mask = tf.sequence_mask(self.sequence_lengths)\n",
    "            losses = tf.boolean_mask(losses, mask)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "        # Accuracy    \n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.predictions = tf.argmax(self.logits, 2, name='predictions')\n",
    "            correct_predictions = tf.equal(self.predictions, tf.cast(self.input_y, tf.int64))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "            \n",
    "def evaluation(y, preds, lengths):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answer, arg_pred = [], []\n",
    "    for i in range(len(y)):\n",
    "        for j in range(lengths[i]):\n",
    "            arg_answer.append(idx2tag[y[i][j]])\n",
    "            arg_pred.append(idx2tag[preds[i][j]])\n",
    "\n",
    "    print(classification_report(arg_answer, arg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-12-a7f1928efb2d>:23: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-a7f1928efb2d>:29: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-12-a7f1928efb2d>:32: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/29.runs/1558025762\n",
      "\n",
      "embedding_path : /home/seungwon/.keras/datasets/glove.6B.300d.txt\n",
      "Load glove file /home/seungwon/.keras/datasets/glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to load pre-trained glove model!\n",
      "\n",
      "2019-05-17T01:57:05.142881: step 500, loss 0.323016, acc 0.95\n",
      "2019-05-17T01:57:22.586146: step 1000, loss 0.121825, acc 0.954375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/15 [00:38<08:52, 38.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:57:40.003960: step 1500, loss 0.128909, acc 0.95875\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.00      0.00      0.00        32\n",
      "       B-geo       0.76      0.85      0.80      3531\n",
      "       B-gpe       0.91      0.90      0.90      1565\n",
      "       B-nat       0.00      0.00      0.00        17\n",
      "       B-org       0.74      0.36      0.49      2029\n",
      "       B-per       0.79      0.67      0.73      1728\n",
      "       B-tim       0.86      0.80      0.83      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.00      0.00      0.00        27\n",
      "       I-geo       0.69      0.54      0.61       685\n",
      "       I-gpe       0.00      0.00      0.00        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.64      0.39      0.49      1694\n",
      "       I-per       0.77      0.86      0.81      1754\n",
      "       I-tim       0.86      0.48      0.61       639\n",
      "           O       0.97      0.99      0.98     88692\n",
      "\n",
      "   micro avg       0.94      0.94      0.94    104466\n",
      "   macro avg       0.47      0.40      0.43    104466\n",
      "weighted avg       0.94      0.94      0.94    104466\n",
      "\n",
      "2019-05-17T01:57:41.119919: step 1500, loss 0.202318, acc 0.9685\n",
      "\n",
      "2019-05-17T01:57:58.603298: step 2000, loss 0.110924, acc 0.966875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 2/15 [01:16<08:17, 38.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:58:16.052804: step 2500, loss 0.136309, acc 0.9675\n",
      "2019-05-17T01:58:33.477654: step 3000, loss 0.163101, acc 0.975\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       1.00      0.03      0.06        32\n",
      "       B-geo       0.81      0.86      0.83      3531\n",
      "       B-gpe       0.92      0.92      0.92      1565\n",
      "       B-nat       0.00      0.00      0.00        17\n",
      "       B-org       0.74      0.49      0.59      2029\n",
      "       B-per       0.79      0.75      0.77      1728\n",
      "       B-tim       0.91      0.82      0.86      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.00      0.00      0.00        27\n",
      "       I-geo       0.76      0.61      0.68       685\n",
      "       I-gpe       0.00      0.00      0.00        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.69      0.47      0.56      1694\n",
      "       I-per       0.83      0.86      0.84      1754\n",
      "       I-tim       0.85      0.62      0.72       639\n",
      "           O       0.97      0.99      0.98     88692\n",
      "\n",
      "   micro avg       0.95      0.95      0.95    104466\n",
      "   macro avg       0.55      0.44      0.46    104466\n",
      "weighted avg       0.95      0.95      0.95    104466\n",
      "\n",
      "2019-05-17T01:58:34.449028: step 3000, loss 0.165574, acc 0.9736\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 3/15 [01:55<07:40, 38.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:58:51.884074: step 3500, loss 0.125756, acc 0.981875\n",
      "2019-05-17T01:59:09.329256: step 4000, loss 0.0950182, acc 0.974375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 4/15 [02:33<06:59, 38.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:59:26.788558: step 4500, loss 0.170435, acc 0.971875\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.64      0.22      0.33        32\n",
      "       B-geo       0.83      0.87      0.85      3531\n",
      "       B-gpe       0.94      0.93      0.94      1565\n",
      "       B-nat       0.50      0.06      0.11        17\n",
      "       B-org       0.75      0.54      0.63      2029\n",
      "       B-per       0.79      0.79      0.79      1728\n",
      "       B-tim       0.90      0.83      0.87      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.00      0.00      0.00        27\n",
      "       I-geo       0.72      0.71      0.72       685\n",
      "       I-gpe       1.00      0.21      0.34        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.70      0.53      0.60      1694\n",
      "       I-per       0.84      0.86      0.85      1754\n",
      "       I-tim       0.87      0.63      0.73       639\n",
      "           O       0.98      0.99      0.98     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.62      0.48      0.51    104466\n",
      "weighted avg       0.95      0.96      0.95    104466\n",
      "\n",
      "2019-05-17T01:59:27.777567: step 4500, loss 0.148803, acc 0.9759\n",
      "\n",
      "2019-05-17T01:59:45.266115: step 5000, loss 0.142059, acc 0.97875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 5/15 [03:11<06:23, 38.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:00:02.767606: step 5500, loss 0.093999, acc 0.97625\n",
      "2019-05-17T02:00:20.263051: step 6000, loss 0.0818391, acc 0.983125\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.70      0.22      0.33        32\n",
      "       B-geo       0.80      0.90      0.85      3531\n",
      "       B-gpe       0.94      0.94      0.94      1565\n",
      "       B-nat       0.25      0.12      0.16        17\n",
      "       B-org       0.76      0.55      0.63      2029\n",
      "       B-per       0.82      0.78      0.80      1728\n",
      "       B-tim       0.89      0.87      0.88      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.00      0.00      0.00        27\n",
      "       I-geo       0.74      0.68      0.71       685\n",
      "       I-gpe       1.00      0.29      0.45        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.74      0.53      0.62      1694\n",
      "       I-per       0.84      0.88      0.86      1754\n",
      "       I-tim       0.86      0.67      0.75       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.61      0.49      0.53    104466\n",
      "weighted avg       0.95      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:00:21.250614: step 6000, loss 0.139379, acc 0.9770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 6/15 [03:50<05:46, 38.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:00:38.735043: step 6500, loss 0.0967997, acc 0.975625\n",
      "2019-05-17T02:00:56.197131: step 7000, loss 0.114105, acc 0.979375\n",
      "2019-05-17T02:01:13.675680: step 7500, loss 0.0653952, acc 0.985\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.73      0.25      0.37        32\n",
      "       B-geo       0.84      0.88      0.86      3531\n",
      "       B-gpe       0.95      0.94      0.94      1565\n",
      "       B-nat       0.33      0.06      0.10        17\n",
      "       B-org       0.76      0.59      0.67      2029\n",
      "       B-per       0.80      0.81      0.80      1728\n",
      "       B-tim       0.92      0.85      0.88      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.00      0.00      0.00        27\n",
      "       I-geo       0.73      0.75      0.74       685\n",
      "       I-gpe       1.00      0.33      0.50        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.72      0.58      0.64      1694\n",
      "       I-per       0.80      0.92      0.86      1754\n",
      "       I-tim       0.84      0.69      0.76       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.61      0.51      0.54    104466\n",
      "weighted avg       0.96      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:01:14.659428: step 7500, loss 0.132968, acc 0.9781\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 7/15 [04:29<05:08, 38.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:01:32.138262: step 8000, loss 0.0463517, acc 0.9825\n",
      "2019-05-17T02:01:49.608532: step 8500, loss 0.11526, acc 0.981875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 8/15 [05:07<04:28, 38.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:02:07.084306: step 9000, loss 0.0655326, acc 0.983125\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.64      0.28      0.39        32\n",
      "       B-geo       0.83      0.89      0.86      3531\n",
      "       B-gpe       0.95      0.93      0.94      1565\n",
      "       B-nat       0.25      0.06      0.10        17\n",
      "       B-org       0.77      0.59      0.67      2029\n",
      "       B-per       0.82      0.80      0.81      1728\n",
      "       B-tim       0.91      0.86      0.88      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.43      0.22      0.29        27\n",
      "       I-geo       0.73      0.78      0.76       685\n",
      "       I-gpe       1.00      0.38      0.55        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.72      0.61      0.66      1694\n",
      "       I-per       0.85      0.88      0.87      1754\n",
      "       I-tim       0.87      0.70      0.78       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.63      0.53      0.56    104466\n",
      "weighted avg       0.96      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:02:08.067651: step 9000, loss 0.127396, acc 0.9788\n",
      "\n",
      "2019-05-17T02:02:25.545789: step 9500, loss 0.158061, acc 0.974375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 9/15 [05:45<03:50, 38.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:02:43.048202: step 10000, loss 0.103522, acc 0.974375\n",
      "2019-05-17T02:03:00.531931: step 10500, loss 0.15755, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.73      0.25      0.37        32\n",
      "       B-geo       0.83      0.89      0.86      3531\n",
      "       B-gpe       0.96      0.93      0.95      1565\n",
      "       B-nat       0.33      0.24      0.28        17\n",
      "       B-org       0.75      0.63      0.69      2029\n",
      "       B-per       0.81      0.82      0.81      1728\n",
      "       B-tim       0.90      0.87      0.88      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.40      0.07      0.12        27\n",
      "       I-geo       0.76      0.75      0.76       685\n",
      "       I-gpe       1.00      0.38      0.55        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.72      0.63      0.67      1694\n",
      "       I-per       0.85      0.89      0.87      1754\n",
      "       I-tim       0.78      0.76      0.77       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.64      0.53      0.56    104466\n",
      "weighted avg       0.96      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:03:01.521037: step 10500, loss 0.124262, acc 0.9792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 10/15 [06:24<03:12, 38.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:03:19.002700: step 11000, loss 0.0577571, acc 0.985625\n",
      "2019-05-17T02:03:36.465624: step 11500, loss 0.0623397, acc 0.98625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 11/15 [07:02<02:33, 38.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:03:53.931706: step 12000, loss 0.124934, acc 0.975\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.67      0.25      0.36        32\n",
      "       B-geo       0.84      0.89      0.87      3531\n",
      "       B-gpe       0.95      0.94      0.94      1565\n",
      "       B-nat       0.33      0.06      0.10        17\n",
      "       B-org       0.79      0.60      0.68      2029\n",
      "       B-per       0.79      0.83      0.81      1728\n",
      "       B-tim       0.92      0.86      0.89      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.60      0.11      0.19        27\n",
      "       I-geo       0.78      0.74      0.76       685\n",
      "       I-gpe       1.00      0.38      0.55        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.78      0.56      0.66      1694\n",
      "       I-per       0.86      0.87      0.87      1754\n",
      "       I-tim       0.91      0.66      0.76       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.66      0.51      0.55    104466\n",
      "weighted avg       0.96      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:03:54.917984: step 12000, loss 0.122941, acc 0.9795\n",
      "\n",
      "2019-05-17T02:04:12.421807: step 12500, loss 0.132141, acc 0.97875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 12/15 [07:41<01:55, 38.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:04:29.880308: step 13000, loss 0.0531821, acc 0.981875\n",
      "2019-05-17T02:04:47.365187: step 13500, loss 0.0966943, acc 0.979375\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.73      0.25      0.37        32\n",
      "       B-geo       0.84      0.90      0.87      3531\n",
      "       B-gpe       0.94      0.94      0.94      1565\n",
      "       B-nat       0.62      0.29      0.40        17\n",
      "       B-org       0.80      0.58      0.68      2029\n",
      "       B-per       0.82      0.81      0.82      1728\n",
      "       B-tim       0.91      0.87      0.89      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.50      0.07      0.13        27\n",
      "       I-geo       0.77      0.75      0.76       685\n",
      "       I-gpe       1.00      0.33      0.50        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.81      0.55      0.65      1694\n",
      "       I-per       0.85      0.88      0.87      1754\n",
      "       I-tim       0.87      0.71      0.78       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.67      0.53      0.57    104466\n",
      "weighted avg       0.96      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:04:48.334003: step 13500, loss 0.12148, acc 0.9799\n",
      "\n",
      "2019-05-17T02:05:05.804326: step 14000, loss 0.123461, acc 0.976875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 13/15 [08:19<01:17, 38.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:05:23.289089: step 14500, loss 0.0771858, acc 0.97625\n",
      "2019-05-17T02:05:40.791205: step 15000, loss 0.0899221, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        33\n",
      "       B-eve       0.62      0.25      0.36        32\n",
      "       B-geo       0.85      0.89      0.87      3531\n",
      "       B-gpe       0.95      0.94      0.95      1565\n",
      "       B-nat       0.67      0.24      0.35        17\n",
      "       B-org       0.77      0.64      0.70      2029\n",
      "       B-per       0.84      0.80      0.82      1728\n",
      "       B-tim       0.93      0.86      0.90      1981\n",
      "       I-art       0.00      0.00      0.00        30\n",
      "       I-eve       0.45      0.19      0.26        27\n",
      "       I-geo       0.78      0.77      0.78       685\n",
      "       I-gpe       1.00      0.46      0.63        24\n",
      "       I-nat       0.00      0.00      0.00         5\n",
      "       I-org       0.76      0.62      0.68      1694\n",
      "       I-per       0.86      0.89      0.87      1754\n",
      "       I-tim       0.85      0.74      0.79       639\n",
      "           O       0.98      0.99      0.99     88692\n",
      "\n",
      "   micro avg       0.96      0.96      0.96    104466\n",
      "   macro avg       0.67      0.55      0.58    104466\n",
      "weighted avg       0.96      0.96      0.96    104466\n",
      "\n",
      "2019-05-17T02:05:41.766269: step 15000, loss 0.11662, acc 0.9805\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 14/15 [08:58<00:38, 38.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T02:05:59.291469: step 15500, loss 0.0990525, acc 0.984375\n",
      "2019-05-17T02:06:16.824446: step 16000, loss 0.0988977, acc 0.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [09:36<00:00, 38.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        sequence_length=x_train.shape[1],\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        embedding_size=Config.embedding_dim,\n",
    "        hidden_size=Config.hidden_size\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.AdadeltaOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"29.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    pretrain_W = load_glove(Config.embedding_dim, word2idx)\n",
    "    sess.run(model.W_text.assign(pretrain_W))\n",
    "    print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(list(zip(x_train, lengths_train, y_train)), Config.batch_size, Config.num_epochs)\n",
    "    \n",
    "    for batch in batches:\n",
    "        x_batch, lengths_train, y_batch = zip(*batch)\n",
    "        \n",
    "        # Train\n",
    "        feed_dict = {\n",
    "            model.input_x: x_batch,\n",
    "            model.input_y: y_batch,\n",
    "            model.sequence_lengths: lengths_train,\n",
    "            model.dropout: 0.5,\n",
    "        }\n",
    "        _, step, loss, accuracy = sess.run(\n",
    "            [train_op, global_step, model.loss, model.accuracy], feed_dict)\n",
    "        \n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            feed_dict = {\n",
    "                model.input_x: x_dev,\n",
    "                model.input_y: y_dev,\n",
    "                model.sequence_lengths: lengths_dev,\n",
    "                model.dropout: 1.0,\n",
    "            }\n",
    "            loss, accuracy, predictions = sess.run(\n",
    "                [model.loss, model.accuracy, model.predictions], feed_dict)\n",
    "            \n",
    "            evaluation(y_dev, predictions, lengths_dev)\n",
    "            \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:.4f}\\n\".format(time_str, step, loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./29.runs --host 0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
