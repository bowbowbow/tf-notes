{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Embeddings\n",
    "    word_embedding_dim = 128\n",
    "    char_embedding_dim = 128\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size_word = 128\n",
    "    hidden_size_char = 128\n",
    "    l2_reg_lambda = 1e-5\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 20\n",
    "    num_epochs = 10\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    \n",
    "    learning_rate = 1.0 \n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"-\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "nlp-challenge SRL dataset: https://github.com/naver/nlp-challenge/tree/master/missions/srl\n",
    "\n",
    "LeaderBoard and Tag description: http://air.changwon.ac.kr/?page_id=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARGM-LOC - - - - - ARG1 ARG1 - - - ARG1 - - -</td>\n",
       "      <td>인사동에 들어서면 다종다양의 창호지, 도자기 등 고미술품들이 진열장에 즐비하게 널려...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARG3 - ARG0 - - ARG1 - ARG1 - - ARG1 -</td>\n",
       "      <td>올림픽에 출진하는 선수라면 어떤 금전적 인유가 없더라도 최악을 다하겠다는 정신력, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARG0 - - - - - - - - - - -</td>\n",
       "      <td>젖먹이들은 비닐 찢어 우산살 일체 붙인 졸악한 것을 연이라 믿고 있을 터인데.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- ARG1 - ARG1 -</td>\n",
       "      <td>때로는 규범이 큰 무도화도 있다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- - - - ARG0 -</td>\n",
       "      <td>삶이란 끊임없는 취택의 불연속이라고 나는 요탁했다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>- - ARG3 - ARG1 - -</td>\n",
       "      <td>천정, 짜임새 마저에 빨간 뼁끼가 칠해져 있었다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>- ARG0 - ARG1 - ARGM-DIR - ARG1 - ARGM-TMP - A...</td>\n",
       "      <td>여기에다가 권뢰가 자신의 양식을 되찾아 현대문화에로 불연속시키는 사역을 시작하기도 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>- ARGM-CAU - - - ARGM-DIR - - ARG1 -</td>\n",
       "      <td>리드미컬한 반동으로 흔들리는 내 교제 위로 노갱이들의 날카로운 발톱들이 보였다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>- - - - - - ARG0 - - - - - - ARG1 - ARGM-TMP -...</td>\n",
       "      <td>마지못해 받아들이는 듯 하면서도 아하스 페르츠의 손길이 미치기만 하면 따뜻해져 감겨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>- - - - ARG1 - -</td>\n",
       "      <td>에너지원으로서 학사당하던 인품도 교질 요막은 얼마든지 있었다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0      ARGM-LOC - - - - - ARG1 ARG1 - - - ARG1 - - -   \n",
       "1             ARG3 - ARG0 - - ARG1 - ARG1 - - ARG1 -   \n",
       "2                         ARG0 - - - - - - - - - - -   \n",
       "3                                    - ARG1 - ARG1 -   \n",
       "4                                     - - - - ARG0 -   \n",
       "5                                - - ARG3 - ARG1 - -   \n",
       "6  - ARG0 - ARG1 - ARGM-DIR - ARG1 - ARGM-TMP - A...   \n",
       "7               - ARGM-CAU - - - ARGM-DIR - - ARG1 -   \n",
       "8  - - - - - - ARG0 - - - - - - ARG1 - ARGM-TMP -...   \n",
       "9                                   - - - - ARG1 - -   \n",
       "\n",
       "                                               words  \n",
       "0  인사동에 들어서면 다종다양의 창호지, 도자기 등 고미술품들이 진열장에 즐비하게 널려...  \n",
       "1  올림픽에 출진하는 선수라면 어떤 금전적 인유가 없더라도 최악을 다하겠다는 정신력, ...  \n",
       "2        젖먹이들은 비닐 찢어 우산살 일체 붙인 졸악한 것을 연이라 믿고 있을 터인데.  \n",
       "3                                 때로는 규범이 큰 무도화도 있다.  \n",
       "4                       삶이란 끊임없는 취택의 불연속이라고 나는 요탁했다.  \n",
       "5                        천정, 짜임새 마저에 빨간 뼁끼가 칠해져 있었다.  \n",
       "6  여기에다가 권뢰가 자신의 양식을 되찾아 현대문화에로 불연속시키는 사역을 시작하기도 ...  \n",
       "7       리드미컬한 반동으로 흔들리는 내 교제 위로 노갱이들의 날카로운 발톱들이 보였다.  \n",
       "8  마지못해 받아들이는 듯 하면서도 아하스 페르츠의 손길이 미치기만 하면 따뜻해져 감겨...  \n",
       "9                 에너지원으로서 학사당하던 인품도 교질 요막은 얼마든지 있었다.  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words, self.all_chars = [], [], []\n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        words_col, tags_col = [], []\n",
    "        with open(path) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 0:\n",
    "                    if len(words) != 0:\n",
    "                        words_col.append(' '.join(words))\n",
    "                        tags_col.append(' '.join(tags))\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split('\\t')\n",
    "                    word, tag = ls[1], ls[2]\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_tags.append(tag)\n",
    "                    self.all_chars.extend(list(word))\n",
    "                    \n",
    "                    \n",
    "        return pd.DataFrame({'words': words_col, 'tags': tags_col})\n",
    "        \n",
    "    def download_and_load_datasets(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"naver_challenge_srl_train_data.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/naver_challenge_srl_train_data.zip\", \n",
    "          extract=True)\n",
    "\n",
    "        train_df = self.load_dataset(os.path.join(os.path.dirname(dataset), 'naver_challenge_srl_train_data'))\n",
    "        return train_df\n",
    "\n",
    "dataset = Dataset()\n",
    "df = dataset.download_and_load_datasets()\n",
    "\n",
    "# shuffle \n",
    "shuffle_indices = np.random.permutation(np.arange(len(df)))\n",
    "shuffled_df= df.iloc[shuffle_indices]\n",
    "\n",
    "train_end = int(len(df) * 0.9)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "dev_df = df.iloc[train_end:]\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "char_list = list(set(dataset.all_chars)) + [Config.PAD, Config.UNK]\n",
    "char2idx = {w: i for i, w in enumerate(char_list)}\n",
    "idx2char = {i: w for i, w in enumerate(char_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               char_size,\n",
    "               word_embedding_dim, \n",
    "               char_embedding_dim,\n",
    "               hidden_size_word,\n",
    "               hidden_size_char):\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name='word_ids') \n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "        \n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name='char_ids') # [batch_size, max_sequence_length, max_word_length]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\") # [batch_size, max_sequence_length]\n",
    "        \n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('word-embedding'):\n",
    "            self._word_embeddings = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim], -0.25, 0.25), name='_word_embeddings')\n",
    "            self.word_embeddings = tf.nn.embedding_lookup(self._word_embeddings, self.word_ids) # [batch_size, max_sequence_length, word_embedding_dim]\n",
    "        \n",
    "        # Char Embedding Layer\n",
    "        with tf.variable_scope('char-embedding'):\n",
    "            self._char_embeddings = tf.get_variable(dtype=tf.float32, shape=[char_size, char_embedding_dim], name='_char_embeddings')\n",
    "            \n",
    "            # [batch_size, max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            self.char_embeddings = tf.nn.embedding_lookup(self._char_embeddings, self.char_ids) \n",
    "            \n",
    "            s = tf.shape(self.char_embeddings)\n",
    "            \n",
    "            # [batch_size*max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            char_embeddings = tf.reshape(self.char_embeddings, shape=[s[0]*s[1], s[2], char_embedding_dim])\n",
    "            word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "            \n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            \n",
    "            _, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, \n",
    "                                                                                   cell_bw=bw_cell, \n",
    "                                                                                   inputs=char_embeddings,\n",
    "                                                                                   sequence_length=word_lengths,\n",
    "                                                                                   dtype=tf.float32)\n",
    "            # shape: [batch_size*max_sequnce_length, 2*hidden_size_char]\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.reshape(output, shape=[s[0], s[1], 2*hidden_size_char])\n",
    "            \n",
    "            # shape: # [batch_size, max_sequence_length, word_embedding_dim + 2*hidden_size_char]\n",
    "            self.word_embeddings = tf.concat([self.word_embeddings, output], axis=-1) \n",
    "            # self.word_embeddings = tf.nn.dropout(self.word_embeddings, self.dropout)\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.word_embeddings,\n",
    "                                                                  sequence_length= self.sequence_lengths, # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, max_sequence_length, 2*hidden_size_word]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "        \n",
    "        \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size_word, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tf.shape(self.rnn_outputs)[1]\n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size_word])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, num_classes]) # [batch_size, max_sequence_length, num_classes]\n",
    "    \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params  # need to evaluate it for decoding\n",
    "            \n",
    "            self.l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "            self.loss = tf.reduce_mean(-log_likelihood) + Config.l2_reg_lambda * self.l2\n",
    "            \n",
    "#             When CRF is not in use\n",
    "#             self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "#             mask = tf.sequence_mask(self.sequence_lengths)\n",
    "#             losses = tf.boolean_mask(self.losses, mask)\n",
    "#             self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @staticmethod\n",
    "    def viterbi_decode(logits, trans_params):\n",
    "        # get tag scores and transition params of CRF\n",
    "        viterbi_sequences = []\n",
    "\n",
    "        # iterate over the sentences because no batching in vitervi_decode\n",
    "        for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "            logit = logit[:sequence_length]  # keep only the valid steps\n",
    "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                logit, trans_params)\n",
    "            viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "        return np.array(viterbi_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(word_embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), word_embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(df, batch_size, num_epochs, shuffle=True, tqdm_disable=False):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(df)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs), disable=tqdm_disable):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_df= df.iloc[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_df = df\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_df.iloc[start_index:end_index]    \n",
    "            \n",
    "\n",
    "def get_feed_dict(batch_df):\n",
    "    max_length = max(map(lambda x : len(x.split(' ')), batch_df['words'].tolist()))\n",
    "    \n",
    "    max_length_word = 0\n",
    "    for seq in batch_df['words'].tolist():\n",
    "        for word in seq.split(' '):\n",
    "            max_length_word = max(max_length_word, len(word))\n",
    "    \n",
    "    word_ids, sequence_lengths, labels, char_ids, word_lengths = [], [], [], [], []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        sentence = row['words'].split(' ')\n",
    "        tags = row['tags'].split(' ')\n",
    "\n",
    "        word_ids_row, labels_row, char_ids_row, word_lengths_row = [], [], [], []\n",
    "        for word in sentence:\n",
    "            word_ids_row.append(word2idx[word])\n",
    "        \n",
    "            char_ids_row.append([char2idx[char] for char in word] + [char2idx[Config.PAD]]* (max_length_word - len(word)) )\n",
    "            word_lengths_row.append(len(word))\n",
    "        \n",
    "        empty_char_ids = [char2idx[Config.PAD]]* max_length_word\n",
    "        char_ids_row += [empty_char_ids] * (max_length - len(char_ids_row))\n",
    "        word_lengths_row += [0] * (max_length - len(word_lengths_row))\n",
    "        \n",
    "        for tag in tags:\n",
    "            labels_row.append(tag2idx[tag])\n",
    "\n",
    "        if len(sentence) < max_length:\n",
    "            word_ids_row += [word2idx[Config.PAD]]* (max_length - len(sentence))\n",
    "            labels_row += [tag2idx[Config.NONE]]* (max_length - len(sentence))\n",
    "\n",
    "        word_ids.append(word_ids_row)\n",
    "        labels.append(labels_row)\n",
    "        sequence_lengths.append(len(sentence))\n",
    "        char_ids.append(char_ids_row)\n",
    "        word_lengths.append(word_lengths_row)\n",
    "    \n",
    "    word_ids = np.array(word_ids)\n",
    "    labels = np.array(labels)\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    char_ids = np.array(char_ids)\n",
    "    word_lengths = np.array(word_lengths)\n",
    "    \n",
    "    return word_ids, labels, sequence_lengths, char_ids, word_lengths\n",
    "\n",
    "def evaluation(y, preds, lengths):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answers, arg_preds = [], []\n",
    "    \n",
    "    accs = []\n",
    "    correct_preds, total_correct, total_preds = 0.0, 0.0, 0.0\n",
    "    for i in range(len(y)):\n",
    "        sent_answers,sent_preds = [], []\n",
    "        sent_answer_chunks, sent_pred_chunks = [], []\n",
    "        \n",
    "        for j in range(lengths[i]):\n",
    "            sent_answers.append(idx2tag[y[i][j]])\n",
    "            sent_preds.append(idx2tag[preds[i][j]])\n",
    "            \n",
    "            if idx2tag[y[i][j]] != Config.NONE:\n",
    "                sent_answer_chunks.append(idx2tag[y[i][j]] + '-' + str(j))\n",
    "            if idx2tag[preds[i][j]] != Config.NONE:\n",
    "                sent_pred_chunks.append(idx2tag[preds[i][j]] + '-' + str(j))\n",
    "    \n",
    "        arg_answers.extend(sent_answers)\n",
    "        arg_preds.extend(sent_preds)\n",
    "        \n",
    "        accs += [a == b for (a, b) in zip(sent_answers, sent_preds)]\n",
    "        \n",
    "        sent_answer_chunks = set(sent_answer_chunks)\n",
    "        sent_pred_chunks = set(sent_pred_chunks)\n",
    "\n",
    "        correct_preds += len(sent_answer_chunks & sent_pred_chunks)\n",
    "        total_preds += len(sent_pred_chunks)\n",
    "        total_correct += len(sent_answer_chunks)\n",
    "    \n",
    "    p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "    acc = np.mean(accs)\n",
    "        \n",
    "    print(classification_report(arg_answers, arg_preds))\n",
    "    \n",
    "    print('Tag based evaluation: acc: {}, f1: {}'.format(acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-b00eadf1e61d>:41: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-b00eadf1e61d>:48: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-b00eadf1e61d>:68: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/34.runs/1559133644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:41:34.711661: step 500, loss 6.77612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.93      0.95       136\n",
      "        ARG0       1.00      0.60      0.75        20\n",
      "        ARG1       0.67      0.94      0.78        32\n",
      "        ARG2       0.00      0.00      0.00         0\n",
      "        ARG3       0.33      0.50      0.40         4\n",
      "    ARGM-CAU       0.00      0.00      0.00         0\n",
      "    ARGM-EXT       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.33      0.14      0.20         7\n",
      "    ARGM-MNR       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       201\n",
      "   macro avg       0.37      0.35      0.34       201\n",
      "weighted avg       0.88      0.85      0.85       201\n",
      "\n",
      "Tag based evaluation: acc: 0.8507462686567164, f1: 0.661764705882353\n",
      "Train Evaluation 2019-05-29T21:42:23.023374: step 1000, loss 6.24329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.96      0.97      0.96       150\n",
      "        ARG0       0.78      0.70      0.74        10\n",
      "        ARG1       0.82      0.91      0.86        35\n",
      "        ARG2       0.00      0.00      0.00         2\n",
      "        ARG3       0.43      0.60      0.50         5\n",
      "    ARGM-INS       0.00      0.00      0.00         3\n",
      "    ARGM-LOC       0.60      0.75      0.67         4\n",
      "    ARGM-MNR       0.50      0.33      0.40         6\n",
      "    ARGM-TMP       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       216\n",
      "   macro avg       0.57      0.58      0.57       216\n",
      "weighted avg       0.88      0.89      0.88       216\n",
      "\n",
      "Tag based evaluation: acc: 0.8935185185185185, f1: 0.732824427480916\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:42:28.607661: loss 0.354366\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.96      0.94     29362\n",
      "        ARG0       0.73      0.66      0.69      1807\n",
      "        ARG1       0.81      0.81      0.81      6967\n",
      "        ARG2       0.65      0.18      0.28       474\n",
      "        ARG3       0.43      0.49      0.46      1151\n",
      "    ARGM-CAU       0.52      0.16      0.24       183\n",
      "    ARGM-DIR       0.49      0.13      0.21       136\n",
      "    ARGM-EXT       0.78      0.42      0.55       292\n",
      "    ARGM-INS       0.00      0.00      0.00       133\n",
      "    ARGM-LOC       0.38      0.46      0.42       618\n",
      "    ARGM-MNR       0.33      0.55      0.41       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.42      0.18      0.25       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.50      0.38      0.40     41882\n",
      "weighted avg       0.86      0.87      0.86     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8650016713623991, f1: 0.6741600790513833\n",
      "Train Evaluation 2019-05-29T21:43:17.719675: step 1500, loss 6.56451\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.95      0.94       142\n",
      "        ARG0       0.64      0.69      0.67        13\n",
      "        ARG1       0.91      0.78      0.84        37\n",
      "        ARG2       0.50      0.50      0.50         2\n",
      "        ARG3       0.33      0.20      0.25         5\n",
      "    ARGM-CAU       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       0.33      1.00      0.50         1\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.57      1.00      0.73         4\n",
      "    ARGM-MNR       0.33      0.33      0.33         3\n",
      "    ARGM-TMP       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       210\n",
      "   macro avg       0.50      0.59      0.52       210\n",
      "weighted avg       0.86      0.87      0.86       210\n",
      "\n",
      "Tag based evaluation: acc: 0.8666666666666667, f1: 0.7067669172932332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [02:39<23:51, 159.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:44:06.015551: step 2000, loss 6.06713\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.96      0.94      0.95       195\n",
      "        ARG0       0.86      0.60      0.71        10\n",
      "        ARG1       0.81      0.91      0.86        46\n",
      "        ARG2       0.50      0.50      0.50         2\n",
      "        ARG3       0.50      1.00      0.67         5\n",
      "    ARGM-CAU       0.00      0.00      0.00         1\n",
      "    ARGM-DIR       0.50      0.50      0.50         2\n",
      "    ARGM-EXT       1.00      1.00      1.00         1\n",
      "    ARGM-LOC       0.00      0.00      0.00         3\n",
      "    ARGM-MNR       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       266\n",
      "   macro avg       0.51      0.55      0.52       266\n",
      "weighted avg       0.90      0.90      0.90       266\n",
      "\n",
      "Tag based evaluation: acc: 0.8984962406015038, f1: 0.7619047619047618\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:44:12.406191: loss 0.32701\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.95      0.95     29362\n",
      "        ARG0       0.72      0.73      0.72      1807\n",
      "        ARG1       0.82      0.83      0.83      6967\n",
      "        ARG2       0.52      0.47      0.49       474\n",
      "        ARG3       0.47      0.59      0.52      1151\n",
      "    ARGM-CAU       0.51      0.15      0.23       183\n",
      "    ARGM-DIR       0.56      0.33      0.42       136\n",
      "    ARGM-EXT       0.80      0.49      0.61       292\n",
      "    ARGM-INS       0.39      0.05      0.09       133\n",
      "    ARGM-LOC       0.50      0.38      0.43       618\n",
      "    ARGM-MNR       0.40      0.45      0.42       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.56      0.25      0.34       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.55      0.44      0.47     41882\n",
      "weighted avg       0.87      0.88      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8772503700873884, f1: 0.7069756117355842\n",
      "Train Evaluation 2019-05-29T21:45:01.242676: step 2500, loss 5.4912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.96      0.98      0.97       146\n",
      "        ARG0       1.00      0.90      0.95        10\n",
      "        ARG1       0.87      0.87      0.87        38\n",
      "        ARG2       0.67      0.67      0.67         3\n",
      "        ARG3       0.20      0.50      0.29         2\n",
      "    ARGM-CAU       0.00      0.00      0.00         1\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.67      0.40      0.50         5\n",
      "    ARGM-MNR       0.00      0.00      0.00         2\n",
      "    ARGM-TMP       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       209\n",
      "   macro avg       0.44      0.43      0.42       209\n",
      "weighted avg       0.90      0.91      0.90       209\n",
      "\n",
      "Tag based evaluation: acc: 0.9090909090909091, f1: 0.7642276422764228\n",
      "Train Evaluation 2019-05-29T21:45:49.860840: step 3000, loss 4.97827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.96      0.95       144\n",
      "        ARG0       0.82      0.82      0.82        11\n",
      "        ARG1       0.86      0.88      0.87        34\n",
      "        ARG2       0.00      0.00      0.00         1\n",
      "        ARG3       1.00      1.00      1.00         2\n",
      "    ARGM-CAU       0.00      0.00      0.00         1\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      0.75      0.86         4\n",
      "    ARGM-LOC       0.67      1.00      0.80         2\n",
      "    ARGM-MNR       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       202\n",
      "   macro avg       0.73      0.69      0.70       202\n",
      "weighted avg       0.91      0.92      0.92       202\n",
      "\n",
      "Tag based evaluation: acc: 0.9207920792079208, f1: 0.8421052631578947\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:45:55.660575: loss 0.314576\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.96      0.95     29362\n",
      "        ARG0       0.74      0.74      0.74      1807\n",
      "        ARG1       0.82      0.83      0.83      6967\n",
      "        ARG2       0.63      0.39      0.48       474\n",
      "        ARG3       0.52      0.48      0.50      1151\n",
      "    ARGM-CAU       0.56      0.08      0.13       183\n",
      "    ARGM-DIR       0.48      0.61      0.54       136\n",
      "    ARGM-EXT       0.74      0.57      0.64       292\n",
      "    ARGM-INS       0.60      0.02      0.04       133\n",
      "    ARGM-LOC       0.49      0.51      0.50       618\n",
      "    ARGM-MNR       0.49      0.36      0.41       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.59      0.30      0.39       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.58      0.45      0.47     41882\n",
      "weighted avg       0.87      0.88      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8809512439711571, f1: 0.7174155217229855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [05:24<21:26, 160.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:46:44.656757: step 3500, loss 7.41537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.96      0.94      0.95       184\n",
      "        ARG0       0.71      0.83      0.77        12\n",
      "        ARG1       0.71      0.78      0.74        37\n",
      "        ARG2       1.00      0.25      0.40         4\n",
      "        ARG3       0.73      0.67      0.70        12\n",
      "    ARGM-CAU       0.00      0.00      0.00         2\n",
      "    ARGM-DIR       0.00      0.00      0.00         0\n",
      "    ARGM-EXT       1.00      1.00      1.00         1\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.40      0.50      0.44         4\n",
      "    ARGM-MNR       0.00      0.00      0.00         0\n",
      "    ARGM-TMP       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       260\n",
      "   macro avg       0.54      0.44      0.46       260\n",
      "weighted avg       0.88      0.87      0.87       260\n",
      "\n",
      "Tag based evaluation: acc: 0.8653846153846154, f1: 0.6709677419354839\n",
      "Train Evaluation 2019-05-29T21:47:32.927946: step 4000, loss 6.123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.96      0.96       180\n",
      "        ARG0       0.78      0.50      0.61        14\n",
      "        ARG1       0.69      0.79      0.74        39\n",
      "        ARG2       0.33      0.50      0.40         2\n",
      "        ARG3       0.80      0.44      0.57         9\n",
      "    ARGM-EXT       1.00      0.50      0.67         2\n",
      "    ARGM-LOC       0.50      0.50      0.50         6\n",
      "    ARGM-MNR       0.50      1.00      0.67         1\n",
      "    ARGM-TMP       0.67      0.67      0.67         3\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       256\n",
      "   macro avg       0.69      0.65      0.64       256\n",
      "weighted avg       0.88      0.87      0.87       256\n",
      "\n",
      "Tag based evaluation: acc: 0.87109375, f1: 0.6666666666666667\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:47:38.641809: loss 0.308896\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.96      0.95     29362\n",
      "        ARG0       0.71      0.80      0.75      1807\n",
      "        ARG1       0.83      0.84      0.83      6967\n",
      "        ARG2       0.61      0.44      0.51       474\n",
      "        ARG3       0.50      0.53      0.51      1151\n",
      "    ARGM-CAU       0.55      0.13      0.20       183\n",
      "    ARGM-DIR       0.48      0.59      0.53       136\n",
      "    ARGM-EXT       0.73      0.60      0.66       292\n",
      "    ARGM-INS       0.44      0.14      0.22       133\n",
      "    ARGM-LOC       0.55      0.44      0.49       618\n",
      "    ARGM-MNR       0.53      0.35      0.42       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.60      0.29      0.39       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.57      0.47      0.50     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8848192540948379, f1: 0.7250538990359191\n",
      "Train Evaluation 2019-05-29T21:48:27.538251: step 4500, loss 5.06772\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.97      0.96       138\n",
      "        ARG0       1.00      0.78      0.88         9\n",
      "        ARG1       0.88      0.88      0.88        40\n",
      "        ARG2       0.20      0.50      0.29         2\n",
      "        ARG3       0.57      0.67      0.62         6\n",
      "    ARGM-CAU       0.00      0.00      0.00         2\n",
      "    ARGM-EXT       1.00      1.00      1.00         3\n",
      "    ARGM-INS       0.00      0.00      0.00         3\n",
      "    ARGM-LOC       0.00      0.00      0.00         1\n",
      "    ARGM-MNR       0.60      0.75      0.67         4\n",
      "    ARGM-PRP       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       209\n",
      "   macro avg       0.47      0.50      0.48       209\n",
      "weighted avg       0.88      0.89      0.89       209\n",
      "\n",
      "Tag based evaluation: acc: 0.8947368421052632, f1: 0.762589928057554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [08:02<18:40, 160.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:49:15.950695: step 5000, loss 6.25285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.97      0.95       154\n",
      "        ARG0       0.89      0.67      0.76        12\n",
      "        ARG1       0.82      0.79      0.81        34\n",
      "        ARG2       0.33      0.33      0.33         3\n",
      "        ARG3       0.36      0.80      0.50         5\n",
      "    ARGM-CAU       0.50      0.33      0.40         3\n",
      "    ARGM-EXT       1.00      0.50      0.67         2\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.75      0.75      0.75         4\n",
      "    ARGM-MNR       1.00      0.50      0.67         6\n",
      "    ARGM-TMP       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       227\n",
      "   macro avg       0.69      0.54      0.58       227\n",
      "weighted avg       0.89      0.87      0.87       227\n",
      "\n",
      "Tag based evaluation: acc: 0.8722466960352423, f1: 0.6950354609929077\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:49:21.535809: loss 0.305213\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.96      0.95     29362\n",
      "        ARG0       0.73      0.80      0.76      1807\n",
      "        ARG1       0.84      0.83      0.83      6967\n",
      "        ARG2       0.57      0.50      0.53       474\n",
      "        ARG3       0.50      0.53      0.51      1151\n",
      "    ARGM-CAU       0.53      0.21      0.30       183\n",
      "    ARGM-DIR       0.56      0.49      0.52       136\n",
      "    ARGM-EXT       0.71      0.59      0.65       292\n",
      "    ARGM-INS       0.39      0.12      0.18       133\n",
      "    ARGM-LOC       0.57      0.42      0.48       618\n",
      "    ARGM-MNR       0.42      0.51      0.46       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.57      0.31      0.40       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.56      0.48      0.51     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8840552027123824, f1: 0.7249043393307825\n",
      "Train Evaluation 2019-05-29T21:50:10.933946: step 5500, loss 5.10475\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.99      0.98       143\n",
      "        ARG0       0.78      0.88      0.82         8\n",
      "        ARG1       0.83      0.91      0.87        33\n",
      "        ARG2       0.67      0.67      0.67         3\n",
      "        ARG3       0.67      0.50      0.57         8\n",
      "    ARGM-INS       0.00      0.00      0.00         2\n",
      "    ARGM-LOC       0.33      1.00      0.50         1\n",
      "    ARGM-MNR       0.00      0.00      0.00         4\n",
      "    ARGM-TMP       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       204\n",
      "   macro avg       0.58      0.60      0.56       204\n",
      "weighted avg       0.89      0.91      0.90       204\n",
      "\n",
      "Tag based evaluation: acc: 0.9117647058823529, f1: 0.7563025210084033\n",
      "Train Evaluation 2019-05-29T21:51:00.815352: step 6000, loss 5.40371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.94      0.95       163\n",
      "        ARG0       0.73      0.89      0.80         9\n",
      "        ARG1       0.91      0.84      0.88        38\n",
      "        ARG2       0.33      0.50      0.40         2\n",
      "        ARG3       0.17      0.33      0.22         3\n",
      "    ARGM-DIR       1.00      0.67      0.80         3\n",
      "    ARGM-EXT       0.50      0.50      0.50         2\n",
      "    ARGM-LOC       0.67      0.50      0.57         4\n",
      "    ARGM-MNR       0.50      1.00      0.67         1\n",
      "    ARGM-TMP       1.00      0.75      0.86         4\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       229\n",
      "   macro avg       0.68      0.69      0.66       229\n",
      "weighted avg       0.91      0.90      0.90       229\n",
      "\n",
      "Tag based evaluation: acc: 0.8951965065502183, f1: 0.7669172932330827\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:51:06.469412: loss 0.305035\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.96      0.95     29362\n",
      "        ARG0       0.79      0.73      0.76      1807\n",
      "        ARG1       0.83      0.84      0.84      6967\n",
      "        ARG2       0.70      0.38      0.49       474\n",
      "        ARG3       0.54      0.49      0.51      1151\n",
      "    ARGM-CAU       0.71      0.07      0.12       183\n",
      "    ARGM-DIR       0.63      0.49      0.55       136\n",
      "    ARGM-EXT       0.79      0.53      0.63       292\n",
      "    ARGM-INS       0.43      0.12      0.19       133\n",
      "    ARGM-LOC       0.54      0.53      0.53       618\n",
      "    ARGM-MNR       0.45      0.47      0.46       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.59      0.36      0.45       335\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     41882\n",
      "   macro avg       0.61      0.46      0.50     41882\n",
      "weighted avg       0.88      0.89      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8852012797860657, f1: 0.7293522854419625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [10:48<16:10, 161.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:51:55.033517: step 6500, loss 4.9999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.99      0.97       162\n",
      "        ARG0       1.00      1.00      1.00        11\n",
      "        ARG1       0.91      0.79      0.85        39\n",
      "        ARG2       1.00      0.75      0.86         4\n",
      "        ARG3       0.43      0.43      0.43         7\n",
      "    ARGM-DIR       1.00      0.50      0.67         2\n",
      "    ARGM-EXT       1.00      0.33      0.50         3\n",
      "    ARGM-INS       1.00      0.50      0.67         2\n",
      "    ARGM-LOC       0.40      1.00      0.57         2\n",
      "    ARGM-MNR       1.00      1.00      1.00         3\n",
      "    ARGM-TMP       1.00      1.00      1.00         2\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       237\n",
      "   macro avg       0.88      0.75      0.77       237\n",
      "weighted avg       0.93      0.92      0.92       237\n",
      "\n",
      "Tag based evaluation: acc: 0.9240506329113924, f1: 0.8111888111888111\n",
      "Train Evaluation 2019-05-29T21:52:43.590238: step 7000, loss 6.3261\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.94      0.94       204\n",
      "        ARG0       0.75      0.60      0.67        10\n",
      "        ARG1       0.80      0.88      0.84        42\n",
      "        ARG2       0.50      0.50      0.50         2\n",
      "        ARG3       0.67      1.00      0.80         4\n",
      "    ARGM-CAU       0.00      0.00      0.00         4\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.67      1.00      0.80         2\n",
      "    ARGM-MNR       0.00      0.00      0.00         3\n",
      "    ARGM-TMP       0.33      0.50      0.40         2\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       275\n",
      "   macro avg       0.51      0.58      0.54       275\n",
      "weighted avg       0.87      0.89      0.88       275\n",
      "\n",
      "Tag based evaluation: acc: 0.8872727272727273, f1: 0.7375886524822696\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:52:48.955467: loss 0.308524\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.93      0.96      0.95     29362\n",
      "        ARG0       0.76      0.77      0.77      1807\n",
      "        ARG1       0.85      0.81      0.83      6967\n",
      "        ARG2       0.62      0.43      0.50       474\n",
      "        ARG3       0.51      0.53      0.52      1151\n",
      "    ARGM-CAU       0.52      0.17      0.26       183\n",
      "    ARGM-DIR       0.64      0.45      0.53       136\n",
      "    ARGM-EXT       0.69      0.62      0.65       292\n",
      "    ARGM-INS       0.36      0.18      0.24       133\n",
      "    ARGM-LOC       0.59      0.44      0.51       618\n",
      "    ARGM-MNR       0.50      0.41      0.45       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.61      0.30      0.40       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.58      0.47      0.51     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8830523852729096, f1: 0.7240470312565379\n",
      "Train Evaluation 2019-05-29T21:53:37.742052: step 7500, loss 5.39253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.91      0.99      0.95       136\n",
      "        ARG0       0.73      0.67      0.70        12\n",
      "        ARG1       0.85      0.83      0.84        35\n",
      "        ARG2       1.00      0.50      0.67         6\n",
      "        ARG3       0.50      0.75      0.60         4\n",
      "    ARGM-DIR       0.00      0.00      0.00         2\n",
      "    ARGM-EXT       1.00      1.00      1.00         2\n",
      "    ARGM-LOC       1.00      0.25      0.40         4\n",
      "    ARGM-MNR       1.00      0.33      0.50         3\n",
      "    ARGM-TMP       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       206\n",
      "   macro avg       0.70      0.53      0.57       206\n",
      "weighted avg       0.87      0.88      0.87       206\n",
      "\n",
      "Tag based evaluation: acc: 0.883495145631068, f1: 0.7343749999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [13:25<13:23, 160.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:54:26.339114: step 8000, loss 4.46467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.96      0.96       191\n",
      "        ARG0       1.00      0.67      0.80        12\n",
      "        ARG1       0.85      0.92      0.88        36\n",
      "        ARG2       0.00      0.00      0.00         2\n",
      "        ARG3       1.00      1.00      1.00         4\n",
      "    ARGM-CAU       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      1.00      1.00         1\n",
      "    ARGM-INS       1.00      1.00      1.00         1\n",
      "    ARGM-LOC       0.67      1.00      0.80         4\n",
      "    ARGM-MNR       0.75      0.60      0.67         5\n",
      "    ARGM-TMP       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       258\n",
      "   macro avg       0.84      0.83      0.83       258\n",
      "weighted avg       0.93      0.93      0.93       258\n",
      "\n",
      "Tag based evaluation: acc: 0.9302325581395349, f1: 0.8484848484848485\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:54:31.967875: loss 0.315304\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.96      0.95     29362\n",
      "        ARG0       0.75      0.77      0.76      1807\n",
      "        ARG1       0.83      0.82      0.83      6967\n",
      "        ARG2       0.57      0.50      0.53       474\n",
      "        ARG3       0.52      0.52      0.52      1151\n",
      "    ARGM-CAU       0.46      0.25      0.33       183\n",
      "    ARGM-DIR       0.57      0.44      0.50       136\n",
      "    ARGM-EXT       0.76      0.58      0.65       292\n",
      "    ARGM-INS       0.34      0.17      0.22       133\n",
      "    ARGM-LOC       0.58      0.46      0.51       618\n",
      "    ARGM-MNR       0.42      0.49      0.45       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.59      0.37      0.46       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.56      0.49      0.52     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8827897426101905, f1: 0.7236250715044535\n",
      "Train Evaluation 2019-05-29T21:55:21.296309: step 8500, loss 4.19959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.97      0.98       174\n",
      "        ARG0       0.92      0.86      0.89        14\n",
      "        ARG1       0.93      1.00      0.96        39\n",
      "        ARG2       0.67      0.67      0.67         3\n",
      "        ARG3       0.86      0.67      0.75         9\n",
      "    ARGM-CAU       1.00      0.50      0.67         2\n",
      "    ARGM-EXT       1.00      0.50      0.67         2\n",
      "    ARGM-LOC       0.33      1.00      0.50         2\n",
      "    ARGM-MNR       0.67      1.00      0.80         2\n",
      "    ARGM-TMP       1.00      1.00      1.00         3\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       250\n",
      "   macro avg       0.84      0.82      0.79       250\n",
      "weighted avg       0.96      0.95      0.95       250\n",
      "\n",
      "Tag based evaluation: acc: 0.948, f1: 0.8774193548387097\n",
      "Train Evaluation 2019-05-29T21:56:11.279939: step 9000, loss 5.10656\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.96      0.99      0.98       189\n",
      "        ARG0       0.82      0.82      0.82        11\n",
      "        ARG1       1.00      0.91      0.95        53\n",
      "        ARG2       0.80      0.50      0.62         8\n",
      "        ARG3       0.50      0.67      0.57         6\n",
      "    ARGM-DIR       0.00      0.00      0.00         1\n",
      "    ARGM-EXT       0.75      1.00      0.86         3\n",
      "    ARGM-INS       0.00      0.00      0.00         2\n",
      "    ARGM-LOC       0.25      0.25      0.25         4\n",
      "    ARGM-MNR       0.50      1.00      0.67         1\n",
      "    ARGM-TMP       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       278\n",
      "   macro avg       0.51      0.56      0.52       278\n",
      "weighted avg       0.93      0.93      0.92       278\n",
      "\n",
      "Tag based evaluation: acc: 0.9280575539568345, f1: 0.813953488372093\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:56:16.771195: loss 0.314248\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.95      0.95     29362\n",
      "        ARG0       0.74      0.78      0.76      1807\n",
      "        ARG1       0.83      0.84      0.83      6967\n",
      "        ARG2       0.59      0.51      0.55       474\n",
      "        ARG3       0.52      0.56      0.53      1151\n",
      "    ARGM-CAU       0.54      0.21      0.31       183\n",
      "    ARGM-DIR       0.49      0.64      0.56       136\n",
      "    ARGM-EXT       0.68      0.62      0.65       292\n",
      "    ARGM-INS       0.40      0.22      0.28       133\n",
      "    ARGM-LOC       0.55      0.52      0.54       618\n",
      "    ARGM-MNR       0.46      0.45      0.45       383\n",
      "    ARGM-PRP       0.50      0.05      0.09        41\n",
      "    ARGM-TMP       0.56      0.36      0.44       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.60      0.52      0.53     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8843894751922067, f1: 0.7288189987163028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [16:11<10:48, 162.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:57:05.437703: step 9500, loss 4.04698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.96      0.98      0.97       176\n",
      "        ARG0       0.86      1.00      0.92         6\n",
      "        ARG1       0.91      0.96      0.93        45\n",
      "        ARG2       0.33      1.00      0.50         1\n",
      "        ARG3       0.40      0.40      0.40         5\n",
      "    ARGM-CAU       1.00      0.33      0.50         3\n",
      "    ARGM-DIR       0.00      0.00      0.00         1\n",
      "    ARGM-EXT       1.00      1.00      1.00         4\n",
      "    ARGM-LOC       0.50      0.25      0.33         4\n",
      "    ARGM-MNR       1.00      1.00      1.00         3\n",
      "    ARGM-TMP       1.00      0.40      0.57         5\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       253\n",
      "   macro avg       0.72      0.67      0.65       253\n",
      "weighted avg       0.93      0.93      0.92       253\n",
      "\n",
      "Tag based evaluation: acc: 0.9288537549407114, f1: 0.8344370860927152\n",
      "Train Evaluation 2019-05-29T21:57:53.954489: step 10000, loss 3.56903\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.97      0.97       163\n",
      "        ARG0       0.89      1.00      0.94         8\n",
      "        ARG1       0.94      0.94      0.94        36\n",
      "        ARG2       0.67      0.50      0.57         4\n",
      "        ARG3       1.00      1.00      1.00         3\n",
      "    ARGM-EXT       0.67      0.67      0.67         3\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       1.00      1.00      1.00         4\n",
      "    ARGM-MNR       1.00      1.00      1.00         3\n",
      "    ARGM-TMP       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       226\n",
      "   macro avg       0.71      0.71      0.71       226\n",
      "weighted avg       0.95      0.95      0.95       226\n",
      "\n",
      "Tag based evaluation: acc: 0.9469026548672567, f1: 0.8888888888888888\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:57:59.783788: loss 0.335946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.95      0.95     29362\n",
      "        ARG0       0.74      0.75      0.75      1807\n",
      "        ARG1       0.82      0.82      0.82      6967\n",
      "        ARG2       0.53      0.42      0.47       474\n",
      "        ARG3       0.51      0.54      0.53      1151\n",
      "    ARGM-CAU       0.47      0.30      0.36       183\n",
      "    ARGM-DIR       0.64      0.46      0.54       136\n",
      "    ARGM-EXT       0.66      0.63      0.65       292\n",
      "    ARGM-INS       0.31      0.20      0.25       133\n",
      "    ARGM-LOC       0.53      0.52      0.53       618\n",
      "    ARGM-MNR       0.39      0.56      0.46       383\n",
      "    ARGM-PRP       0.29      0.05      0.08        41\n",
      "    ARGM-TMP       0.49      0.40      0.44       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.56      0.51      0.52     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8786590898237906, f1: 0.7150018045474595\n",
      "Train Evaluation 2019-05-29T21:58:48.294192: step 10500, loss 3.72383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.98      0.98      0.98       223\n",
      "        ARG0       1.00      0.86      0.92        14\n",
      "        ARG1       0.96      0.94      0.95        52\n",
      "        ARG2       0.40      1.00      0.57         2\n",
      "        ARG3       0.50      0.60      0.55         5\n",
      "    ARGM-CAU       0.00      0.00      0.00         2\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      1.00      1.00         1\n",
      "    ARGM-LOC       0.83      0.62      0.71         8\n",
      "    ARGM-MNR       0.33      1.00      0.50         1\n",
      "    ARGM-TMP       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       309\n",
      "   macro avg       0.64      0.73      0.65       309\n",
      "weighted avg       0.96      0.94      0.95       309\n",
      "\n",
      "Tag based evaluation: acc: 0.9449838187702265, f1: 0.8554913294797688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [18:50<08:03, 161.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T21:59:37.055736: step 11000, loss 3.06738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      1.00      1.00       121\n",
      "        ARG0       1.00      1.00      1.00        11\n",
      "        ARG1       0.92      0.97      0.94        34\n",
      "        ARG2       1.00      0.67      0.80         3\n",
      "        ARG3       0.88      0.64      0.74        11\n",
      "    ARGM-DIR       1.00      1.00      1.00         2\n",
      "    ARGM-LOC       0.50      1.00      0.67         2\n",
      "    ARGM-MNR       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       185\n",
      "   macro avg       0.79      0.78      0.77       185\n",
      "weighted avg       0.96      0.96      0.96       185\n",
      "\n",
      "Tag based evaluation: acc: 0.9621621621621622, f1: 0.8976377952755906\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T21:59:42.837212: loss 0.34159\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.95      0.95     29362\n",
      "        ARG0       0.75      0.75      0.75      1807\n",
      "        ARG1       0.82      0.82      0.82      6967\n",
      "        ARG2       0.62      0.46      0.53       474\n",
      "        ARG3       0.49      0.60      0.54      1151\n",
      "    ARGM-CAU       0.42      0.23      0.30       183\n",
      "    ARGM-DIR       0.53      0.56      0.54       136\n",
      "    ARGM-EXT       0.71      0.60      0.65       292\n",
      "    ARGM-INS       0.35      0.26      0.30       133\n",
      "    ARGM-LOC       0.56      0.50      0.53       618\n",
      "    ARGM-MNR       0.52      0.37      0.43       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.49      0.40      0.44       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.55      0.50      0.52     41882\n",
      "weighted avg       0.88      0.88      0.88     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8800916861658946, f1: 0.7176704062208904\n",
      "Train Evaluation 2019-05-29T22:00:31.580865: step 11500, loss 3.49192\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       1.00      0.96      0.98       216\n",
      "        ARG0       0.86      1.00      0.92        12\n",
      "        ARG1       0.93      1.00      0.96        40\n",
      "        ARG2       1.00      0.50      0.67         4\n",
      "        ARG3       0.75      1.00      0.86         6\n",
      "    ARGM-CAU       1.00      1.00      1.00         2\n",
      "    ARGM-DIR       0.00      0.00      0.00         0\n",
      "    ARGM-EXT       1.00      1.00      1.00         3\n",
      "    ARGM-INS       1.00      1.00      1.00         1\n",
      "    ARGM-LOC       1.00      1.00      1.00         3\n",
      "    ARGM-MNR       0.00      0.00      0.00         0\n",
      "    ARGM-TMP       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       289\n",
      "   macro avg       0.79      0.75      0.75       289\n",
      "weighted avg       0.98      0.96      0.97       289\n",
      "\n",
      "Tag based evaluation: acc: 0.9619377162629758, f1: 0.9150326797385621\n",
      "Train Evaluation 2019-05-29T22:01:19.949699: step 12000, loss 3.24371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      1.00      0.99       155\n",
      "        ARG0       1.00      0.93      0.96        14\n",
      "        ARG1       1.00      0.97      0.99        37\n",
      "        ARG2       0.67      1.00      0.80         2\n",
      "        ARG3       0.80      0.89      0.84         9\n",
      "    ARGM-DIR       1.00      1.00      1.00         2\n",
      "    ARGM-EXT       0.67      1.00      0.80         2\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       1.00      0.80      0.89         5\n",
      "    ARGM-MNR       0.50      0.33      0.40         3\n",
      "    ARGM-TMP       1.00      0.33      0.50         3\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       233\n",
      "   macro avg       0.78      0.75      0.74       233\n",
      "weighted avg       0.96      0.96      0.96       233\n",
      "\n",
      "Tag based evaluation: acc: 0.9613733905579399, f1: 0.9078947368421053\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T22:01:25.863322: loss 0.367891\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.94      0.94     29362\n",
      "        ARG0       0.71      0.76      0.73      1807\n",
      "        ARG1       0.81      0.81      0.81      6967\n",
      "        ARG2       0.54      0.49      0.51       474\n",
      "        ARG3       0.51      0.48      0.50      1151\n",
      "    ARGM-CAU       0.38      0.28      0.32       183\n",
      "    ARGM-DIR       0.51      0.63      0.56       136\n",
      "    ARGM-EXT       0.63      0.61      0.62       292\n",
      "    ARGM-INS       0.28      0.23      0.25       133\n",
      "    ARGM-LOC       0.53      0.50      0.52       618\n",
      "    ARGM-MNR       0.43      0.45      0.44       383\n",
      "    ARGM-PRP       0.18      0.05      0.08        41\n",
      "    ARGM-TMP       0.47      0.44      0.46       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.53      0.51      0.52     41882\n",
      "weighted avg       0.87      0.87      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8725705553698486, f1: 0.7046291846868493\n",
      "Train Evaluation 2019-05-29T22:02:14.669265: step 12500, loss 4.74079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.97      0.97       209\n",
      "        ARG0       0.88      0.78      0.82         9\n",
      "        ARG1       0.88      0.94      0.91        52\n",
      "        ARG2       0.75      1.00      0.86         3\n",
      "        ARG3       0.77      0.67      0.71        15\n",
      "    ARGM-EXT       0.50      1.00      0.67         1\n",
      "    ARGM-INS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.50      0.50      0.50         2\n",
      "    ARGM-MNR       1.00      0.33      0.50         3\n",
      "    ARGM-TMP       1.00      0.50      0.67         2\n",
      "\n",
      "   micro avg       0.93      0.93      0.93       297\n",
      "   macro avg       0.72      0.67      0.66       297\n",
      "weighted avg       0.93      0.93      0.93       297\n",
      "\n",
      "Tag based evaluation: acc: 0.9292929292929293, f1: 0.8342857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [21:34<05:24, 162.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T22:03:03.468058: step 13000, loss 3.06052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       147\n",
      "        ARG0       0.93      0.93      0.93        15\n",
      "        ARG1       0.97      0.95      0.96        40\n",
      "        ARG2       0.67      0.67      0.67         3\n",
      "        ARG3       0.00      0.00      0.00         0\n",
      "    ARGM-CAU       1.00      0.50      0.67         4\n",
      "    ARGM-DIR       1.00      1.00      1.00         2\n",
      "    ARGM-EXT       0.00      0.00      0.00         0\n",
      "    ARGM-INS       1.00      1.00      1.00         1\n",
      "    ARGM-LOC       1.00      1.00      1.00         1\n",
      "    ARGM-MNR       0.67      1.00      0.80         2\n",
      "    ARGM-TMP       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       217\n",
      "   macro avg       0.73      0.71      0.71       217\n",
      "weighted avg       0.97      0.96      0.96       217\n",
      "\n",
      "Tag based evaluation: acc: 0.9585253456221198, f1: 0.8936170212765958\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T22:03:09.073736: loss 0.395497\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.95      0.94     29362\n",
      "        ARG0       0.72      0.73      0.73      1807\n",
      "        ARG1       0.81      0.81      0.81      6967\n",
      "        ARG2       0.54      0.42      0.47       474\n",
      "        ARG3       0.50      0.51      0.50      1151\n",
      "    ARGM-CAU       0.38      0.28      0.32       183\n",
      "    ARGM-DIR       0.58      0.49      0.53       136\n",
      "    ARGM-EXT       0.63      0.62      0.63       292\n",
      "    ARGM-INS       0.29      0.22      0.25       133\n",
      "    ARGM-LOC       0.53      0.53      0.53       618\n",
      "    ARGM-MNR       0.42      0.46      0.44       383\n",
      "    ARGM-PRP       0.00      0.00      0.00        41\n",
      "    ARGM-TMP       0.49      0.41      0.45       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.52      0.50      0.51     41882\n",
      "weighted avg       0.87      0.87      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8730958406952868, f1: 0.7039151057158936\n",
      "Train Evaluation 2019-05-29T22:03:57.816498: step 13500, loss 3.15718\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.98      0.99      0.98       165\n",
      "        ARG0       1.00      1.00      1.00         9\n",
      "        ARG1       0.97      0.94      0.96        36\n",
      "        ARG2       1.00      0.33      0.50         3\n",
      "        ARG3       0.67      0.67      0.67         6\n",
      "    ARGM-CAU       0.33      1.00      0.50         1\n",
      "    ARGM-EXT       1.00      1.00      1.00         3\n",
      "    ARGM-LOC       0.50      1.00      0.67         2\n",
      "    ARGM-MNR       1.00      0.50      0.67         2\n",
      "    ARGM-TMP       0.75      0.60      0.67         5\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       232\n",
      "   macro avg       0.82      0.80      0.76       232\n",
      "weighted avg       0.96      0.95      0.95       232\n",
      "\n",
      "Tag based evaluation: acc: 0.9525862068965517, f1: 0.8721804511278195\n",
      "Train Evaluation 2019-05-29T22:04:46.285719: step 14000, loss 2.93281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.98      0.99       178\n",
      "        ARG0       1.00      1.00      1.00        12\n",
      "        ARG1       0.98      0.93      0.96        46\n",
      "        ARG2       0.50      1.00      0.67         1\n",
      "        ARG3       1.00      0.89      0.94         9\n",
      "    ARGM-DIR       1.00      0.75      0.86         4\n",
      "    ARGM-EXT       1.00      1.00      1.00         3\n",
      "    ARGM-LOC       1.00      0.80      0.89         5\n",
      "    ARGM-MNR       0.00      0.00      0.00         0\n",
      "    ARGM-TMP       0.60      1.00      0.75         3\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       261\n",
      "   macro avg       0.81      0.84      0.80       261\n",
      "weighted avg       0.98      0.97      0.97       261\n",
      "\n",
      "Tag based evaluation: acc: 0.9655172413793104, f1: 0.9221556886227544\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T22:04:52.269218: loss 0.383443\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.94      0.95      0.95     29362\n",
      "        ARG0       0.71      0.75      0.73      1807\n",
      "        ARG1       0.82      0.82      0.82      6967\n",
      "        ARG2       0.55      0.42      0.48       474\n",
      "        ARG3       0.48      0.57      0.52      1151\n",
      "    ARGM-CAU       0.46      0.21      0.29       183\n",
      "    ARGM-DIR       0.60      0.50      0.54       136\n",
      "    ARGM-EXT       0.68      0.65      0.67       292\n",
      "    ARGM-INS       0.32      0.20      0.25       133\n",
      "    ARGM-LOC       0.52      0.54      0.53       618\n",
      "    ARGM-MNR       0.49      0.37      0.42       383\n",
      "    ARGM-PRP       0.33      0.02      0.05        41\n",
      "    ARGM-TMP       0.51      0.36      0.42       335\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     41882\n",
      "   macro avg       0.57      0.49      0.51     41882\n",
      "weighted avg       0.87      0.88      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8759849099851965, f1: 0.7098359995164605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [24:18<02:42, 162.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-05-29T22:05:41.117579: step 14500, loss 3.22836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      0.99      0.99       183\n",
      "        ARG0       0.82      1.00      0.90         9\n",
      "        ARG1       0.92      1.00      0.96        46\n",
      "        ARG2       1.00      0.50      0.67         2\n",
      "        ARG3       1.00      0.62      0.77         8\n",
      "    ARGM-CAU       1.00      0.50      0.67         2\n",
      "    ARGM-EXT       1.00      1.00      1.00         1\n",
      "    ARGM-INS       1.00      1.00      1.00         1\n",
      "    ARGM-LOC       0.67      0.40      0.50         5\n",
      "    ARGM-MNR       0.80      0.80      0.80         5\n",
      "    ARGM-TMP       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       262\n",
      "   macro avg       0.84      0.71      0.75       262\n",
      "weighted avg       0.97      0.96      0.96       262\n",
      "\n",
      "Tag based evaluation: acc: 0.9618320610687023, f1: 0.8860759493670886\n",
      "Train Evaluation 2019-05-29T22:06:30.472922: step 15000, loss 2.68801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.97      0.99      0.98       156\n",
      "        ARG0       1.00      1.00      1.00         7\n",
      "        ARG1       0.93      0.95      0.94        41\n",
      "        ARG2       1.00      1.00      1.00         1\n",
      "        ARG3       1.00      0.86      0.92         7\n",
      "    ARGM-CAU       1.00      0.67      0.80         3\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      0.50      0.67         2\n",
      "    ARGM-INS       1.00      1.00      1.00         2\n",
      "    ARGM-LOC       0.86      1.00      0.92         6\n",
      "    ARGM-MNR       0.80      1.00      0.89         4\n",
      "    ARGM-PRP       0.00      0.00      0.00         1\n",
      "    ARGM-TMP       1.00      0.80      0.89         5\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       236\n",
      "   macro avg       0.89      0.83      0.85       236\n",
      "weighted avg       0.96      0.96      0.96       236\n",
      "\n",
      "Tag based evaluation: acc: 0.961864406779661, f1: 0.9240506329113924\n",
      "\n",
      "Dev Evaluation\n",
      "2019-05-29T22:06:36.192518: loss 0.427588\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.95      0.94      0.95     29362\n",
      "        ARG0       0.69      0.76      0.72      1807\n",
      "        ARG1       0.81      0.82      0.82      6967\n",
      "        ARG2       0.48      0.51      0.49       474\n",
      "        ARG3       0.49      0.56      0.52      1151\n",
      "    ARGM-CAU       0.41      0.26      0.32       183\n",
      "    ARGM-DIR       0.55      0.51      0.53       136\n",
      "    ARGM-EXT       0.63      0.66      0.64       292\n",
      "    ARGM-INS       0.26      0.23      0.25       133\n",
      "    ARGM-LOC       0.58      0.44      0.50       618\n",
      "    ARGM-MNR       0.44      0.48      0.46       383\n",
      "    ARGM-PRP       0.17      0.02      0.04        41\n",
      "    ARGM-TMP       0.46      0.41      0.44       335\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     41882\n",
      "   macro avg       0.53      0.51      0.51     41882\n",
      "weighted avg       0.87      0.87      0.87     41882\n",
      "\n",
      "Tag based evaluation: acc: 0.8737643856549353, f1: 0.7063576945929887\n",
      "Train Evaluation 2019-05-29T22:07:25.063784: step 15500, loss 2.71087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           -       0.99      1.00      0.99       152\n",
      "        ARG0       1.00      0.92      0.96        12\n",
      "        ARG1       0.94      1.00      0.97        34\n",
      "        ARG2       0.25      1.00      0.40         1\n",
      "        ARG3       1.00      0.89      0.94         9\n",
      "    ARGM-DIR       1.00      1.00      1.00         1\n",
      "    ARGM-EXT       1.00      1.00      1.00         2\n",
      "    ARGM-LOC       1.00      0.20      0.33         5\n",
      "    ARGM-MNR       0.67      0.67      0.67         3\n",
      "    ARGM-PRP       0.00      0.00      0.00         1\n",
      "    ARGM-TMP       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       221\n",
      "   macro avg       0.80      0.79      0.75       221\n",
      "weighted avg       0.97      0.96      0.96       221\n",
      "\n",
      "Tag based evaluation: acc: 0.9638009049773756, f1: 0.8970588235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [26:58<00:00, 161.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        char_size=len(char_list),\n",
    "        word_embedding_dim=Config.word_embedding_dim,\n",
    "        char_embedding_dim=Config.char_embedding_dim,\n",
    "        hidden_size_word=Config.hidden_size_word,\n",
    "        hidden_size_char=Config.hidden_size_char\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # train_op = tf.train.AdamOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(Config.learning_rate, Config.decay_rate, 1e-6)\n",
    "    gvs = optimizer.compute_gradients(model.loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"34.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     pretrain_W = load_glove(Config.word_embedding_dim, word2idx)\n",
    "#     sess.run(model._word_embeddings.assign(pretrain_W))\n",
    "#     print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(train_df, Config.batch_size, Config.num_epochs)\n",
    "    for batch_df in batches:\n",
    "        word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "        feed_dict = {\n",
    "            model.word_ids: word_ids,\n",
    "            model.labels: labels,\n",
    "            model.sequence_lengths: sequence_lengths,\n",
    "            model.char_ids: char_ids,\n",
    "            model.word_lengths: word_lengths,\n",
    "            model.dropout: 0.5,\n",
    "        }\n",
    "        _, step, loss, logits, trans_params = sess.run([\n",
    "            train_op, global_step, model.loss, model.logits, model.trans_params], feed_dict)\n",
    "        \n",
    "        predictions = model.viterbi_decode(logits, trans_params)\n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"Train Evaluation {}: step {}, loss {:g}\".format(time_str, step, loss))\n",
    "#             evaluation(labels, predictions, sequence_lengths)\n",
    "            \n",
    "            \n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            batches = batch_iter(dev_df, Config.batch_size, 1, tqdm_disable=True)\n",
    "            \n",
    "            total_loss, predictions_all, labels_all, sequence_lengths_all  = 0, [], [], []\n",
    "            for batch_df in batches:\n",
    "                word_ids, labels, sequence_lengths, char_ids, word_lengths = get_feed_dict(batch_df)\n",
    "                feed_dict = {\n",
    "                    model.word_ids: word_ids,\n",
    "                    model.labels: labels,\n",
    "                    model.sequence_lengths: sequence_lengths,\n",
    "                    model.char_ids: char_ids,\n",
    "                    model.word_lengths: word_lengths,\n",
    "                    model.dropout: 1.0,\n",
    "                }\n",
    "                loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "                predictions = model.viterbi_decode(logits, trans_params)\n",
    "                \n",
    "                total_loss += loss\n",
    "                predictions_all += predictions.tolist()\n",
    "                labels_all += labels.tolist()\n",
    "                sequence_lengths_all += sequence_lengths.tolist()\n",
    "        \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"\\nDev Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "            evaluation(labels_all, predictions_all, sequence_lengths_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
