{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data loading params\n",
    "    max_sentence_length = 40\n",
    "    dev_sample_percentage = 0.1\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_dim = 300\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size = 300\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 40\n",
    "    num_epochs = 20\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    learning_rate = 0.001\n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"O\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Load annotated corpus for named entity recognition\n",
    "\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-ORG O I-MISC O O O I-MISC O O</td>\n",
       "      <td>eu rejects german call to boycott british lamb .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-PER I-PER</td>\n",
       "      <td>peter blackburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-LOC O</td>\n",
       "      <td>brussels 1996-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O I-ORG I-ORG O O O O O O I-MISC O O O O O I-M...</td>\n",
       "      <td>the european commission said on thursday it di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-LOC O O O O I-ORG I-ORG O O O I-PER I-PER O ...</td>\n",
       "      <td>germany 's representative to the european unio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O I-ORG ...</td>\n",
       "      <td>\" we do n't support any such recommendation be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O I-...</td>\n",
       "      <td>he said further scientific study was required ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O O O O O O O I-ORG O O I-PER I-PER O O O O O ...</td>\n",
       "      <td>he said a proposal last month by eu farm commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-PER O I-MISC O O O O I-LOC O I-LOC O O O O O...</td>\n",
       "      <td>fischler proposed eu-wide measures after repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O I-PER O O O O O O O I-ORG O O O O O O O O O ...</td>\n",
       "      <td>but fischler agreed to review his proposal aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0                    I-ORG O I-MISC O O O I-MISC O O   \n",
       "1                                        I-PER I-PER   \n",
       "2                                            I-LOC O   \n",
       "3  O I-ORG I-ORG O O O O O O I-MISC O O O O O I-M...   \n",
       "4  I-LOC O O O O I-ORG I-ORG O O O I-PER I-PER O ...   \n",
       "5  O O O O O O O O O O O O O O O O O O O O I-ORG ...   \n",
       "6  O O O O O O O O O O O O O O O O O O O O O O I-...   \n",
       "7  O O O O O O O I-ORG O O I-PER I-PER O O O O O ...   \n",
       "8  I-PER O I-MISC O O O O I-LOC O I-LOC O O O O O...   \n",
       "9  O I-PER O O O O O O O I-ORG O O O O O O O O O ...   \n",
       "\n",
       "                                               words  \n",
       "0   eu rejects german call to boycott british lamb .  \n",
       "1                                    peter blackburn  \n",
       "2                                brussels 1996-08-22  \n",
       "3  the european commission said on thursday it di...  \n",
       "4  germany 's representative to the european unio...  \n",
       "5  \" we do n't support any such recommendation be...  \n",
       "6  he said further scientific study was required ...  \n",
       "7  he said a proposal last month by eu farm commi...  \n",
       "8  fischler proposed eu-wide measures after repor...  \n",
       "9  but fischler agreed to review his proposal aft...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, path):\n",
    "        words_col, tags_col = [], []\n",
    "        with open(path) as f:\n",
    "            words, tags = [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if (len(line) == 0 or line.startswith(\"-DOCSTART-\")):\n",
    "                    if len(words) != 0:\n",
    "                        words_col.append(' '.join(words))\n",
    "                        tags_col.append(' '.join(tags))\n",
    "                        words, tags = [], []\n",
    "                else:\n",
    "                    ls = line.split(' ')\n",
    "                    word, tag = ls[0],ls[3]\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    words.append(word)\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_tags.append(tag)\n",
    "                    \n",
    "        return pd.DataFrame({'words': words_col, 'tags': tags_col})\n",
    "        \n",
    "    def download_and_load_datasets(self):\n",
    "        self.all_tags, self.all_words = [], [] \n",
    "        \n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"CoNLL-2003.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/CoNLL-2003.zip\", \n",
    "          extract=True)\n",
    "        \n",
    "        dir_path = os.path.join(os.path.dirname(dataset), 'CoNLL-2003')\n",
    "        train_df = self.load_dataset(os.path.join(dir_path, 'eng.train'))\n",
    "        dev_df = self.load_dataset(os.path.join(dir_path, 'eng.testa'))\n",
    "        test_df = self.load_dataset(os.path.join(dir_path, 'eng.testb'))\n",
    "        return train_df, dev_df, test_df\n",
    "\n",
    "dataset = Dataset()\n",
    "train_df, dev_df, test_df = dataset.download_and_load_datasets()\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags))\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "\n",
    "def get_data(df):\n",
    "    x, lengths, y = [], [], []\n",
    "    for index, row in train_df.iterrows():\n",
    "        sentence = row['words'].split(' ')\n",
    "        tags = row['tags'].split(' ')\n",
    "\n",
    "        sentence = sentence[:Config.max_sentence_length]\n",
    "        tags = tags[:Config.max_sentence_length]\n",
    "\n",
    "        lengths.append(Config.max_sentence_length)\n",
    "        x_row, y_row = [], []\n",
    "        for word in sentence:\n",
    "            x_row.append(word2idx[word])\n",
    "        for tag in tags:\n",
    "            y_row.append(tag2idx[tag])\n",
    "\n",
    "        if len(sentence) < Config.max_sentence_length:\n",
    "            lengths[-1] = len(sentence)\n",
    "            x_row += [word2idx[Config.PAD]]* (Config.max_sentence_length - len(sentence))\n",
    "            y_row += [tag2idx[Config.NONE]]* (Config.max_sentence_length - len(sentence))\n",
    "        \n",
    "        x.append(x_row)\n",
    "        y.append(y_row)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    lengths = np.array(lengths)\n",
    "    return x, y, lengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, lengths_train = get_data(train_df)\n",
    "x_dev, y_dev, lengths_dev = get_data(dev_df)\n",
    "x_test, y_test, lengths_test = get_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               sequence_length, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               embedding_size, \n",
    "               hidden_size):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_y')\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.variable_scope('text-embedding'):\n",
    "            self.W_text = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -0.25, 0.25), name='W_text', trainable=False)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_x) # [batch_size, sequence_length, embedding_size]\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.embedded_chars,\n",
    "                                                                  sequence_length= self.sequence_lengths, # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, sequence_length, 2*hidden_size]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "    \n",
    "    \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, sequence_length, num_classes]) # [batch_size, sequence_length, num_classes]\n",
    "    \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            mask = tf.sequence_mask(self.sequence_lengths)\n",
    "            losses = tf.boolean_mask(self.losses, mask)\n",
    "            \n",
    "            self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "        # Accuracy    \n",
    "        with tf.name_scope('accuracy'):\n",
    "            self.predictions = tf.argmax(self.logits, 2, name='predictions')\n",
    "            correct_predictions = tf.equal(self.predictions, tf.cast(self.input_y, tf.int64))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')\n",
    "    \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "def evaluation(y, preds, lengths):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answer, arg_pred = [], []\n",
    "    for i in range(len(y)):\n",
    "        for j in range(lengths[i]):\n",
    "            arg_answer.append(idx2tag[y[i][j]])\n",
    "            arg_pred.append(idx2tag[preds[i][j]])\n",
    "\n",
    "    print(classification_report(arg_answer, arg_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-6-fe38c6f48bc2>:23: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-fe38c6f48bc2>:29: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-fe38c6f48bc2>:32: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/30.runs/1558023759\n",
      "\n",
      "embedding_path : /home/seungwon/.keras/datasets/glove.6B.300d.txt\n",
      "Load glove file /home/seungwon/.keras/datasets/glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to load pre-trained glove model!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:11<03:45, 11.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:23:36.239300: step 500, loss 0.179014, acc 0.985625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:23<03:31, 11.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:23:52.606882: step 1000, loss 0.0815813, acc 0.9925\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.60      0.27      0.37        11\n",
      "      B-MISC       0.00      0.00      0.00        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       0.96      0.91      0.93      8225\n",
      "      I-MISC       0.92      0.79      0.85      4530\n",
      "       I-ORG       0.92      0.82      0.87      9926\n",
      "       I-PER       0.98      0.96      0.97     11031\n",
      "           O       0.98      1.00      0.99    167606\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    201390\n",
      "   macro avg       0.80      0.72      0.75    201390\n",
      "weighted avg       0.98      0.98      0.98    201390\n",
      "\n",
      "2019-05-17T01:23:54.969905: step 1000, loss 0.0750772, acc 0.9919\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:48<03:14, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:24:11.401229: step 1500, loss 0.751189, acc 0.99375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [01:00<02:59, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:24:27.892104: step 2000, loss 0.0867381, acc 0.985\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      0.64      0.78        11\n",
      "      B-MISC       1.00      0.14      0.24        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       0.97      0.97      0.97      8225\n",
      "      I-MISC       0.98      0.87      0.92      4530\n",
      "       I-ORG       0.96      0.92      0.94      9926\n",
      "       I-PER       0.98      0.99      0.99     11031\n",
      "           O       0.99      1.00      1.00    167606\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    201390\n",
      "   macro avg       0.99      0.82      0.85    201390\n",
      "weighted avg       0.99      0.99      0.99    201390\n",
      "\n",
      "2019-05-17T01:24:30.179855: step 2000, loss 0.0352761, acc 0.9962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:25<02:39, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:24:46.623662: step 2500, loss 0.360474, acc 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [01:37<02:24, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:25:03.129294: step 3000, loss 0.0126071, acc 0.99875\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      0.82      0.90        11\n",
      "      B-MISC       0.94      0.43      0.59        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       0.99      0.99      0.99      8225\n",
      "      I-MISC       0.98      0.97      0.97      4530\n",
      "       I-ORG       0.99      0.97      0.98      9926\n",
      "       I-PER       1.00      0.99      1.00     11031\n",
      "           O       1.00      1.00      1.00    167606\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    201390\n",
      "   macro avg       0.99      0.90      0.93    201390\n",
      "weighted avg       1.00      1.00      1.00    201390\n",
      "\n",
      "2019-05-17T01:25:05.373110: step 3000, loss 0.0146638, acc 0.9986\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [01:51<02:18, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:25:21.863307: step 3500, loss 0.0230563, acc 0.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:14<01:48, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:25:38.321058: step 4000, loss 0.182173, acc 0.99875\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      0.82      0.90        11\n",
      "      B-MISC       1.00      0.57      0.72        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       0.99      0.99      0.99      8225\n",
      "      I-MISC       0.99      0.99      0.99      4530\n",
      "       I-ORG       0.99      0.97      0.98      9926\n",
      "       I-PER       1.00      1.00      1.00     11031\n",
      "           O       1.00      1.00      1.00    167606\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    201390\n",
      "   macro avg       1.00      0.92      0.95    201390\n",
      "weighted avg       1.00      1.00      1.00    201390\n",
      "\n",
      "2019-05-17T01:25:40.596633: step 4000, loss 0.00980013, acc 0.9991\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [02:28<01:40, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:25:57.027431: step 4500, loss 0.136358, acc 0.998125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:51<01:12, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:26:13.517254: step 5000, loss 0.00424521, acc 0.999375\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      0.91      0.95        11\n",
      "      B-MISC       1.00      0.86      0.93        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       1.00      1.00      1.00      8225\n",
      "      I-MISC       1.00      0.99      0.99      4530\n",
      "       I-ORG       0.99      0.99      0.99      9926\n",
      "       I-PER       1.00      1.00      1.00     11031\n",
      "           O       1.00      1.00      1.00    167606\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    201390\n",
      "   macro avg       1.00      0.97      0.98    201390\n",
      "weighted avg       1.00      1.00      1.00    201390\n",
      "\n",
      "2019-05-17T01:26:15.786567: step 5000, loss 0.00464551, acc 0.9996\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [03:05<01:03, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:26:32.218181: step 5500, loss 0.00488206, acc 0.99875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [03:28<00:36, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:26:48.681353: step 6000, loss 0.0114078, acc 0.99875\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      1.00      1.00        11\n",
      "      B-MISC       0.97      0.89      0.93        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       1.00      1.00      1.00      8225\n",
      "      I-MISC       1.00      1.00      1.00      4530\n",
      "       I-ORG       1.00      0.99      1.00      9926\n",
      "       I-PER       1.00      1.00      1.00     11031\n",
      "           O       1.00      1.00      1.00    167606\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    201390\n",
      "   macro avg       1.00      0.98      0.99    201390\n",
      "weighted avg       1.00      1.00      1.00    201390\n",
      "\n",
      "2019-05-17T01:26:50.971277: step 6000, loss 0.00300757, acc 0.9997\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [03:42<00:25, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:27:07.420553: step 6500, loss 0.00349693, acc 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [03:54<00:12, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-17T01:27:23.876156: step 7000, loss 0.00711772, acc 0.999375\n",
      "\n",
      "Dev Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      1.00      1.00        11\n",
      "      B-MISC       0.97      0.95      0.96        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       1.00      1.00      1.00      8225\n",
      "      I-MISC       1.00      1.00      1.00      4530\n",
      "       I-ORG       1.00      0.99      1.00      9926\n",
      "       I-PER       1.00      1.00      1.00     11031\n",
      "           O       1.00      1.00      1.00    167606\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    201390\n",
      "   macro avg       1.00      0.99      0.99    201390\n",
      "weighted avg       1.00      1.00      1.00    201390\n",
      "\n",
      "2019-05-17T01:27:26.168493: step 7000, loss 0.00233567, acc 0.9998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:07<00:00, 12.78s/it]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        sequence_length=x_train.shape[1],\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        embedding_size=Config.embedding_dim,\n",
    "        hidden_size=Config.hidden_size\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = tf.train.AdamOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"30.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    pretrain_W = load_glove(Config.embedding_dim, word2idx)\n",
    "    sess.run(model.W_text.assign(pretrain_W))\n",
    "    print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(list(zip(x_train, lengths_train, y_train)), Config.batch_size, Config.num_epochs)\n",
    "    \n",
    "    for batch in batches:\n",
    "        x_batch, lengths_batch, y_batch = zip(*batch)\n",
    "\n",
    "        # Train\n",
    "        feed_dict = {\n",
    "            model.input_x: x_batch,\n",
    "            model.input_y: y_batch,\n",
    "            model.sequence_lengths: lengths_batch,\n",
    "            model.dropout: 0.5,\n",
    "        }\n",
    "\n",
    "        _, step, loss, accuracy, predictions = sess.run(\n",
    "            [train_op, global_step, model.loss, model.accuracy, model.predictions], feed_dict)\n",
    "\n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            print(\"\\nDev Evaluation:\")\n",
    "            feed_dict = {\n",
    "                model.input_x: x_dev,\n",
    "                model.input_y: y_dev,\n",
    "                model.sequence_lengths: lengths_dev,\n",
    "                model.dropout: 1.0,\n",
    "            }\n",
    "            loss, accuracy, predictions = sess.run(\n",
    "                [model.loss, model.accuracy, model.predictions], feed_dict)\n",
    "            evaluation(y_dev, predictions, lengths_dev)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:.4f}\\n\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       1.00      1.00      1.00        11\n",
      "      B-MISC       1.00      0.92      0.96        37\n",
      "       B-ORG       1.00      1.00      1.00        24\n",
      "       I-LOC       1.00      1.00      1.00      8225\n",
      "      I-MISC       1.00      1.00      1.00      4530\n",
      "       I-ORG       1.00      0.99      1.00      9926\n",
      "       I-PER       1.00      1.00      1.00     11031\n",
      "           O       1.00      1.00      1.00    167606\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    201390\n",
      "   macro avg       1.00      0.99      0.99    201390\n",
      "weighted avg       1.00      1.00      1.00    201390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Evaluation:\")\n",
    "predictions = sess.run([model.predictions], feed_dict={\n",
    "    model.input_x: x_test,\n",
    "    model.input_y: y_test,\n",
    "    model.sequence_lengths: lengths_test,\n",
    "    model.dropout: 1.0,\n",
    "})\n",
    "evaluation(y_test, predictions[0], lengths_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./30.runs --host 0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
