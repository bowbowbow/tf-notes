{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://github.com/golbin/TensorFlow-Tutorials \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S: 디코딩 입력의 시작을 나타내는 심볼\n",
    "- E: 디코딩 출력의 끝을 나타내는 심볼\n",
    "- P: 현재 배치 데이터의 time step 크기보다 작은 경우 빈 시퀀스를 채우는 심볼\n",
    "\n",
    "```\n",
    "예) 현재 배치 데이터의 최대 크기가 4인 경우\n",
    "word -> ['w', 'o', 'r', 'd']\n",
    "to   -> ['t', 'o', 'P', 'P']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "# 영어를 한글로 번역하기 위한 학습 데이터\n",
    "seq_data = [['word', '단어'], ['wood', '나무'],\n",
    "            ['game', '놀이'], ['girl', '소녀'],\n",
    "            ['kiss', '키스'], ['love', '사랑']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        # 인코더 셀의 입력값\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        # 디코더 셀의 입력값. 시작을 타나내는 S 심볼을 맨 앞에 붙여준다.\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E를 붙여준다.\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        \n",
    "        # sparse_softmax_cross_entropy_with_logits 사용할거라 one-hot 인코딩이 아님\n",
    "        target_batch.append(target)\n",
    "        \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "# 입력과 출력의 형태가 one-hot 인코딩으로 같기 때문에\n",
    "n_class = n_input = dic_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-461c2c532ce3>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-461c2c532ce3>:13: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "outputs : Tensor(\"encode/rnn/transpose_1:0\", shape=(?, ?, 128), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-5-461c2c532ce3>:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# 모델 구성\n",
    "\n",
    "# [batch_size, time_steps, input_size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "# 인코더 셀 구성\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n",
    "    # outputs: [batch_size, step_size, n_hidden]\n",
    "    # enc_states: [batch_size, n_hidden(=128)]\n",
    "    \n",
    "# 디코더 셀 구성\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    \n",
    "    # seq2seq에서는 인코더 셀의 최종 상태값을 디코더 셀의 초기값으로 넣어줘야함\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "model = tf.layers.dense(outputs, n_class, activation=None)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0001 cost =  3.765495\n",
      "Epoch : 0002 cost =  2.848657\n",
      "Epoch : 0003 cost =  1.686130\n",
      "Epoch : 0004 cost =  1.003551\n",
      "Epoch : 0005 cost =  0.609220\n",
      "Epoch : 0006 cost =  0.342718\n",
      "Epoch : 0007 cost =  0.290152\n",
      "Epoch : 0008 cost =  0.255709\n",
      "Epoch : 0009 cost =  0.265073\n",
      "Epoch : 0010 cost =  0.122246\n",
      "Epoch : 0011 cost =  0.083720\n",
      "Epoch : 0012 cost =  0.153749\n",
      "Epoch : 0013 cost =  0.204028\n",
      "Epoch : 0014 cost =  0.116245\n",
      "Epoch : 0015 cost =  0.160800\n",
      "Epoch : 0016 cost =  0.123206\n",
      "Epoch : 0017 cost =  0.084600\n",
      "Epoch : 0018 cost =  0.050823\n",
      "Epoch : 0019 cost =  0.091640\n",
      "Epoch : 0020 cost =  0.048518\n",
      "Epoch : 0021 cost =  0.059746\n",
      "Epoch : 0022 cost =  0.174631\n",
      "Epoch : 0023 cost =  0.013302\n",
      "Epoch : 0024 cost =  0.024974\n",
      "Epoch : 0025 cost =  0.237662\n",
      "Epoch : 0026 cost =  0.020074\n",
      "Epoch : 0027 cost =  0.260382\n",
      "Epoch : 0028 cost =  0.047037\n",
      "Epoch : 0029 cost =  0.008343\n",
      "Epoch : 0030 cost =  0.010418\n",
      "Epoch : 0031 cost =  0.059241\n",
      "Epoch : 0032 cost =  0.114934\n",
      "Epoch : 0033 cost =  0.023340\n",
      "Epoch : 0034 cost =  0.014599\n",
      "Epoch : 0035 cost =  0.012197\n",
      "Epoch : 0036 cost =  0.012751\n",
      "Epoch : 0037 cost =  0.031096\n",
      "Epoch : 0038 cost =  0.002458\n",
      "Epoch : 0039 cost =  0.012747\n",
      "Epoch : 0040 cost =  0.034324\n",
      "Epoch : 0041 cost =  0.013661\n",
      "Epoch : 0042 cost =  0.027187\n",
      "Epoch : 0043 cost =  0.007271\n",
      "Epoch : 0044 cost =  0.019755\n",
      "Epoch : 0045 cost =  0.019483\n",
      "Epoch : 0046 cost =  0.011864\n",
      "Epoch : 0047 cost =  0.002713\n",
      "Epoch : 0048 cost =  0.003790\n",
      "Epoch : 0049 cost =  0.002360\n",
      "Epoch : 0050 cost =  0.002515\n",
      "Epoch : 0051 cost =  0.001945\n",
      "Epoch : 0052 cost =  0.003364\n",
      "Epoch : 0053 cost =  0.002536\n",
      "Epoch : 0054 cost =  0.003441\n",
      "Epoch : 0055 cost =  0.002175\n",
      "Epoch : 0056 cost =  0.001420\n",
      "Epoch : 0057 cost =  0.001451\n",
      "Epoch : 0058 cost =  0.002940\n",
      "Epoch : 0059 cost =  0.001860\n",
      "Epoch : 0060 cost =  0.002200\n",
      "Epoch : 0061 cost =  0.001357\n",
      "Epoch : 0062 cost =  0.001187\n",
      "Epoch : 0063 cost =  0.001580\n",
      "Epoch : 0064 cost =  0.001013\n",
      "Epoch : 0065 cost =  0.000589\n",
      "Epoch : 0066 cost =  0.002268\n",
      "Epoch : 0067 cost =  0.001535\n",
      "Epoch : 0068 cost =  0.001195\n",
      "Epoch : 0069 cost =  0.000476\n",
      "Epoch : 0070 cost =  0.003439\n",
      "Epoch : 0071 cost =  0.000634\n",
      "Epoch : 0072 cost =  0.000639\n",
      "Epoch : 0073 cost =  0.000605\n",
      "Epoch : 0074 cost =  0.000547\n",
      "Epoch : 0075 cost =  0.002737\n",
      "Epoch : 0076 cost =  0.001140\n",
      "Epoch : 0077 cost =  0.000490\n",
      "Epoch : 0078 cost =  0.002448\n",
      "Epoch : 0079 cost =  0.000289\n",
      "Epoch : 0080 cost =  0.000566\n",
      "Epoch : 0081 cost =  0.001283\n",
      "Epoch : 0082 cost =  0.000502\n",
      "Epoch : 0083 cost =  0.001116\n",
      "Epoch : 0084 cost =  0.001621\n",
      "Epoch : 0085 cost =  0.000584\n",
      "Epoch : 0086 cost =  0.000871\n",
      "Epoch : 0087 cost =  0.001251\n",
      "Epoch : 0088 cost =  0.000240\n",
      "Epoch : 0089 cost =  0.000533\n",
      "Epoch : 0090 cost =  0.000423\n",
      "Epoch : 0091 cost =  0.001603\n",
      "Epoch : 0092 cost =  0.000329\n",
      "Epoch : 0093 cost =  0.000185\n",
      "Epoch : 0094 cost =  0.001422\n",
      "Epoch : 0095 cost =  0.000746\n",
      "Epoch : 0096 cost =  0.000374\n",
      "Epoch : 0097 cost =  0.000938\n",
      "Epoch : 0098 cost =  0.001036\n",
      "Epoch : 0099 cost =  0.000905\n",
      "Epoch : 0100 cost =  0.001486\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 학습\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost], feed_dict={\n",
    "                                                     enc_input: input_batch, \n",
    "                                                     dec_input: output_batch,\n",
    "                                                     targets: target_batch,\n",
    "                                                    })\n",
    "    \n",
    "    print('Epoch :', '%04d' % (epoch + 1), 'cost = ', '{:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 번역 테스트 ===\n",
      "['소', '녀', '사', 'r', '스', '이', 'E', '나', '소']\n",
      "word -> 소녀사r스이\n",
      "['나', '무', 'E', '녀', 'E']\n",
      "wodr -> 나무\n",
      "['사', '랑', 'E', 'o', '소']\n",
      "love -> 사랑\n",
      "['사', '랑', 'E', '이', '소']\n",
      "loev -> 사랑\n",
      "['키', '스', 'E', '이', '소']\n",
      "abcd -> 키스\n"
     ]
    }
   ],
   "source": [
    "# 번역 테스트\n",
    "def translate(word):\n",
    "    # 예측시 디코더의 입출력값을 의미 없는 값인 P로 채운다.\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    \n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    # 결과가 [batch_size, time_step, input] 으로 나오기 때문에\n",
    "    # 2번째 차원원인 input에서 가장 확률이 높은 글자를 예측값으로 만든다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "    \n",
    "    result = sess.run(prediction, feed_dict={\n",
    "        enc_input: input_batch,\n",
    "        dec_input: output_batch,\n",
    "        targets: target_batch,\n",
    "    })\n",
    "    \n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    \n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "    print(decoded)\n",
    "    \n",
    "    return translated\n",
    "\n",
    "print('\\n=== 번역 테스트 ===')\n",
    "\n",
    "print('word ->', translate('wordword'))\n",
    "print('wodr ->', translate('wodr'))\n",
    "print('love ->', translate('love'))\n",
    "print('loev ->', translate('loev'))\n",
    "print('abcd ->', translate('abcd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
