{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Embeddings\n",
    "    word_embedding_dim = 300\n",
    "    char_embedding_dim = 128\n",
    "    pos_tag_embedding_dim = 64\n",
    "\n",
    "    \n",
    "    # RNN\n",
    "    hidden_size_word = 128\n",
    "    hidden_size_char = 64\n",
    "    l2_reg_lambda = 1e-5\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 30\n",
    "    display_every = 100\n",
    "    evaluate_every = 250\n",
    "    num_checkpoints = 5\n",
    "    \n",
    "    learning_rate = 1.0 \n",
    "    decay_rate = 0.95\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "    \n",
    "    UNK = \"$UNK$\"\n",
    "    NUM = \"$NUM$\"\n",
    "    NONE = \"O\"\n",
    "    PAD = '$PAD$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "ACE2005 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data Size]\n",
      "train :  14958\n",
      "dev :  891\n",
      "test :  713\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNP NN VBD JJ NN IN NNP POS NNP NN NN</td>\n",
       "      <td>O O O O O O O O O O O</td>\n",
       "      <td>energy regulator named new head of britain 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NNP , NNP CD -LRB- NN -RRB-</td>\n",
       "      <td>O O O O O O O</td>\n",
       "      <td>london , april $NUM$ -lrb- afp -rrb-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJ NNP IN DT NNP NNP NNP IN NNP VBD DT JJ NN I...</td>\n",
       "      <td>O O O O O O O O O B-Personnel:Nominate O O O O...</td>\n",
       "      <td>british chancellor of the exchequer gordon bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJ JJ NN NNP NNP VBZ WP VBZ CD IN DT RBS JJ NN...</td>\n",
       "      <td>B-Personnel:End-Position O O O O B-Personnel:S...</td>\n",
       "      <td>former senior banker callum mccarthy begins wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NNP VBZ VBG TO VB NN IN DT NNP NNP IN NNP , CD...</td>\n",
       "      <td>O O B-Personnel:End-Position O B-Personnel:Sta...</td>\n",
       "      <td>davies is leaving to become chairman of the lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NNP VBD NNP MD VB TO DT NNP `` DT JJ NN IN NN ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>brown said mccarthy would bring to the fsa `` ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP RB VBZ DT NNP IN NNP CC NNP NNPS , CC NNP ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O</td>\n",
       "      <td>mccarthy currently heads the office of gas and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RB RB IN RB VBG JJ NNS IN NNP NNP , NNP CC NNP...</td>\n",
       "      <td>O O O B-Personnel:End-Position O O O O O O O O...</td>\n",
       "      <td>as well as previously holding senior positions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IN NNP POS NN , DT NNP VBD NNP POS JJ JJ NN IN...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>under davies 's watch , the fsa became britain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NNP VBZ `` NN NN '' NN IN NNP CC NNP VBZ VBG I...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O</td>\n",
       "      <td>russia hints `` peace camp '' alliance with ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pos_tags  \\\n",
       "0              NNP NN VBD JJ NN IN NNP POS NNP NN NN   \n",
       "1                        NNP , NNP CD -LRB- NN -RRB-   \n",
       "2  JJ NNP IN DT NNP NNP NNP IN NNP VBD DT JJ NN I...   \n",
       "3  JJ JJ NN NNP NNP VBZ WP VBZ CD IN DT RBS JJ NN...   \n",
       "4  NNP VBZ VBG TO VB NN IN DT NNP NNP IN NNP , CD...   \n",
       "5  NNP VBD NNP MD VB TO DT NNP `` DT JJ NN IN NN ...   \n",
       "6  NNP RB VBZ DT NNP IN NNP CC NNP NNPS , CC NNP ...   \n",
       "7  RB RB IN RB VBG JJ NNS IN NNP NNP , NNP CC NNP...   \n",
       "8  IN NNP POS NN , DT NNP VBD NNP POS JJ JJ NN IN...   \n",
       "9  NNP VBZ `` NN NN '' NN IN NNP CC NNP VBZ VBG I...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                              O O O O O O O O O O O   \n",
       "1                                      O O O O O O O   \n",
       "2  O O O O O O O O O B-Personnel:Nominate O O O O...   \n",
       "3  B-Personnel:End-Position O O O O B-Personnel:S...   \n",
       "4  O O B-Personnel:End-Position O B-Personnel:Sta...   \n",
       "5  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "6        O O O O O O O O O O O O O O O O O O O O O O   \n",
       "7  O O O B-Personnel:End-Position O O O O O O O O...   \n",
       "8  O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "9                    O O O O O O O O O O O O O O O O   \n",
       "\n",
       "                                               words  \n",
       "0  energy regulator named new head of britain 's ...  \n",
       "1               london , april $NUM$ -lrb- afp -rrb-  \n",
       "2  british chancellor of the exchequer gordon bro...  \n",
       "3  former senior banker callum mccarthy begins wh...  \n",
       "4  davies is leaving to become chairman of the lo...  \n",
       "5  brown said mccarthy would bring to the fsa `` ...  \n",
       "6  mccarthy currently heads the office of gas and...  \n",
       "7  as well as previously holding senior positions...  \n",
       "8  under davies 's watch , the fsa became britain...  \n",
       "9  russia hints `` peace camp '' alliance with ge...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.all_tags, self.all_words, self.all_chars, self.all_pos_tags = [], [], [], []\n",
    "        self.dir_path = './data/ace2005'\n",
    "        \n",
    "    def processing_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word.replace(',','').isdigit():\n",
    "            word = Config.NUM\n",
    "        return word\n",
    "        \n",
    "    def load_dataset(self, name):\n",
    "        words_col, tags_col, pos_tags_col = [], [], []\n",
    "        \n",
    "        path = os.path.join(self.dir_path, '{}.json'.format(name))\n",
    "        \n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            for item in data:\n",
    "                words = item['tokens']\n",
    "                \n",
    "                words = []\n",
    "                for word in item['tokens']:\n",
    "                    word = self.processing_word(word)\n",
    "                    \n",
    "                    self.all_words.append(word)\n",
    "                    self.all_chars.extend(list(word))\n",
    "                    \n",
    "                    words.append(word)\n",
    "                \n",
    "                pos_tags = item['pos-tag']\n",
    "                for pos_tag in pos_tags:\n",
    "                    self.all_pos_tags.append(pos_tag)\n",
    "                    \n",
    "                tags = [Config.NONE] * len(words)\n",
    "                    \n",
    "                for event_mention in item['golden_event_mentions']:\n",
    "                    for i in range(event_mention['trigger']['start'], event_mention['trigger']['end']+1):\n",
    "                        if len(tags) <= i:\n",
    "                            print('out of range! argument:', argument, words)\n",
    "                            continue\n",
    "                            \n",
    "                        event_type = event_mention['event_type']\n",
    "                        if i == event_mention['trigger']['start']:\n",
    "                            tags[i] = 'B-{}'.format(event_type)\n",
    "                        else:\n",
    "                            tags[i] = 'I-{}'.format(event_type)\n",
    "                            \n",
    "                        self.all_tags.append(tags[i])\n",
    "                \n",
    "                max_len = 50\n",
    "                \n",
    "                words_col.append(' '.join(words[:max_len]))\n",
    "                tags_col.append(' '.join(tags[:max_len]))\n",
    "                pos_tags_col.append(' '.join(pos_tags[:max_len]))\n",
    "        \n",
    "        \n",
    "        return pd.DataFrame({'words': words_col, \n",
    "                             'tags': tags_col,\n",
    "                             'pos_tags': pos_tags_col})\n",
    "        \n",
    "    def load_datasets(self):\n",
    "        train_df = self.load_dataset('train')\n",
    "        dev_df =  self.load_dataset('dev')\n",
    "        test_df = self.load_dataset('test')\n",
    "        \n",
    "        return train_df, dev_df, test_df\n",
    "\n",
    "dataset = Dataset()\n",
    "train_df, dev_df, test_df = dataset.load_datasets()\n",
    "\n",
    "print('[Data Size]')\n",
    "print('train : ', len(train_df))\n",
    "print('dev : ', len(dev_df))\n",
    "print('test : ', len(test_df))\n",
    "\n",
    "train_df.head(10)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(set(dataset.all_words)) + [Config.PAD, Config.UNK]\n",
    "word2idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx2word = {i: w for i, w in enumerate(word_list)}\n",
    "\n",
    "tag_list = list(set(dataset.all_tags)) + [Config.NONE, Config.PAD]\n",
    "tag2idx = {w: i for i, w in enumerate(tag_list)}\n",
    "idx2tag = {i: w for i, w in enumerate(tag_list)}\n",
    "\n",
    "char_list = list(set(dataset.all_chars)) + [Config.PAD, Config.UNK]\n",
    "char2idx = {w: i for i, w in enumerate(char_list)}\n",
    "idx2char = {i: w for i, w in enumerate(char_list)}\n",
    "\n",
    "pos_tag_list =  list(set(dataset.all_pos_tags)) + [Config.PAD, Config.UNK]\n",
    "pos_tag2idx = {w: i for i, w in enumerate(pos_tag_list)}\n",
    "idx2pos_tag = {i: w for i, w in enumerate(pos_tag_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, \n",
    "               num_classes, \n",
    "               vocab_size, \n",
    "               char_size,\n",
    "               word_embedding_dim, \n",
    "               char_embedding_dim,\n",
    "               pos_tag_embedding_dim,\n",
    "               hidden_size_word,\n",
    "               hidden_size_char):\n",
    "        \n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name='word_ids') \n",
    "        self.pos_tag_ids = tf.placeholder(tf.int32, shape=[None, None], name='pos_tag_ids') \n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        \n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')\n",
    "        \n",
    "        self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name='char_ids') # [batch_size, max_sequence_length, max_word_length]\n",
    "        self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\") # [batch_size, max_sequence_length]\n",
    "        \n",
    "        self.dropout = tf.placeholder(dtype=tf.float32, shape=[],name=\"dropout\")\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        # Word Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.variable_scope('word-embedding'):\n",
    "            self._word_embeddings = tf.Variable(tf.random_uniform([vocab_size, word_embedding_dim], -0.25, 0.25), name='_word_embeddings')\n",
    "            self.word_embeddings = tf.nn.embedding_lookup(self._word_embeddings, self.word_ids) # [batch_size, max_sequence_length, word_embedding_dim]\n",
    "        \n",
    "        with tf.variable_scope('pos-tag-embedding'):\n",
    "            self._pos_tag_embeddings = tf.get_variable('_pos_tag_embeddings', [vocab_size, pos_tag_embedding_dim], initializer=tf.keras.initializers.glorot_normal())\n",
    "            self.pos_tag_embeddings = tf.nn.embedding_lookup(self._pos_tag_embeddings, self.pos_tag_ids) # [batch_size, max_sequence_length, pos_tag_embedding_dim]\n",
    "        \n",
    "        # Char Embedding Layer\n",
    "        with tf.variable_scope('char-embedding'):\n",
    "            self._char_embeddings = tf.get_variable(dtype=tf.float32, shape=[char_size, char_embedding_dim], name='_char_embeddings')\n",
    "            \n",
    "            # [batch_size, max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            self.char_embeddings = tf.nn.embedding_lookup(self._char_embeddings, self.char_ids) \n",
    "            \n",
    "            s = tf.shape(self.char_embeddings)\n",
    "            \n",
    "            # [batch_size*max_sequence_length, max_word_length, char_embedding_dim]\n",
    "            char_embeddings = tf.reshape(self.char_embeddings, shape=[s[0]*s[1], s[2], char_embedding_dim])\n",
    "            word_lengths = tf.reshape(self.word_lengths, shape=[-1])\n",
    "            \n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_char, state_is_tuple=True)\n",
    "            \n",
    "            _, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell, \n",
    "                                                                                   cell_bw=bw_cell, \n",
    "                                                                                   inputs=char_embeddings,\n",
    "                                                                                   sequence_length=word_lengths,\n",
    "                                                                                   dtype=tf.float32)\n",
    "            # shape: [batch_size*max_sequnce_length, 2*hidden_size_char]\n",
    "            output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "            output = tf.reshape(output, shape=[s[0], s[1], 2*hidden_size_char])\n",
    "            \n",
    "            # shape: # [batch_size, max_sequence_length, word_embedding_dim + 2*hidden_size_char + pos_embedding_dim]\n",
    "            self.word_embeddings = tf.concat([self.word_embeddings, output, self.pos_tag_embeddings], axis=-1) \n",
    "            # self.word_embeddings = tf.nn.dropout(self.word_embeddings, self.dropout)\n",
    "            \n",
    "        # Bidirectional LSTM\n",
    "        with tf.variable_scope(\"bi-lstm\"):\n",
    "            fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size_word)\n",
    "            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_cell,\n",
    "                                                                  cell_bw=bw_cell,\n",
    "                                                                  inputs=self.word_embeddings,\n",
    "                                                                  sequence_length= self.sequence_lengths, # [batch_size],\n",
    "                                                                  dtype=tf.float32)\n",
    "            \n",
    "            self.rnn_outputs = tf.concat([output_fw, output_bw], axis=-1)  # [batch_size, max_sequence_length, 2*hidden_size_word]\n",
    "            self.rnn_outputs = tf.nn.dropout(self.rnn_outputs, self.dropout)\n",
    "        \n",
    "        \n",
    "        # Fully connected layer\n",
    "        with tf.variable_scope('output'):\n",
    "            self.W_output = tf.get_variable('W_output', shape=[2*hidden_size_word, num_classes],  dtype=tf.float32)\n",
    "            self.b_output = tf.get_variable('b_output', shape=[num_classes], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            \n",
    "            nsteps = tf.shape(self.rnn_outputs)[1]\n",
    "            rnn_outputs_flat = tf.reshape(self.rnn_outputs, [-1, 2*hidden_size_word])\n",
    "            pred = tf.matmul(rnn_outputs_flat, self.W_output) + self.b_output\n",
    "            \n",
    "            self.logits = tf.reshape(pred, [-1, nsteps, num_classes]) # [batch_size, max_sequence_length, num_classes]\n",
    "    \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)\n",
    "            self.trans_params = trans_params  # need to evaluate it for decoding\n",
    "            \n",
    "            self.l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "            self.loss = tf.reduce_mean(-log_likelihood) + Config.l2_reg_lambda * self.l2\n",
    "            \n",
    "#             When CRF is not in use\n",
    "#             self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.labels)\n",
    "#             mask = tf.sequence_mask(self.sequence_lengths)\n",
    "#             losses = tf.boolean_mask(self.losses, mask)\n",
    "#             self.loss = tf.reduce_mean(losses) \n",
    "        \n",
    "    # Length of the sequence data\n",
    "    @staticmethod\n",
    "    def _length(seq):\n",
    "        relevant = tf.sign(tf.abs(seq))\n",
    "        length = tf.reduce_sum(relevant, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "    \n",
    "    @staticmethod\n",
    "    def viterbi_decode(logits, trans_params):\n",
    "        # get tag scores and transition params of CRF\n",
    "        viterbi_sequences = []\n",
    "\n",
    "        # iterate over the sentences because no batching in vitervi_decode\n",
    "        for logit, sequence_length in zip(logits, sequence_lengths):\n",
    "            logit = logit[:sequence_length]  # keep only the valid steps\n",
    "            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(\n",
    "                logit, trans_params)\n",
    "            viterbi_sequences += [viterbi_seq]\n",
    "\n",
    "        return np.array(viterbi_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained glove\n",
    "def load_glove(word_embedding_dim, word2idx):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"glove.6B.zip\", \n",
    "      origin=\"http://nlp.stanford.edu/data/glove.6B.zip\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'glove.6B.300d.txt')\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(word2idx), word_embedding_dim).astype(np.float32) / np.sqrt(len(word2idx))\n",
    "    # load any vectors from the glove\n",
    "    print(\"Load glove file {0}\".format(embedding_path))\n",
    "    f = open(embedding_path, 'r', encoding='utf8')\n",
    "    for line in f:\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        if word in word2idx:\n",
    "            initW[word2idx[word]] = embedding\n",
    "    return initW\n",
    "\n",
    "def batch_iter(df, batch_size, num_epochs, shuffle=True, tqdm_disable=False):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data_size = len(df)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for epoch in tqdm(range(num_epochs), disable=tqdm_disable):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_df= df.iloc[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_df = df\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_df.iloc[start_index:end_index]    \n",
    "            \n",
    "\n",
    "def get_feed_dict(batch_df):\n",
    "    max_length = max(map(lambda x : len(x.split(' ')), batch_df['words'].tolist()))\n",
    "    \n",
    "    # for char length\n",
    "    max_length_word = 0\n",
    "    for seq in batch_df['words'].tolist():\n",
    "        for word in seq.split(' '):\n",
    "            max_length_word = max(max_length_word, len(word))\n",
    "    \n",
    "    word_ids, sequence_lengths, labels, char_ids, pos_tag_ids, word_lengths = [], [], [], [], [], []\n",
    "    for index, row in batch_df.iterrows():\n",
    "        sentence = row['words'].split(' ')\n",
    "        pos_tags = row['pos_tags'].split(' ')\n",
    "        tags = row['tags'].split(' ')\n",
    "\n",
    "        word_ids_row, labels_row, char_ids_row, pos_tag_ids_row, word_lengths_row = [], [], [], [], []\n",
    "        for word in sentence:\n",
    "            word_ids_row.append(word2idx[word])\n",
    "        \n",
    "            char_ids_row.append([char2idx[char] for char in word] + [char2idx[Config.PAD]]* (max_length_word - len(word)) )\n",
    "            word_lengths_row.append(len(word))\n",
    "        \n",
    "        for pos_tag in pos_tags:\n",
    "            pos_tag_ids_row.append(pos_tag2idx[pos_tag])\n",
    "            \n",
    "        empty_char_ids = [char2idx[Config.PAD]]* max_length_word\n",
    "        char_ids_row += [empty_char_ids] * (max_length - len(char_ids_row))\n",
    "        word_lengths_row += [0] * (max_length - len(word_lengths_row))\n",
    "        \n",
    "        for tag in tags:\n",
    "            labels_row.append(tag2idx[tag])\n",
    "\n",
    "        if len(sentence) < max_length:\n",
    "            word_ids_row += [word2idx[Config.PAD]]* (max_length - len(sentence))\n",
    "            labels_row += [tag2idx[Config.PAD]]* (max_length - len(sentence))\n",
    "            pos_tag_ids_row += [pos_tag2idx[Config.PAD]]* (max_length - len(pos_tag_ids_row))\n",
    "        \n",
    "        assert len(word_ids_row) == max_length\n",
    "        assert len(labels_row) == max_length\n",
    "        assert len(pos_tag_ids_row) == max_length\n",
    "        for item in char_ids_row:\n",
    "            if len(item) != max_length_word:\n",
    "                print('len(item) :', len(item))\n",
    "                print('item :', item)\n",
    "            assert len(item) == max_length_word\n",
    "        \n",
    "        word_ids.append(word_ids_row)\n",
    "        labels.append(labels_row)\n",
    "        sequence_lengths.append(len(sentence))\n",
    "        char_ids.append(char_ids_row)\n",
    "        word_lengths.append(word_lengths_row)\n",
    "        pos_tag_ids.append(pos_tag_ids_row)\n",
    "    \n",
    "    word_ids = np.array(word_ids)\n",
    "    labels = np.array(labels)\n",
    "    sequence_lengths = np.array(sequence_lengths)\n",
    "    char_ids = np.array(char_ids)\n",
    "    word_lengths = np.array(word_lengths)\n",
    "    pos_tag_ids = np.array(pos_tag_ids)\n",
    "    \n",
    "    return word_ids, labels, sequence_lengths, char_ids, word_lengths, pos_tag_ids\n",
    "\n",
    "def evaluation(y, preds, lengths, words):\n",
    "    from sklearn.metrics import classification_report\n",
    "    arg_answers, arg_preds = [], []\n",
    "    \n",
    "    with open('./evalutation.txt', 'w') as f:\n",
    "        correct_preds, total_correct, total_preds = 0.0, 0.0, 0.0\n",
    "        for i in range(len(y)):\n",
    "            sent_answers,sent_preds = [], []\n",
    "            sent_answer_chunks, sent_pred_chunks = [], []\n",
    "\n",
    "            f.write('\\n-----------------\\n')\n",
    "            f.write(' '.join([idx2word[word] for word in words[i][:lengths[i]]]) + '\\n')\n",
    "            f.write(' '.join([idx2tag[y_item] for y_item in y[i][:lengths[i]]]) + '\\n')\n",
    "            f.write(' '.join([idx2tag[pred_item] for pred_item in preds[i][:lengths[i]]]) + '\\n')\n",
    "\n",
    "            for j in range(lengths[i]):\n",
    "                sent_answers.append(idx2tag[y[i][j]])\n",
    "                sent_preds.append(idx2tag[preds[i][j]])\n",
    "\n",
    "                if idx2tag[y[i][j]] != Config.NONE:\n",
    "                    sent_answer_chunks.append(idx2tag[y[i][j]] + '-' + str(j))\n",
    "                if idx2tag[preds[i][j]] != Config.NONE:\n",
    "                    sent_pred_chunks.append(idx2tag[preds[i][j]] + '-' + str(j))\n",
    "\n",
    "            arg_answers.extend(sent_answers)\n",
    "            arg_preds.extend(sent_preds)\n",
    "\n",
    "            sent_answer_chunks = set(sent_answer_chunks)\n",
    "            sent_pred_chunks = set(sent_pred_chunks)\n",
    "\n",
    "            correct_preds += len(sent_answer_chunks & sent_pred_chunks)\n",
    "            total_preds += len(sent_pred_chunks)\n",
    "            total_correct += len(sent_answer_chunks)\n",
    "\n",
    "        p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "        print(classification_report(arg_answers, arg_preds))\n",
    "\n",
    "        print('Tag based evaluation:\\nprecision: {}, recall: {}, f1: {}'.format(p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-e1447156919d>:47: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-e1447156919d>:54: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-e1447156919d>:74: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/seungwon/project/tf-notes/39.runs/1560788731\n",
      "\n",
      "embedding_path : /home/seungwon/.keras/datasets/glove.6B.300d.txt\n",
      "Load glove file /home/seungwon/.keras/datasets/glove.6B.300d.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to load pre-trained glove model!\n",
      "\n",
      "Train Evaluation 2019-06-18T01:26:26.494551: step 100, loss 5.31784\n",
      "Train Evaluation 2019-06-18T01:26:40.637703: step 200, loss 5.02272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 1/30 [00:33<16:22, 33.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev Evaluation\n",
      "2019-06-18T01:26:49.131533: loss 0.079289\n",
      "\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "   B-Business:Declare-Bankruptcy       0.00      0.00      0.00         1\n",
      "               B-Conflict:Attack       0.68      0.74      0.71       174\n",
      "          B-Conflict:Demonstrate       0.00      0.00      0.00         9\n",
      "                  B-Contact:Meet       0.75      0.43      0.55        28\n",
      "           B-Contact:Phone-Write       0.00      0.00      0.00         3\n",
      "                B-Justice:Appeal       0.00      0.00      0.00         7\n",
      "           B-Justice:Arrest-Jail       0.00      0.00      0.00         4\n",
      "         B-Justice:Charge-Indict       0.00      0.00      0.00         1\n",
      "               B-Justice:Convict       0.00      0.00      0.00         6\n",
      "               B-Justice:Execute       0.00      0.00      0.00         5\n",
      "              B-Justice:Sentence       0.00      0.00      0.00         4\n",
      "                   B-Justice:Sue       0.00      0.00      0.00         9\n",
      "         B-Justice:Trial-Hearing       0.00      0.00      0.00         1\n",
      "                      B-Life:Die       0.87      0.70      0.78        57\n",
      "                   B-Life:Injure       0.00      0.00      0.00        14\n",
      "            B-Movement:Transport       0.67      0.04      0.07        57\n",
      "               B-Personnel:Elect       0.00      0.00      0.00         3\n",
      "        B-Personnel:End-Position       0.00      0.00      0.00        26\n",
      "      B-Personnel:Start-Position       0.00      0.00      0.00        13\n",
      "    B-Transaction:Transfer-Money       0.00      0.00      0.00        40\n",
      "B-Transaction:Transfer-Ownership       0.00      0.00      0.00         4\n",
      "               I-Conflict:Attack       0.00      0.00      0.00         6\n",
      "                      I-Life:Die       0.00      0.00      0.00         1\n",
      "            I-Movement:Transport       0.00      0.00      0.00         6\n",
      "        I-Personnel:End-Position       0.00      0.00      0.00         1\n",
      "      I-Personnel:Start-Position       0.00      0.00      0.00         1\n",
      "    I-Transaction:Transfer-Money       0.00      0.00      0.00         2\n",
      "                               O       0.98      1.00      0.99     17682\n",
      "\n",
      "                       micro avg       0.98      0.98      0.98     18165\n",
      "                       macro avg       0.14      0.10      0.11     18165\n",
      "                    weighted avg       0.97      0.98      0.97     18165\n",
      "\n",
      "Tag based evaluation:\n",
      "precision: 0.7193675889328063, recall: 0.37681159420289856, f1: 0.4945652173913043\n",
      "Train Evaluation 2019-06-18T01:26:56.256947: step 300, loss 4.01173\n",
      "Train Evaluation 2019-06-18T01:27:10.687363: step 400, loss 4.23328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 2/30 [01:08<15:56, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation 2019-06-18T01:27:24.872394: step 500, loss 4.1362\n",
      "\n",
      "Dev Evaluation\n",
      "2019-06-18T01:27:26.101911: loss 0.073686\n",
      "\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "   B-Business:Declare-Bankruptcy       1.00      1.00      1.00         1\n",
      "               B-Conflict:Attack       0.79      0.69      0.74       174\n",
      "          B-Conflict:Demonstrate       0.50      0.11      0.18         9\n",
      "                  B-Contact:Meet       0.70      0.25      0.37        28\n",
      "           B-Contact:Phone-Write       0.00      0.00      0.00         3\n",
      "                B-Justice:Appeal       0.00      0.00      0.00         7\n",
      "           B-Justice:Arrest-Jail       1.00      0.50      0.67         4\n",
      "         B-Justice:Charge-Indict       0.00      0.00      0.00         1\n",
      "               B-Justice:Convict       1.00      1.00      1.00         6\n",
      "               B-Justice:Execute       0.00      0.00      0.00         5\n",
      "              B-Justice:Sentence       1.00      0.50      0.67         4\n",
      "                   B-Justice:Sue       0.00      0.00      0.00         9\n",
      "         B-Justice:Trial-Hearing       0.50      1.00      0.67         1\n",
      "                      B-Life:Die       0.86      0.63      0.73        57\n",
      "                   B-Life:Injure       0.73      0.57      0.64        14\n",
      "            B-Movement:Transport       0.71      0.09      0.16        57\n",
      "               B-Personnel:Elect       0.00      0.00      0.00         3\n",
      "        B-Personnel:End-Position       0.80      0.15      0.26        26\n",
      "      B-Personnel:Start-Position       0.00      0.00      0.00        13\n",
      "    B-Transaction:Transfer-Money       0.00      0.00      0.00        40\n",
      "B-Transaction:Transfer-Ownership       0.00      0.00      0.00         4\n",
      "               I-Conflict:Attack       0.00      0.00      0.00         6\n",
      "                      I-Life:Die       0.00      0.00      0.00         1\n",
      "            I-Movement:Transport       0.00      0.00      0.00         6\n",
      "        I-Personnel:End-Position       0.00      0.00      0.00         1\n",
      "      I-Personnel:Start-Position       0.00      0.00      0.00         1\n",
      "    I-Transaction:Transfer-Money       0.00      0.00      0.00         2\n",
      "                               O       0.98      1.00      0.99     17682\n",
      "\n",
      "                       micro avg       0.98      0.98      0.98     18165\n",
      "                       macro avg       0.38      0.27      0.29     18165\n",
      "                    weighted avg       0.97      0.98      0.98     18165\n",
      "\n",
      "Tag based evaluation:\n",
      "precision: 0.7975206611570248, recall: 0.3995859213250518, f1: 0.5324137931034483\n",
      "Train Evaluation 2019-06-18T01:27:40.584655: step 600, loss 3.89541\n",
      "Train Evaluation 2019-06-18T01:27:54.658255: step 700, loss 4.00535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 3/30 [01:43<15:26, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dev Evaluation\n",
      "2019-06-18T01:28:02.960847: loss 0.0726257\n",
      "\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "   B-Business:Declare-Bankruptcy       1.00      1.00      1.00         1\n",
      "               B-Conflict:Attack       0.79      0.70      0.74       174\n",
      "          B-Conflict:Demonstrate       0.75      0.33      0.46         9\n",
      "                  B-Contact:Meet       0.69      0.39      0.50        28\n",
      "           B-Contact:Phone-Write       0.00      0.00      0.00         3\n",
      "                B-Justice:Appeal       0.00      0.00      0.00         7\n",
      "           B-Justice:Arrest-Jail       1.00      0.50      0.67         4\n",
      "         B-Justice:Charge-Indict       0.00      0.00      0.00         1\n",
      "               B-Justice:Convict       0.00      0.00      0.00         6\n",
      "               B-Justice:Execute       0.00      0.00      0.00         5\n",
      "              B-Justice:Sentence       1.00      0.50      0.67         4\n",
      "                   B-Justice:Sue       1.00      0.33      0.50         9\n",
      "         B-Justice:Trial-Hearing       0.50      1.00      0.67         1\n",
      "                      B-Life:Die       0.97      0.49      0.65        57\n",
      "                   B-Life:Injure       0.69      0.64      0.67        14\n",
      "            B-Movement:Transport       0.74      0.25      0.37        57\n",
      "               B-Personnel:Elect       0.67      0.67      0.67         3\n",
      "        B-Personnel:End-Position       0.75      0.12      0.20        26\n",
      "      B-Personnel:Start-Position       1.00      0.08      0.14        13\n",
      "    B-Transaction:Transfer-Money       0.00      0.00      0.00        40\n",
      "B-Transaction:Transfer-Ownership       0.00      0.00      0.00         4\n",
      "               I-Conflict:Attack       0.00      0.00      0.00         6\n",
      "                      I-Life:Die       0.00      0.00      0.00         1\n",
      "            I-Movement:Transport       0.00      0.00      0.00         6\n",
      "        I-Personnel:End-Position       0.00      0.00      0.00         1\n",
      "      I-Personnel:Start-Position       0.00      0.00      0.00         1\n",
      "    I-Transaction:Transfer-Money       0.00      0.00      0.00         2\n",
      "                               O       0.98      1.00      0.99     17682\n",
      "\n",
      "                       micro avg       0.98      0.98      0.98     18165\n",
      "                       macro avg       0.45      0.29      0.32     18165\n",
      "                    weighted avg       0.98      0.98      0.98     18165\n",
      "\n",
      "Tag based evaluation:\n",
      "precision: 0.7976190476190477, recall: 0.4161490683229814, f1: 0.5469387755102041\n",
      "Train Evaluation 2019-06-18T01:28:10.279112: step 800, loss 3.93365\n",
      "Train Evaluation 2019-06-18T01:28:24.439971: step 900, loss 4.20839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 4/30 [02:17<14:53, 34.37s/it]"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    model = Model(\n",
    "        num_classes=len(tag_list),\n",
    "        vocab_size=len(word_list),\n",
    "        char_size=len(char_list),\n",
    "        word_embedding_dim=Config.word_embedding_dim,\n",
    "        char_embedding_dim=Config.char_embedding_dim,\n",
    "        hidden_size_word=Config.hidden_size_word,\n",
    "        hidden_size_char=Config.hidden_size_char,\n",
    "        pos_tag_embedding_dim=Config.pos_tag_embedding_dim\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # train_op = tf.train.AdamOptimizer(Config.learning_rate).minimize(model.loss, global_step=global_step)\n",
    "    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(Config.learning_rate, Config.decay_rate, 1e-6)\n",
    "    gvs = optimizer.compute_gradients(model.loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"39.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    pretrain_W = load_glove(Config.word_embedding_dim, word2idx)\n",
    "    sess.run(model._word_embeddings.assign(pretrain_W))\n",
    "    print(\"Success to load pre-trained glove model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(train_df, Config.batch_size, Config.num_epochs)\n",
    "    for batch_df in batches:\n",
    "        word_ids, labels, sequence_lengths, char_ids, word_lengths, pos_tag_ids = get_feed_dict(batch_df)\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.word_ids: word_ids,\n",
    "            model.labels: labels,\n",
    "            model.sequence_lengths: sequence_lengths,\n",
    "            model.char_ids: char_ids,\n",
    "            model.word_lengths: word_lengths,\n",
    "            model.dropout: 0.5,\n",
    "            model.pos_tag_ids: pos_tag_ids\n",
    "        }\n",
    "        _, step, loss, logits, trans_params = sess.run([\n",
    "            train_op, global_step, model.loss, model.logits, model.trans_params], feed_dict)\n",
    "        \n",
    "        predictions = model.viterbi_decode(logits, trans_params)\n",
    "        \n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"Train Evaluation {}: step {}, loss {:g}\".format(time_str, step, loss))\n",
    "#             evaluation(labels, predictions, sequence_lengths)\n",
    "            \n",
    "            \n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            batches = batch_iter(dev_df, Config.batch_size, 1, tqdm_disable=True)\n",
    "            \n",
    "            total_loss, predictions_all, labels_all, sequence_lengths_all, words_all  = 0, [], [], [], []\n",
    "            for batch_df in batches:\n",
    "                word_ids, labels, sequence_lengths, char_ids, word_lengths, pos_tag_ids = get_feed_dict(batch_df)\n",
    "                feed_dict = {\n",
    "                    model.word_ids: word_ids,\n",
    "                    model.labels: labels,\n",
    "                    model.sequence_lengths: sequence_lengths,\n",
    "                    model.char_ids: char_ids,\n",
    "                    model.word_lengths: word_lengths,\n",
    "                    model.dropout: 1.0,\n",
    "                    model.pos_tag_ids: pos_tag_ids\n",
    "                }\n",
    "                loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "                predictions = model.viterbi_decode(logits, trans_params)\n",
    "                \n",
    "                total_loss += loss\n",
    "                predictions_all += predictions.tolist()\n",
    "                labels_all += labels.tolist()\n",
    "                sequence_lengths_all += sequence_lengths.tolist()\n",
    "                words_all += word_ids.tolist()\n",
    "        \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"\\nDev Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "            evaluation(labels_all, predictions_all, sequence_lengths_all, words_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batch_iter(test_df, Config.batch_size, 1, tqdm_disable=True)\n",
    "total_loss, predictions_all, labels_all, sequence_lengths_all, words_all  = 0, [], [], [], []\n",
    "for batch_df in batches:\n",
    "    word_ids, labels, sequence_lengths, char_ids, word_lengths, pos_tag_ids = get_feed_dict(batch_df)\n",
    "    feed_dict = {\n",
    "        model.word_ids: word_ids,\n",
    "        model.labels: labels,\n",
    "        model.sequence_lengths: sequence_lengths,\n",
    "        model.char_ids: char_ids,\n",
    "        model.word_lengths: word_lengths,\n",
    "        model.dropout: 1.0,\n",
    "        model.pos_tag_ids: pos_tag_ids\n",
    "    }\n",
    "    loss, logits, trans_params = sess.run([model.loss, model.logits, model.trans_params], feed_dict)\n",
    "    predictions = model.viterbi_decode(logits, trans_params)\n",
    "\n",
    "    total_loss += loss\n",
    "    predictions_all += predictions.tolist()\n",
    "    labels_all += labels.tolist()\n",
    "    sequence_lengths_all += sequence_lengths.tolist()\n",
    "    words_all += word_ids.tolist()\n",
    "\n",
    "time_str = datetime.datetime.now().isoformat()\n",
    "print(\"\\Test Evaluation\\n{}: loss {:g}\\n\".format(time_str, total_loss/len(predictions_all)))\n",
    "evaluation(labels_all, predictions_all, sequence_lengths_all, words_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
