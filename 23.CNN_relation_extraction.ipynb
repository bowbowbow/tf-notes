{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Reference : https://github.com/roomylee/cnn-relation-extraction\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data loading params\n",
    "    max_sentence_length = 90\n",
    "    dev_sample_percentage = 0.1\n",
    "    \n",
    "    # Embeddings\n",
    "    embedding_path = ''\n",
    "    text_embedding_dim = 300\n",
    "    pos_embedding_dim = 50\n",
    "    \n",
    "    # CNN\n",
    "    filter_sizes = '2,3,4,5'\n",
    "    num_filters = 128\n",
    "    dropout_keep_prob = 0.5\n",
    "    l2_reg_lambda = 1e-5\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 20\n",
    "    num_epochs = 100\n",
    "    display_every = 500\n",
    "    evaluate_every = 1000\n",
    "    num_checkpoints = 5\n",
    "    learning_rate = 1.0\n",
    "    decay_rate = 0.9\n",
    "    \n",
    "    # Testing parameters\n",
    "    checkpoint_dir = ''\n",
    "\n",
    "    labels_count = 19\n",
    "    class2label = {'Other': 0,\n",
    "               'Message-Topic(e1,e2)': 1, 'Message-Topic(e2,e1)': 2,\n",
    "               'Product-Producer(e1,e2)': 3, 'Product-Producer(e2,e1)': 4,\n",
    "               'Instrument-Agency(e1,e2)': 5, 'Instrument-Agency(e2,e1)': 6,\n",
    "               'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8,\n",
    "               'Cause-Effect(e1,e2)': 9, 'Cause-Effect(e2,e1)': 10,\n",
    "               'Component-Whole(e1,e2)': 11, 'Component-Whole(e2,e1)': 12,\n",
    "               'Entity-Origin(e1,e2)': 13, 'Entity-Origin(e2,e1)': 14,\n",
    "               'Member-Collection(e1,e2)': 15, 'Member-Collection(e2,e1)': 16,\n",
    "               'Content-Container(e1,e2)': 17, 'Content-Container(e2,e1)': 18}\n",
    "\n",
    "    label2class = {0: 'Other',\n",
    "                   1: 'Message-Topic(e1,e2)', 2: 'Message-Topic(e2,e1)',\n",
    "                   3: 'Product-Producer(e1,e2)', 4: 'Product-Producer(e2,e1)',\n",
    "                   5: 'Instrument-Agency(e1,e2)', 6: 'Instrument-Agency(e2,e1)',\n",
    "                   7: 'Entity-Destination(e1,e2)', 8: 'Entity-Destination(e2,e1)',\n",
    "                   9: 'Cause-Effect(e1,e2)', 10: 'Cause-Effect(e2,e1)',\n",
    "                   11: 'Component-Whole(e1,e2)', 12: 'Component-Whole(e2,e1)',\n",
    "                   13: 'Entity-Origin(e1,e2)', 14: 'Entity-Origin(e2,e1)',\n",
    "                   15: 'Member-Collection(e1,e2)', 16: 'Member-Collection(e2,e1)',\n",
    "                   17: 'Content-Container(e1,e2)', 18: 'Content-Container(e2,e1)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "Load Relation Extraction dataset of SemEval2010 task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seungwon/.keras/datasets/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\n",
      "max sentence length = 89\n",
      "\n",
      "/home/seungwon/.keras/datasets/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\n",
      "max sentence length = 68\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>pos1</th>\n",
       "      <th>pos2</th>\n",
       "      <th>relation</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "      <td>71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 8...</td>\n",
       "      <td>Component-Whole(e2,e1)</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>the e11 child e12 was carefully wrapped and bo...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>87 88 89 90 91 92 93 94 95 96 97 98 99 100 101...</td>\n",
       "      <td>77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 9...</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>the e11 author e12 of a keygen uses a e21 disa...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>87 88 89 90 91 92 93 94 95 96 97 98 99 100 101...</td>\n",
       "      <td>79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 9...</td>\n",
       "      <td>Instrument-Agency(e2,e1)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a misty e11 ridge e12 uprises from the e21 sur...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>86 87 88 89 90 91 92 93 94 95 96</td>\n",
       "      <td>80 81 82 83 84 85 86 87 88 89 90</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>the e11 student e12 e21 association e22 is the...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>87 88 89 90 91 92 93 94 95 96 97 98 99 100 101...</td>\n",
       "      <td>84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 9...</td>\n",
       "      <td>Member-Collection(e1,e2)</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           sentence  e1  e2  \\\n",
       "0  1  the system as described above has its greatest...  13  18   \n",
       "1  2  the e11 child e12 was carefully wrapped and bo...   2  12   \n",
       "2  3  the e11 author e12 of a keygen uses a e21 disa...   2  10   \n",
       "3  4  a misty e11 ridge e12 uprises from the e21 sur...   3   9   \n",
       "4  5  the e11 student e12 e21 association e22 is the...   2   5   \n",
       "\n",
       "                                                pos1  \\\n",
       "0  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...   \n",
       "1  87 88 89 90 91 92 93 94 95 96 97 98 99 100 101...   \n",
       "2  87 88 89 90 91 92 93 94 95 96 97 98 99 100 101...   \n",
       "3                  86 87 88 89 90 91 92 93 94 95 96    \n",
       "4  87 88 89 90 91 92 93 94 95 96 97 98 99 100 101...   \n",
       "\n",
       "                                                pos2  \\\n",
       "0  71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 8...   \n",
       "1  77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 9...   \n",
       "2  79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 9...   \n",
       "3                  80 81 82 83 84 85 86 87 88 89 90    \n",
       "4  84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 9...   \n",
       "\n",
       "                   relation  label  \n",
       "0    Component-Whole(e2,e1)     12  \n",
       "1                     Other      0  \n",
       "2  Instrument-Agency(e2,e1)      6  \n",
       "3                     Other      0  \n",
       "4  Member-Collection(e1,e2)     15  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "\n",
    "class Dataset:\n",
    "    def clean_str(self, text):\n",
    "        text = text.lower()\n",
    "        # Clean the text\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"that's\", \"that is \", text)\n",
    "        text = re.sub(r\"there's\", \"there is \", text)\n",
    "        text = re.sub(r\"it's\", \"it is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"can not \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data_and_labels(self, path):\n",
    "        # Data Format\n",
    "        # 1\\t\"The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.\"\n",
    "        # Component-Whole(e2,e1)\n",
    "        # Comment: Not a collection: there is structure here, organisation.\n",
    "        # \n",
    "        # 2\\t\"The <e1>child</e1> was carefully wrapped and bound into the <e2>cradle</e2> by means of a cord.\"\n",
    "        # Other\n",
    "        # Comment:\n",
    "        # \n",
    "        data = []\n",
    "        lines = [line.strip() for line in open(path)]\n",
    "        max_sentence_length = 0\n",
    "        for idx in range(0, len(lines), 4):\n",
    "            id = lines[idx].split(\"\\t\")[0]\n",
    "            \n",
    "            # Sentence\n",
    "            sentence = lines[idx].split(\"\\t\")[1][1:-1]\n",
    "            sentence = sentence.replace('<e1>', ' _e11_ ')\n",
    "            sentence = sentence.replace('</e1>', ' _e12_ ')\n",
    "            sentence = sentence.replace('<e2>', ' _e21_ ')\n",
    "            sentence = sentence.replace('</e2>', ' _e22_ ')\n",
    "\n",
    "            sentence = self.clean_str(sentence)\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            sentence = \" \".join(tokens)\n",
    "            \n",
    "            # Max Sentence Length\n",
    "            if max_sentence_length < len(tokens):\n",
    "                max_sentence_length = len(tokens)\n",
    "                \n",
    "            # e1, e2 position\n",
    "            e1 = tokens.index(\"e12\") - 1\n",
    "            e2 = tokens.index(\"e22\") - 1\n",
    "            \n",
    "            # Relative Position\n",
    "            pos1 = \"\"\n",
    "            pos2 = \"\"\n",
    "            for word_idx in range(len(tokens)):\n",
    "                pos1 += str((Config.max_sentence_length - 1) + word_idx - e1) + \" \"\n",
    "                pos2 += str((Config.max_sentence_length - 1) + word_idx - e2) + \" \"\n",
    "                \n",
    "            # Label\n",
    "            relation = lines[idx + 1]\n",
    "            label = Config.class2label[relation]\n",
    "            data.append([id, sentence, e1, e2, pos1, pos2, relation, label])\n",
    "\n",
    "        print(path)\n",
    "        print(\"max sentence length = {}\\n\".format(max_sentence_length))\n",
    "\n",
    "        df = pd.DataFrame(data=data, columns=[\"id\", \"sentence\", \"e1\", \"e2\", 'pos1', 'pos2', 'relation', 'label'])\n",
    "        return df\n",
    "    \n",
    "    def download_and_load_datasets(self):\n",
    "        dataset = tf.keras.utils.get_file(\n",
    "          fname=\"SemEval2010_task8_all_data.zip\", \n",
    "          origin=\"https://s3.ap-northeast-2.amazonaws.com/bowbowbow-storage/dataset/SemEval2010_task8_all_data.zip\", \n",
    "          extract=True)\n",
    "        \n",
    "        train_file = 'SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT'\n",
    "        test_file = 'SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT'\n",
    "        \n",
    "        train_df = self.load_data_and_labels(os.path.join(os.path.dirname(dataset), train_file))\n",
    "        test_df = self.load_data_and_labels(os.path.join(os.path.dirname(dataset), test_file))\n",
    "        return train_df, test_df\n",
    "\n",
    "dataset = Dataset()\n",
    "train_df, test_df = dataset.download_and_load_datasets()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, \n",
    "               sequence_length, \n",
    "               num_classes, \n",
    "               text_vocab_size, \n",
    "               text_embedding_size, \n",
    "               pos_vocab_size, \n",
    "               pos_embedding_size, \n",
    "               filter_sizes,\n",
    "               num_filters, \n",
    "               l2_reg_lambda=0.0):\n",
    "        \n",
    "        self.input_text = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_text')\n",
    "        self.input_p1 = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_p1')\n",
    "        self.input_p2 = tf.placeholder(tf.int32, shape=[None, sequence_length], name='input_p2')\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[None, num_classes], name='input_y')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.variable_scope('text-embedding'):\n",
    "            self.W_text = tf.Variable(tf.random_uniform([text_vocab_size, text_embedding_size], -0.25, 0.25), name='W_text')\n",
    "            self.text_embedded_chars = tf.nn.embedding_lookup(self.W_text, self.input_text)\n",
    "            self.text_embedded_chars_expanded = tf.expand_dims(self.text_embedded_chars, -1)\n",
    "            \n",
    "        with tf.variable_scope('position-embedding'):\n",
    "            self.W_pos = tf.get_variable('W_pos', [pos_vocab_size, pos_embedding_size], initializer=tf.keras.initializers.glorot_normal())\n",
    "            self.p1_embedded_chars = tf.nn.embedding_lookup(self.W_pos, self.input_p1)\n",
    "            self.p2_embedded_chars = tf.nn.embedding_lookup(self.W_pos, self.input_p2)\n",
    "            self.p1_embedded_chars_expanded = tf.expand_dims(self.p1_embedded_chars, -1)\n",
    "            self.p2_embedded_chars_expanded = tf.expand_dims(self.p2_embedded_chars, -1)\n",
    "        \n",
    "        self.embedded_chars_expanded = tf.concat([self.text_embedded_chars_expanded,\n",
    "                                                  self.p1_embedded_chars_expanded,\n",
    "                                                  self.p2_embedded_chars_expanded], 2)\n",
    "        \n",
    "        _embedding_size = text_embedding_size + 2*pos_embedding_size\n",
    "        \n",
    "        # convolution + maxpool layer\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.variable_scope('conv-maxpool-{}'.format(filter_size)):\n",
    "                conv = tf.layers.conv2d(self.embedded_chars_expanded, \n",
    "                                       num_filters, \n",
    "                                       [filter_size, _embedding_size], \n",
    "                                       kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
    "                                       activation=tf.nn.relu,\n",
    "                                       name='conv')\n",
    "                pooled = tf.nn.max_pool(conv, \n",
    "                                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                                        strides=[1, 1, 1, 1],\n",
    "                                        padding='VALID',\n",
    "                                        name='pool'\n",
    "                                       )\n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        \n",
    "        self.h_pool = tf.concat(pooled_outputs, 3) # [batch_size, 1, 1, num_filters]\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.variable_scope('dropout'):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "        \n",
    "        # Final scores and predictions\n",
    "        with tf.variable_scope('output'):\n",
    "            self.logits = tf.layers.dense(self.h_drop, num_classes, kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "            self.predictions = tf.argmax(self.logits, 1, name='predictions')\n",
    "        \n",
    "        # Calculate mean corss-entropy loss\n",
    "        with tf.variable_scope('loss'):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.input_y)\n",
    "            self.l2 = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * self.l2\n",
    "        \n",
    "        # Accuracy    \n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-dd03f76453cd>:2: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Text Vocabulary Size 19151\n",
      "X = (8000, 90)\n",
      "Y = (8000, 19)\n",
      "Position Vocabulary Size 162\n",
      "position1 = (8000, 90)\n",
      "position2 = (8000, 90)\n",
      "Train/Dev split: 7200/800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "text_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(Config.max_sentence_length)\n",
    "x = np.array(list(text_vocab_processor.fit_transform(train_df['sentence'])))\n",
    "y = np.array([np.eye(Config.labels_count)[label] for label in train_df['label']]) # One-hot encoding \n",
    "\n",
    "print('Text Vocabulary Size {}'.format(len(text_vocab_processor.vocabulary_)))\n",
    "print('X = {}'.format(x.shape))\n",
    "print('Y = {}'.format(y.shape))\n",
    "\n",
    "# Example: pos1[3] = '95 96 97 98 99 100 101 999 999 999 ... 999' (without offset: [-2 -1  0  1  2   3   4 999 999 999 ... 999])\n",
    "# =>\n",
    "# [11 12 13 14 15  16  21  17  17  17 ...  17]\n",
    "pos_vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(Config.max_sentence_length)\n",
    "pos_vocab_processor.fit(train_df['pos1'] + train_df['pos2'])\n",
    "\n",
    "p1 = np.array(list(pos_vocab_processor.transform(train_df['pos1'])))\n",
    "p2 = np.array(list(pos_vocab_processor.transform(train_df['pos2'])))\n",
    "\n",
    "print('Position Vocabulary Size {}'.format(len(pos_vocab_processor.vocabulary_)))\n",
    "print('position1 = {}'.format(p1.shape))\n",
    "print('position2 = {}'.format(p2.shape))\n",
    "\n",
    "# Randomly shuffle data to split into train and dev\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled, p1_shuffled, p2_shuffled, y_shuffled = x[shuffle_indices], p1[shuffle_indices], p2[shuffle_indices], y[shuffle_indices]\n",
    "\n",
    "# Split train/dev set\n",
    "dev_sample_index = -1 * int(Config.dev_sample_percentage*float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:] \n",
    "p1_train, p1_dev = p1_shuffled[:dev_sample_index], p1_shuffled[dev_sample_index:] \n",
    "p2_train, p2_dev = p2_shuffled[:dev_sample_index], p2_shuffled[dev_sample_index:] \n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:] \n",
    "print(\"Train/Dev split: {:d}/{:d}\\n\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained word2vec\n",
    "def load_word2vec(embedding_dim, vocab):\n",
    "    download_path = tf.keras.utils.get_file(\n",
    "      fname=\"GoogleNews-vectors-negative300.bin.gz\", \n",
    "      origin=\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \n",
    "      extract=True)\n",
    "    \n",
    "    embedding_path = os.path.join(os.path.dirname(download_path), 'GoogleNews-vectors-negative300.bin')\n",
    "    if not os.path.exists(embedding_path):\n",
    "        print('unzip :', embedding_path)\n",
    "        import gzip\n",
    "        import shutil\n",
    "        with gzip.open('{}.gz'.format(embedding_path), 'rb') as f_in:\n",
    "            with open(embedding_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "    print('embedding_path :', embedding_path)\n",
    "\n",
    "    # initial matrix with random uniform\n",
    "    initW = np.random.randn(len(vocab.vocabulary_), embedding_dim).astype(np.float32) / np.sqrt(len(vocab.vocabulary_))\n",
    "    # load any vectors from the word2vec\n",
    "    print(\"Load word2vec file {0}\".format(embedding_path))\n",
    "    with open(embedding_path, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * layer_size\n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1).decode('latin-1')\n",
    "                if ch == ' ':\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n':\n",
    "                    word.append(ch)\n",
    "            idx = vocab.vocabulary_.get(word)\n",
    "            if idx != 0:\n",
    "                initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return initW\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-add9d4c6d64c>:47: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-add9d4c6d64c>:64: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-4-add9d4c6d64c>:68: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Writing to /home/seungwon/project/tf-notes/23.runs/1557233711\n",
      "\n",
      "embedding_path : /home/seungwon/.keras/datasets/GoogleNews-vectors-negative300.bin\n",
      "Load word2vec file /home/seungwon/.keras/datasets/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/ipykernel_launcher.py:38: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success to load pre-trained word2vec model!\n",
      "\n",
      "2019-05-07T21:55:37.014310: step 500, loss 1.38153, acc 0.8\n",
      "2019-05-07T21:55:44.275194: step 1000, loss 1.54936, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:55:44.773806: step 1000, loss 1.63837, acc 0.7575\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.732765\n",
      "\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.733-1000\n",
      "\n",
      "2019-05-07T21:55:52.238061: step 1500, loss 1.01738, acc 0.9\n",
      "2019-05-07T21:55:59.522175: step 2000, loss 1.02222, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:55:59.598847: step 2000, loss 1.65998, acc 0.77125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.74938\n",
      "\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.749-2000\n",
      "\n",
      "2019-05-07T21:56:07.002339: step 2500, loss 0.890682, acc 1\n",
      "2019-05-07T21:56:14.277781: step 3000, loss 0.839589, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:56:14.355677: step 3000, loss 1.67266, acc 0.79625\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.770133\n",
      "\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.77-3000\n",
      "\n",
      "2019-05-07T21:56:21.759343: step 3500, loss 0.8276, acc 1\n",
      "2019-05-07T21:56:29.039636: step 4000, loss 0.840783, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:56:29.116637: step 4000, loss 1.70541, acc 0.78625\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.767892\n",
      "\n",
      "2019-05-07T21:56:36.422015: step 4500, loss 0.81044, acc 1\n",
      "2019-05-07T21:56:43.705826: step 5000, loss 0.806412, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:56:43.783071: step 5000, loss 1.72827, acc 0.78625\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.75667\n",
      "\n",
      "2019-05-07T21:56:51.031355: step 5500, loss 0.794787, acc 1\n",
      "2019-05-07T21:56:58.292072: step 6000, loss 0.790947, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:56:58.368499: step 6000, loss 1.76264, acc 0.78875\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.766275\n",
      "\n",
      "2019-05-07T21:57:05.648333: step 6500, loss 0.780075, acc 1\n",
      "2019-05-07T21:57:12.972878: step 7000, loss 0.770583, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:57:13.050380: step 7000, loss 1.76394, acc 0.78125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.764104\n",
      "\n",
      "2019-05-07T21:57:20.361538: step 7500, loss 0.762993, acc 1\n",
      "2019-05-07T21:57:27.653118: step 8000, loss 0.755306, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:57:27.730462: step 8000, loss 1.7526, acc 0.7925\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.775729\n",
      "\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.776-8000\n",
      "\n",
      "2019-05-07T21:57:35.227760: step 8500, loss 0.75444, acc 1\n",
      "2019-05-07T21:57:42.578475: step 9000, loss 0.741814, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:57:42.655651: step 9000, loss 1.7839, acc 0.7925\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.769774\n",
      "\n",
      "2019-05-07T21:57:49.994763: step 9500, loss 0.737858, acc 1\n",
      "2019-05-07T21:57:57.315193: step 10000, loss 0.726571, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:57:57.391198: step 10000, loss 1.79441, acc 0.79125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.77151\n",
      "\n",
      "2019-05-07T21:58:04.713235: step 10500, loss 0.719658, acc 1\n",
      "2019-05-07T21:58:12.031577: step 11000, loss 0.712711, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:58:12.107956: step 11000, loss 1.766, acc 0.78125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.760451\n",
      "\n",
      "2019-05-07T21:58:19.402601: step 11500, loss 0.706617, acc 1\n",
      "2019-05-07T21:58:26.693736: step 12000, loss 0.700177, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:58:26.770945: step 12000, loss 1.72467, acc 0.78875\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.774966\n",
      "\n",
      "2019-05-07T21:58:34.077222: step 12500, loss 0.694031, acc 1\n",
      "2019-05-07T21:58:41.417804: step 13000, loss 0.68503, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:58:41.494726: step 13000, loss 1.70862, acc 0.795\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.774223\n",
      "\n",
      "2019-05-07T21:58:48.813753: step 13500, loss 0.686331, acc 1\n",
      "2019-05-07T21:58:56.129452: step 14000, loss 0.68028, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:58:56.205506: step 14000, loss 1.72951, acc 0.79625\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.77744\n",
      "\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.777-14000\n",
      "\n",
      "2019-05-07T21:59:03.631601: step 14500, loss 0.665115, acc 1\n",
      "2019-05-07T21:59:10.978333: step 15000, loss 0.661455, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:59:11.055185: step 15000, loss 1.72905, acc 0.7975\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.778415\n",
      "\n",
      "WARNING:tensorflow:From /home/seungwon/project/tf-notes/venv/lib/python3.5/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.778-15000\n",
      "\n",
      "2019-05-07T21:59:18.512592: step 15500, loss 0.654529, acc 1\n",
      "2019-05-07T21:59:25.863538: step 16000, loss 0.646417, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:59:25.942363: step 16000, loss 1.71296, acc 0.79125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.768906\n",
      "\n",
      "2019-05-07T21:59:33.259783: step 16500, loss 0.641072, acc 1\n",
      "2019-05-07T21:59:40.597348: step 17000, loss 0.634592, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:59:40.674111: step 17000, loss 1.70244, acc 0.7975\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.773481\n",
      "\n",
      "2019-05-07T21:59:48.019652: step 17500, loss 0.627294, acc 1\n",
      "2019-05-07T21:59:55.354602: step 18000, loss 0.62387, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T21:59:55.430582: step 18000, loss 1.65317, acc 0.79\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.777356\n",
      "\n",
      "2019-05-07T22:00:02.787850: step 18500, loss 0.614921, acc 1\n",
      "2019-05-07T22:00:10.146657: step 19000, loss 0.609646, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:00:10.224110: step 19000, loss 1.6287, acc 0.80125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.785086\n",
      "\n",
      "Saved model checkpoint to /home/seungwon/project/tf-notes/23.runs/1557233711/checkpoints/model-0.785-19000\n",
      "\n",
      "2019-05-07T22:00:17.701764: step 19500, loss 0.604697, acc 1\n",
      "2019-05-07T22:00:25.054772: step 20000, loss 0.597385, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:00:25.130988: step 20000, loss 1.6537, acc 0.79625\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.776879\n",
      "\n",
      "2019-05-07T22:00:32.527209: step 20500, loss 0.591104, acc 1\n",
      "2019-05-07T22:00:39.867433: step 21000, loss 0.589313, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:00:39.944257: step 21000, loss 1.62481, acc 0.79\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.774959\n",
      "\n",
      "2019-05-07T22:00:47.323076: step 21500, loss 0.589663, acc 1\n",
      "2019-05-07T22:00:54.673772: step 22000, loss 0.574484, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:00:54.753355: step 22000, loss 1.60864, acc 0.795\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.768861\n",
      "\n",
      "2019-05-07T22:01:02.094673: step 22500, loss 0.568493, acc 1\n",
      "2019-05-07T22:01:09.448383: step 23000, loss 0.563675, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:01:09.525291: step 23000, loss 1.59518, acc 0.7925\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.774148\n",
      "\n",
      "2019-05-07T22:01:16.867870: step 23500, loss 0.557773, acc 1\n",
      "2019-05-07T22:01:24.180807: step 24000, loss 0.552059, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:01:24.260028: step 24000, loss 1.58957, acc 0.79125\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.776322\n",
      "\n",
      "2019-05-07T22:01:31.590479: step 24500, loss 0.54666, acc 1\n",
      "2019-05-07T22:01:38.923317: step 25000, loss 0.544141, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-05-07T22:01:39.000044: step 25000, loss 1.58328, acc 0.795\n",
      "[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): 0.77416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    cnn = TextCNN(\n",
    "        sequence_length=x_train.shape[1],\n",
    "        num_classes=y_train.shape[1],\n",
    "        text_vocab_size=len(text_vocab_processor.vocabulary_),\n",
    "        text_embedding_size=Config.text_embedding_dim,\n",
    "        pos_vocab_size=len(pos_vocab_processor.vocabulary_),\n",
    "        pos_embedding_size=Config.pos_embedding_dim,\n",
    "        filter_sizes=list(map(int, Config.filter_sizes.split(','))),\n",
    "        num_filters=Config.num_filters,\n",
    "        l2_reg_lambda=Config.l2_reg_lambda\n",
    "    )\n",
    "    \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(Config.learning_rate, Config.decay_rate, 1e-6)\n",
    "    gvs = optimizer.compute_gradients(cnn.loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in gvs]\n",
    "    train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)\n",
    "    \n",
    "    # Output directory for models and summary\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"23.runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Dev summaries\n",
    "    dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "    dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "    dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "    # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=Config.num_checkpoints)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    pretrain_W = load_word2vec(Config.text_embedding_dim, text_vocab_processor)\n",
    "    sess.run(cnn.W_text.assign(pretrain_W))\n",
    "    print(\"Success to load pre-trained word2vec model!\\n\")\n",
    "    \n",
    "    # Generate batches\n",
    "    batches = batch_iter(list(zip(x_train, p1_train, p2_train, y_train)),\n",
    "                                      Config.batch_size, \n",
    "                                      Config.num_epochs)\n",
    "    \n",
    "    # Training loop. For each batch...\n",
    "    best_f1 = 0.0  # For save checkpoint(model)\n",
    "    for batch in batches:\n",
    "        x_batch, p1_batch, p2_batch, y_batch = zip(*batch)\n",
    "        # Train\n",
    "        feed_dict = {\n",
    "            cnn.input_text: x_batch,\n",
    "            cnn.input_p1: p1_batch,\n",
    "            cnn.input_p2: p2_batch,\n",
    "            cnn.input_y: y_batch,\n",
    "            cnn.dropout_keep_prob: Config.dropout_keep_prob\n",
    "        }\n",
    "        _, step, summaries, loss, accuracy = sess.run(\n",
    "            [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], feed_dict)\n",
    "        train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        # Training log display\n",
    "        if step % Config.display_every == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "        # Evaluation\n",
    "        if step % Config.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            feed_dict = {\n",
    "                cnn.input_text: x_dev,\n",
    "                cnn.input_p1: p1_dev,\n",
    "                cnn.input_p2: p2_dev,\n",
    "                cnn.input_y: y_dev,\n",
    "                cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            summaries, loss, accuracy, predictions = sess.run(\n",
    "                [dev_summary_op, cnn.loss, cnn.accuracy, cnn.predictions], feed_dict)\n",
    "            dev_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            f1 = f1_score(np.argmax(y_dev, axis=1), predictions, labels=np.array(range(1, 19)), average=\"macro\")\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            print(\"[UNOFFICIAL] (2*9+1)-Way Macro-Average F1 Score (excluding Other): {:g}\\n\".format(f1))\n",
    "\n",
    "            # Model checkpoint\n",
    "            if best_f1 < f1:\n",
    "                best_f1 = f1\n",
    "                path = saver.save(sess, checkpoint_prefix + \"-{:.3g}\".format(best_f1), global_step=step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./23.runs --host 0.0.0.0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
